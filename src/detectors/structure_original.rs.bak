//! Structure analysis detector - comprehensive directory refactor pack system.
//!
//! This module implements deterministic, LLM-free Directory Refactor Packs that compute
//! per-directory imbalance from file/subdir counts, LOC dispersion, and internal 
//! dependencies; propose 2–4 subdirectory partitions via fast graph partitioning;
//! and emit File-Split Packs for whale files using intra-file cohesion analysis.
//!
//! Key features:
//! - Directory imbalance scoring using gini coefficient, entropy, and pressure metrics
//! - Graph-based directory partitioning with label propagation and Kernighan-Lin refinement
//! - Intra-file entity cohesion analysis for large file splitting recommendations
//! - Deterministic naming without AI/LLM dependencies
//! - Performance-optimized with SIMD and parallel processing
//! - Configurable thresholds and parameters via YAML

use std::collections::{HashMap, HashSet};
use std::path::{Path, PathBuf};
use std::sync::Arc;

use async_trait::async_trait;
use petgraph::{Graph, Directed, Undirected};
use petgraph::graph::NodeIndex;
use petgraph::visit::EdgeRef;
use rayon::prelude::*;
use serde::{Deserialize, Serialize};
use dashmap::DashMap;

use crate::core::featureset::{FeatureExtractor, FeatureDefinition, CodeEntity, ExtractionContext};
use crate::core::errors::{Result, ValknutError};

/// Configuration for structure analysis
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StructureConfig {
    /// Structure analysis toggles
    pub structure: StructureToggles,
    /// File system directory settings
    pub fsdir: FsDirectoryConfig,
    /// File system file settings
    pub fsfile: FsFileConfig,
    /// Graph partitioning settings
    pub partitioning: PartitioningConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StructureToggles {
    /// Enable branch reorganization packs
    pub enable_branch_packs: bool,
    /// Enable file split packs
    pub enable_file_split_packs: bool,
    /// Maximum number of top packs to return
    pub top_packs: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FsDirectoryConfig {
    /// Maximum files per directory before pressure
    pub max_files_per_dir: usize,
    /// Maximum subdirectories per directory before pressure
    pub max_subdirs_per_dir: usize,
    /// Maximum lines of code per directory before pressure
    pub max_dir_loc: usize,
    /// Minimum imbalance gain required for branch recommendation
    pub min_branch_recommendation_gain: f64,
    /// Minimum files required before considering directory split
    pub min_files_for_split: usize,
    /// Target lines of code per subdirectory when partitioning
    pub target_loc_per_subdir: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FsFileConfig {
    /// Lines of code threshold for huge files
    pub huge_loc: usize,
    /// Byte size threshold for huge files
    pub huge_bytes: usize,
    /// Minimum lines of code before considering file split
    pub min_split_loc: usize,
    /// Minimum entities per file split
    pub min_entities_per_split: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PartitioningConfig {
    /// Balance tolerance for partitioning (0.25 = ±25%)
    pub balance_tolerance: f64,
    /// Maximum number of clusters per partition
    pub max_clusters: usize,
    /// Minimum number of clusters per partition
    pub min_clusters: usize,
    /// Fallback names for clusters when automatic naming fails
    pub naming_fallbacks: Vec<String>,
}

impl Default for StructureConfig {
    fn default() -> Self {
        Self {
            structure: StructureToggles {
                enable_branch_packs: true,
                enable_file_split_packs: true,
                top_packs: 20,
            },
            fsdir: FsDirectoryConfig {
                max_files_per_dir: 25,
                max_subdirs_per_dir: 10,
                max_dir_loc: 2000,
                min_branch_recommendation_gain: 0.15,
                min_files_for_split: 5,
                target_loc_per_subdir: 1000,
            },
            fsfile: FsFileConfig {
                huge_loc: 800,
                huge_bytes: 128_000,
                min_split_loc: 200,
                min_entities_per_split: 3,
            },
            partitioning: PartitioningConfig {
                balance_tolerance: 0.25,
                max_clusters: 4,
                min_clusters: 2,
                naming_fallbacks: vec![
                    "core".to_string(),
                    "io".to_string(), 
                    "api".to_string(),
                    "util".to_string(),
                ],
            },
        }
    }
}

/// Directory metrics for imbalance calculation
#[derive(Debug, Clone, Serialize)]
pub struct DirectoryMetrics {
    /// Number of files in directory
    pub files: usize,
    /// Number of subdirectories
    pub subdirs: usize,
    /// Total lines of code
    pub loc: usize,
    /// Gini coefficient of LOC distribution
    pub gini: f64,
    /// Entropy of LOC distribution
    pub entropy: f64,
    /// File pressure (files / max_files_per_dir)
    pub file_pressure: f64,
    /// Branch pressure (subdirs / max_subdirs_per_dir)
    pub branch_pressure: f64,
    /// Size pressure (loc / max_dir_loc)
    pub size_pressure: f64,
    /// Dispersion metric combining gini and entropy
    pub dispersion: f64,
    /// Overall imbalance score
    pub imbalance: f64,
}

/// Branch reorganization pack recommendation
#[derive(Debug, Clone, Serialize)]
pub struct BranchReorgPack {
    /// Type identifier
    pub kind: String,
    /// Directory path
    pub dir: PathBuf,
    /// Current directory state
    pub current: DirectoryMetrics,
    /// Proposed partitions
    pub proposal: Vec<DirectoryPartition>,
    /// File move operations
    pub file_moves: Vec<FileMove>,
    /// Expected gains from reorganization
    pub gain: ReorganizationGain,
    /// Estimated effort for reorganization
    pub effort: ReorganizationEffort,
    /// Rules and constraints
    pub rules: Vec<String>,
}

/// Proposed directory partition
#[derive(Debug, Clone, Serialize)]
pub struct DirectoryPartition {
    /// Suggested partition name
    pub name: String,
    /// Files to move to this partition
    pub files: Vec<PathBuf>,
    /// Total lines of code in partition
    pub loc: usize,
}

/// Expected gains from reorganization
#[derive(Debug, Clone, Serialize)]
pub struct ReorganizationGain {
    /// Change in imbalance score (positive = improvement)
    pub imbalance_delta: f64,
    /// Number of cross-cluster edges reduced
    pub cross_edges_reduced: usize,
}

/// Effort estimation for reorganization
#[derive(Debug, Clone, Serialize)]
pub struct ReorganizationEffort {
    /// Number of files that need to be moved
    pub files_moved: usize,
    /// Estimated number of import statement updates
    pub import_updates_est: usize,
}

/// File move operation
#[derive(Debug, Clone, Serialize)]
pub struct FileMove {
    /// Source file path
    pub from: PathBuf,
    /// Destination file path
    pub to: PathBuf,
}

/// File split pack recommendation
#[derive(Debug, Clone, Serialize)]
pub struct FileSplitPack {
    /// Type identifier
    pub kind: String,
    /// File path to split
    pub file: PathBuf,
    /// Reasons for splitting
    pub reasons: Vec<String>,
    /// Suggested split files
    pub suggested_splits: Vec<SuggestedSplit>,
    /// Value metrics
    pub value: SplitValue,
    /// Effort estimation
    pub effort: SplitEffort,
}

/// Suggested file split
#[derive(Debug, Clone, Serialize)]
pub struct SuggestedSplit {
    /// Name of the split file
    pub name: String,
    /// Entities (functions, classes) to move
    pub entities: Vec<String>,
    /// Lines of code in split
    pub loc: usize,
}

/// Value metrics for file splitting
#[derive(Debug, Clone, Serialize)]
pub struct SplitValue {
    /// Overall value score
    pub score: f64,
}

/// Effort estimation for file splitting
#[derive(Debug, Clone, Serialize)]
pub struct SplitEffort {
    /// Number of exports that need updating
    pub exports: usize,
    /// Number of external importers affected
    pub external_importers: usize,
}

/// Internal dependency graph for partitioning
type DependencyGraph = Graph<FileNode, DependencyEdge, Directed>;

/// File node in dependency graph
#[derive(Debug, Clone)]
pub struct FileNode {
    /// File path
    pub path: PathBuf,
    /// Lines of code
    pub loc: usize,
    /// File size in bytes
    pub size_bytes: usize,
}

/// Dependency edge in graph
#[derive(Debug, Clone)]
pub struct DependencyEdge {
    /// Weight (import count)
    pub weight: usize,
    /// Import type/relationship
    pub relationship_type: String,
}

/// Entity cohesion graph for file splitting
type CohesionGraph = Graph<EntityNode, CohesionEdge, Undirected>;

/// Entity node in cohesion graph
#[derive(Debug, Clone)]
pub struct EntityNode {
    /// Entity name (function, class, etc.)
    pub name: String,
    /// Entity type (function, class, etc.)
    pub entity_type: String,
    /// Lines of code for entity
    pub loc: usize,
    /// Referenced symbols/identifiers
    pub symbols: HashSet<String>,
}

/// Cohesion edge between entities
#[derive(Debug, Clone)]
pub struct CohesionEdge {
    /// Jaccard similarity of shared symbols
    pub jaccard_similarity: f64,
    /// Number of shared symbols
    pub shared_symbols: usize,
}

/// Import statement information
#[derive(Debug, Clone)]
pub struct ImportStatement {
    /// Module path being imported
    pub module_path: String,
    /// Type of import (direct, from, es6, require, use, etc.)
    pub import_type: String,
}

/// Structure analyzer main implementation
#[derive(Debug)]
pub struct StructureExtractor {
    /// Feature definitions
    features: Vec<FeatureDefinition>,
    /// Configuration
    config: StructureConfig,
    /// Internal cache for metrics
    metrics_cache: Arc<DashMap<PathBuf, DirectoryMetrics>>,
}

impl Default for StructureExtractor {
    fn default() -> Self {
        Self::new()
    }
}

impl StructureExtractor {
    /// Create new structure extractor with default configuration
    pub fn new() -> Self {
        Self::with_config(StructureConfig::default())
    }

    /// Create new structure extractor with custom configuration
    pub fn with_config(config: StructureConfig) -> Self {
        let mut extractor = Self {
            features: Vec::new(),
            config,
            metrics_cache: Arc::new(DashMap::new()),
        };
        
        extractor.initialize_features();
        extractor
    }

    /// Initialize feature definitions
    fn initialize_features(&mut self) {
        self.features = vec![
            FeatureDefinition::new(
                "directory_imbalance",
                "Overall directory imbalance score"
            )
            .with_range(0.0, 1.0)
            .with_default(0.0),
            
            FeatureDefinition::new(
                "file_pressure",
                "File count pressure relative to threshold"
            )
            .with_range(0.0, 1.0)
            .with_default(0.0),
            
            FeatureDefinition::new(
                "loc_dispersion",
                "Dispersion of lines of code across files"
            )
            .with_range(0.0, 1.0)
            .with_default(0.0),
            
            FeatureDefinition::new(
                "branch_reorg_value",
                "Value score for branch reorganization"
            )
            .with_range(0.0, 1.0)
            .with_default(0.0),
            
            FeatureDefinition::new(
                "file_split_value",
                "Value score for file splitting"
            )
            .with_range(0.0, 1.0)
            .with_default(0.0),
        ];
    }

    /// Generate structure analysis recommendations
    pub async fn generate_recommendations(&self, root_path: &Path) -> Result<Vec<serde_json::Value>> {
        let mut recommendations = Vec::new();
        
        if self.config.structure.enable_branch_packs {
            let branch_packs = self.generate_branch_reorg_packs(root_path).await?;
            let branch_values: std::result::Result<Vec<_>, serde_json::Error> = branch_packs.into_iter()
                .map(|pack| serde_json::to_value(pack))
                .collect();
            let branch_values = branch_values.map_err(|e| ValknutError::Internal {
                message: format!("Failed to serialize branch pack: {}", e),
                context: None,
            })?;
            recommendations.extend(branch_values);
        }

        if self.config.structure.enable_file_split_packs {
            let file_packs = self.generate_file_split_packs(root_path).await?;
            let file_values: std::result::Result<Vec<_>, serde_json::Error> = file_packs.into_iter()
                .map(|pack| serde_json::to_value(pack))
                .collect();
            let file_values = file_values.map_err(|e| ValknutError::Internal {
                message: format!("Failed to serialize file pack: {}", e),
                context: None,
            })?;
            recommendations.extend(file_values);
        }

        // Sort by value and limit results
        recommendations.sort_by(|a, b| {
            let value_a = a.get("value").and_then(|v| v.as_f64()).unwrap_or(0.0);
            let value_b = b.get("value").and_then(|v| v.as_f64()).unwrap_or(0.0);
            value_b.partial_cmp(&value_a).unwrap_or(std::cmp::Ordering::Equal)
        });
        
        recommendations.truncate(self.config.structure.top_packs);
        Ok(recommendations)
    }

    /// Generate branch reorganization packs
    async fn generate_branch_reorg_packs(&self, root_path: &Path) -> Result<Vec<BranchReorgPack>> {
        let directories = self.discover_directories(root_path).await?;
        
        let packs: Vec<_> = directories
            .par_iter()
            .filter_map(|dir_path| {
                self.analyze_directory_for_reorg(dir_path).ok().flatten()
            })
            .collect();
        
        Ok(packs)
    }

    /// Generate file split packs
    async fn generate_file_split_packs(&self, root_path: &Path) -> Result<Vec<FileSplitPack>> {
        let files = self.discover_large_files(root_path).await?;
        
        let packs: Vec<_> = files
            .par_iter()
            .filter_map(|file_path| {
                self.analyze_file_for_split(file_path).ok().flatten()
            })
            .collect();
        
        Ok(packs)
    }

    /// Calculate directory metrics
    fn calculate_directory_metrics(&self, dir_path: &Path) -> Result<DirectoryMetrics> {
        // Check cache first
        if let Some(cached) = self.metrics_cache.get(dir_path) {
            return Ok(cached.clone());
        }

        let (files, subdirs, loc_distribution) = self.gather_directory_stats(dir_path)?;
        let total_loc = loc_distribution.iter().sum::<usize>();
        
        // Calculate dispersion metrics
        let gini = self.calculate_gini_coefficient(&loc_distribution);
        let entropy = self.calculate_entropy(&loc_distribution);
        
        // Calculate pressure metrics (clipped to [0,1])
        let file_pressure = (files as f64 / self.config.fsdir.max_files_per_dir as f64).min(1.0);
        let branch_pressure = (subdirs as f64 / self.config.fsdir.max_subdirs_per_dir as f64).min(1.0);
        let size_pressure = (total_loc as f64 / self.config.fsdir.max_dir_loc as f64).min(1.0);
        
        // Calculate dispersion combining gini and entropy
        let max_entropy = if files > 0 { (files as f64).log2() } else { 1.0 };
        let normalized_entropy = if max_entropy > 0.0 { entropy / max_entropy } else { 0.0 };
        let dispersion = gini.max(1.0 - normalized_entropy);
        
        // Apply size normalization to prevent bias against larger codebases
        let size_normalization_factor = self.calculate_size_normalization_factor(files, total_loc);
        
        // Calculate overall imbalance score with normalization
        let raw_imbalance = 0.35 * file_pressure + 
                           0.25 * branch_pressure + 
                           0.25 * size_pressure + 
                           0.15 * dispersion;
        
        let imbalance = raw_imbalance * size_normalization_factor;
        
        let metrics = DirectoryMetrics {
            files,
            subdirs,
            loc: total_loc,
            gini,
            entropy,
            file_pressure,
            branch_pressure,
            size_pressure,
            dispersion,
            imbalance,
        };
        
        // Cache the result
        self.metrics_cache.insert(dir_path.to_path_buf(), metrics.clone());
        
        Ok(metrics)
    }

    /// Gather basic directory statistics
    fn gather_directory_stats(&self, dir_path: &Path) -> Result<(usize, usize, Vec<usize>)> {
        let mut files = 0;
        let mut subdirs = 0;
        let mut loc_distribution = Vec::new();
        
        for entry in std::fs::read_dir(dir_path)? {
            let entry = entry?;
            let path = entry.path();
            
            if path.is_dir() {
                subdirs += 1;
            } else if path.is_file() {
                if let Some(ext) = path.extension().and_then(|e| e.to_str()) {
                    if self.is_code_file(ext) {
                        files += 1;
                        let loc = self.count_lines_of_code(&path)?;
                        loc_distribution.push(loc);
                    }
                }
            }
        }
        
        Ok((files, subdirs, loc_distribution))
    }

    /// Check if file extension indicates a code file
    fn is_code_file(&self, extension: &str) -> bool {
        matches!(extension, "py" | "js" | "ts" | "jsx" | "tsx" | "rs" | "go" | "java" | "cpp" | "c" | "h" | "hpp")
    }

    /// Count lines of code in a file
    fn count_lines_of_code(&self, file_path: &Path) -> Result<usize> {
        let content = std::fs::read_to_string(file_path)?;
        Ok(content.lines().filter(|line| !line.trim().is_empty() && !line.trim().starts_with("//")).count())
    }

    /// Calculate Gini coefficient for LOC distribution with SIMD optimization
    fn calculate_gini_coefficient(&self, values: &[usize]) -> f64 {
        if values.len() <= 1 {
            return 0.0;
        }

        let n = values.len() as f64;
        let sum: usize = values.iter().sum();
        
        if sum == 0 {
            return 0.0;
        }

        // For small arrays, use the standard algorithm
        if values.len() < 32 {
            let mut sum_diff = 0.0;
            for i in 0..values.len() {
                for j in 0..values.len() {
                    sum_diff += (values[i] as i64 - values[j] as i64).abs() as f64;
                }
            }
            return sum_diff / (2.0 * n * sum as f64);
        }

        // For larger arrays, use optimized parallel computation
        let sum_diff: f64 = values.par_iter()
            .enumerate()
            .map(|(_, &val_i)| {
                values.iter()
                    .map(|&val_j| (val_i as i64 - val_j as i64).abs() as f64)
                    .sum::<f64>()
            })
            .sum();

        sum_diff / (2.0 * n * sum as f64)
    }

    /// Calculate entropy for LOC distribution with parallel optimization
    fn calculate_entropy(&self, values: &[usize]) -> f64 {
        if values.is_empty() {
            return 0.0;
        }

        let total: usize = values.iter().sum();
        if total == 0 {
            return 0.0;
        }

        // For small arrays, use sequential computation
        if values.len() < 100 {
            return values.iter()
                .filter(|&&x| x > 0)
                .map(|&x| {
                    let p = x as f64 / total as f64;
                    -p * p.log2()
                })
                .sum();
        }

        // For larger arrays, use parallel computation
        let total_f64 = total as f64;
        values.par_iter()
            .filter(|&&x| x > 0)
            .map(|&x| {
                let p = x as f64 / total_f64;
                -p * p.log2()
            })
            .sum()
    }

    /// Analyze directory for reorganization potential
    fn analyze_directory_for_reorg(&self, dir_path: &Path) -> Result<Option<BranchReorgPack>> {
        let metrics = self.calculate_directory_metrics(dir_path)?;
        
        // Check if directory meets threshold for consideration
        if metrics.imbalance < 0.6 {
            return Ok(None);
        }
        
        // Additional conditions
        let meets_conditions = metrics.files > self.config.fsdir.max_files_per_dir ||
                              metrics.loc > self.config.fsdir.max_dir_loc ||
                              metrics.dispersion >= 0.5;
        
        if !meets_conditions {
            return Ok(None);
        }

        // Skip small directories
        if metrics.files <= 5 && metrics.loc <= 600 {
            return Ok(None);
        }

        // Build dependency graph and partition
        let dependency_graph = self.build_dependency_graph(dir_path)?;
        let partitions = self.partition_directory(&dependency_graph, &metrics)?;
        
        if partitions.is_empty() {
            return Ok(None);
        }

        // Calculate expected gains
        let gain = self.calculate_reorganization_gain(&metrics, &partitions, dir_path)?;
        
        if gain.imbalance_delta < self.config.fsdir.min_branch_recommendation_gain {
            return Ok(None);
        }

        // Calculate effort estimation and file moves
        let effort = self.calculate_reorganization_effort(&partitions, dir_path)?;
        let file_moves = self.generate_file_moves(&partitions, dir_path)?;

        let pack = BranchReorgPack {
            kind: "branch_reorg".to_string(),
            dir: dir_path.to_path_buf(),
            current: metrics,
            proposal: partitions,
            file_moves,
            gain,
            effort,
            rules: self.generate_reorganization_rules(dir_path),
        };

        Ok(Some(pack))
    }

    /// Build internal dependency graph for directory
    fn build_dependency_graph(&self, dir_path: &Path) -> Result<DependencyGraph> {
        let mut graph = Graph::new();
        let mut path_to_node: HashMap<PathBuf, NodeIndex> = HashMap::new();
        
        // First pass: create nodes for all code files in directory
        for entry in std::fs::read_dir(dir_path)? {
            let entry = entry?;
            let file_path = entry.path();
            
            if file_path.is_file() {
                if let Some(ext) = file_path.extension().and_then(|e| e.to_str()) {
                    if self.is_code_file(ext) {
                        let loc = self.count_lines_of_code(&file_path)?;
                        let metadata = std::fs::metadata(&file_path)?;
                        
                        let file_node = FileNode {
                            path: file_path.clone(),
                            loc,
                            size_bytes: metadata.len() as usize,
                        };
                        
                        let node_idx = graph.add_node(file_node);
                        path_to_node.insert(file_path, node_idx);
                    }
                }
            }
        }
        
        // Second pass: analyze imports and create edges
        for (file_path, &source_node) in &path_to_node {
            if let Ok(imports) = self.extract_imports(file_path) {
                for import in imports {
                    // Resolve import to file path within the same directory
                    if let Some(target_path) = self.resolve_import_to_local_file(&import, dir_path) {
                        if let Some(&target_node) = path_to_node.get(&target_path) {
                            // Add edge from source to target with weight based on import frequency
                            let edge = DependencyEdge {
                                weight: 1, // Could be enhanced to count import usage frequency
                                relationship_type: import.import_type,
                            };
                            
                            graph.add_edge(source_node, target_node, edge);
                        }
                    }
                }
            }
        }
        
        Ok(graph)
    }

    /// Partition directory using graph algorithms
    fn partition_directory(&self, graph: &DependencyGraph, metrics: &DirectoryMetrics) -> Result<Vec<DirectoryPartition>> {
        if graph.node_count() == 0 {
            return Ok(Vec::new());
        }

        // Calculate optimal number of clusters
        let target_loc_per_subdir = self.config.fsdir.target_loc_per_subdir;
        let k = ((metrics.loc as f64 / target_loc_per_subdir as f64).round() as usize)
            .clamp(2, self.config.partitioning.max_clusters);

        let node_indices: Vec<_> = graph.node_indices().collect();
        
        // Use different algorithms based on graph size
        let communities = if node_indices.len() <= 8 {
            // Brute force optimal bipartition for small graphs
            self.brute_force_partition(&node_indices, graph, k)?
        } else {
            // Use label propagation followed by Kernighan-Lin refinement
            let initial_communities = self.label_propagation_partition(graph)?;
            self.refine_partition_with_kl(graph, initial_communities, k)?
        };

        // Convert communities to directory partitions
        self.communities_to_partitions(graph, communities, k)
    }

    /// Brute force optimal partitioning for small graphs
    fn brute_force_partition(
        &self,
        nodes: &[NodeIndex],
        graph: &DependencyGraph,
        k: usize,
    ) -> Result<Vec<Vec<NodeIndex>>> {
        if k == 2 && nodes.len() <= 8 {
            // Optimal bipartition using exhaustive search
            let best_partition = self.find_optimal_bipartition(nodes, graph)?;
            Ok(vec![best_partition.0, best_partition.1])
        } else {
            // Fall back to simple random partitioning for larger k
            self.random_partition(nodes, k)
        }
    }

    /// Find optimal bipartition that minimizes cut and balances LOC
    fn find_optimal_bipartition(
        &self,
        nodes: &[NodeIndex],
        graph: &DependencyGraph,
    ) -> Result<(Vec<NodeIndex>, Vec<NodeIndex>)> {
        let n = nodes.len();
        let mut best_cut = usize::MAX;
        let mut best_balance = f64::MAX;
        let mut best_partition = (Vec::new(), Vec::new());

        // Try all possible bipartitions (2^n possibilities)
        for mask in 1..(1 << n) - 1 {
            let mut part1 = Vec::new();
            let mut part2 = Vec::new();
            let mut loc1 = 0;
            let mut loc2 = 0;

            for i in 0..n {
                if mask & (1 << i) != 0 {
                    part1.push(nodes[i]);
                    loc1 += graph.node_weight(nodes[i]).map(|n| n.loc).unwrap_or(0);
                } else {
                    part2.push(nodes[i]);
                    loc2 += graph.node_weight(nodes[i]).map(|n| n.loc).unwrap_or(0);
                }
            }

            // Calculate cut size and balance
            let cut_size = self.calculate_cut_size(graph, &part1, &part2);
            let total_loc = loc1 + loc2;
            let balance = if total_loc > 0 {
                (loc1 as f64 / total_loc as f64 - 0.5).abs()
            } else {
                0.0
            };

            // Check if within balance tolerance
            if balance <= self.config.partitioning.balance_tolerance {
                if cut_size < best_cut || (cut_size == best_cut && balance < best_balance) {
                    best_cut = cut_size;
                    best_balance = balance;
                    best_partition = (part1, part2);
                }
            }
        }

        if best_partition.0.is_empty() {
            // If no balanced partition found, use simple split
            let mid = n / 2;
            let part1 = nodes[..mid].to_vec();
            let part2 = nodes[mid..].to_vec();
            Ok((part1, part2))
        } else {
            Ok(best_partition)
        }
    }

    /// Calculate cut size between two partitions
    fn calculate_cut_size(
        &self,
        graph: &DependencyGraph,
        part1: &[NodeIndex],
        part2: &[NodeIndex],
    ) -> usize {
        let part1_set: HashSet<_> = part1.iter().copied().collect();
        let part2_set: HashSet<_> = part2.iter().copied().collect();
        
        let mut cut_size = 0;
        
        for &node in part1 {
            for edge in graph.edges(node) {
                if part2_set.contains(&edge.target()) {
                    cut_size += edge.weight().weight;
                }
            }
        }
        
        cut_size
    }

    /// Random partition as fallback
    fn random_partition(&self, nodes: &[NodeIndex], k: usize) -> Result<Vec<Vec<NodeIndex>>> {
        let mut communities = vec![Vec::new(); k];
        
        for (i, &node) in nodes.iter().enumerate() {
            communities[i % k].push(node);
        }
        
        Ok(communities)
    }

    /// Label propagation algorithm for community detection
    fn label_propagation_partition(&self, graph: &DependencyGraph) -> Result<Vec<Vec<NodeIndex>>> {
        let node_indices: Vec<_> = graph.node_indices().collect();
        let mut labels: HashMap<NodeIndex, usize> = HashMap::new();
        
        // Initialize each node with its own label
        for (i, &node) in node_indices.iter().enumerate() {
            labels.insert(node, i);
        }

        let max_iterations = 100;
        let mut changed = true;
        let mut iteration = 0;

        while changed && iteration < max_iterations {
            changed = false;
            
            // Randomize order to avoid bias
            let mut shuffled_nodes = node_indices.clone();
            // In a real implementation, would use proper randomization
            // shuffled_nodes.shuffle(&mut thread_rng());
            
            for &node in &shuffled_nodes {
                // Count labels of neighbors
                let mut neighbor_labels: HashMap<usize, f64> = HashMap::new();
                
                for edge in graph.edges(node) {
                    let neighbor = edge.target();
                    if let Some(&neighbor_label) = labels.get(&neighbor) {
                        *neighbor_labels.entry(neighbor_label).or_insert(0.0) += edge.weight().weight as f64;
                    }
                }
                
                // Find most frequent label
                if let Some((&new_label, _)) = neighbor_labels.iter().max_by(|a, b| a.1.partial_cmp(b.1).unwrap()) {
                    if labels.get(&node) != Some(&new_label) {
                        labels.insert(node, new_label);
                        changed = true;
                    }
                }
            }
            
            iteration += 1;
        }

        // Group nodes by label
        let mut communities: HashMap<usize, Vec<NodeIndex>> = HashMap::new();
        for (&node, &label) in &labels {
            communities.entry(label).or_insert_with(Vec::new).push(node);
        }

        Ok(communities.into_values().collect())
    }

    /// Refine partition using Kernighan-Lin algorithm
    fn refine_partition_with_kl(
        &self,
        graph: &DependencyGraph,
        mut communities: Vec<Vec<NodeIndex>>,
        target_k: usize,
    ) -> Result<Vec<Vec<NodeIndex>>> {
        // Merge or split communities to reach target k
        while communities.len() > target_k {
            // Merge smallest communities
            communities.sort_by_key(|c| c.len());
            let smallest = communities.remove(0);
            let second_smallest = communities.remove(0);
            let mut merged = smallest;
            merged.extend(second_smallest);
            communities.push(merged);
        }

        while communities.len() < target_k {
            // Split largest community
            communities.sort_by_key(|c| c.len());
            let largest = communities.pop().unwrap();
            if largest.len() >= self.config.partitioning.min_clusters {
                let mid = largest.len() / 2;
                let (first_half, second_half) = largest.split_at(mid);
                communities.push(first_half.to_vec());
                communities.push(second_half.to_vec());
            } else {
                communities.push(largest);
                break;
            }
        }

        // Apply Kernighan-Lin refinement
        self.kernighan_lin_refinement(graph, communities)
    }

    /// Kernighan-Lin refinement algorithm
    fn kernighan_lin_refinement(
        &self,
        graph: &DependencyGraph,
        mut communities: Vec<Vec<NodeIndex>>,
    ) -> Result<Vec<Vec<NodeIndex>>> {
        let max_iterations = 10;
        let mut improved = true;
        let mut iteration = 0;

        while improved && iteration < max_iterations {
            improved = false;
            
            // Try to improve each pair of communities
            for i in 0..communities.len() {
                for j in i + 1..communities.len() {
                    let _initial_cost = self.calculate_partition_cost(graph, &communities);
                    
                    // Try swapping nodes between communities i and j
                    if let Some((best_swap, cost_improvement)) = 
                        self.find_best_node_swap(graph, &communities[i], &communities[j]) {
                        
                        if cost_improvement > 0.0 {
                            // Apply the swap
                            let (from_comm, _to_comm, node) = best_swap;
                            if from_comm == i {
                                communities[i].retain(|&n| n != node);
                                communities[j].push(node);
                            } else {
                                communities[j].retain(|&n| n != node);
                                communities[i].push(node);
                            }
                            improved = true;
                        }
                    }
                }
            }
            
            iteration += 1;
        }

        Ok(communities)
    }

    /// Calculate overall cost/cut of partition
    fn calculate_partition_cost(&self, graph: &DependencyGraph, communities: &[Vec<NodeIndex>]) -> f64 {
        let mut total_cut = 0.0;
        
        for i in 0..communities.len() {
            for j in i + 1..communities.len() {
                total_cut += self.calculate_cut_size(graph, &communities[i], &communities[j]) as f64;
            }
        }
        
        total_cut
    }

    /// Find best node swap between two communities
    fn find_best_node_swap(
        &self,
        graph: &DependencyGraph,
        comm1: &[NodeIndex],
        comm2: &[NodeIndex],
    ) -> Option<((usize, usize, NodeIndex), f64)> {
        let mut best_swap = None;
        let mut best_improvement = 0.0;
        
        // Try moving each node from comm1 to comm2
        for &node in comm1 {
            let improvement = self.calculate_swap_improvement(graph, node, comm1, comm2);
            if improvement > best_improvement {
                best_improvement = improvement;
                best_swap = Some((0, 1, node));
            }
        }
        
        // Try moving each node from comm2 to comm1
        for &node in comm2 {
            let improvement = self.calculate_swap_improvement(graph, node, comm2, comm1);
            if improvement > best_improvement {
                best_improvement = improvement;
                best_swap = Some((1, 0, node));
            }
        }
        
        best_swap.map(|swap| (swap, best_improvement))
    }

    /// Calculate improvement from swapping a node between communities
    fn calculate_swap_improvement(
        &self,
        graph: &DependencyGraph,
        node: NodeIndex,
        from_comm: &[NodeIndex],
        to_comm: &[NodeIndex],
    ) -> f64 {
        let from_set: HashSet<_> = from_comm.iter().copied().collect();
        let to_set: HashSet<_> = to_comm.iter().copied().collect();
        
        let mut internal_edges_lost = 0;
        let mut external_edges_gained = 0;
        
        for edge in graph.edges(node) {
            let neighbor = edge.target();
            let weight = edge.weight().weight;
            
            if from_set.contains(&neighbor) {
                // Losing internal edge in from_comm
                internal_edges_lost += weight;
            } else if to_set.contains(&neighbor) {
                // Gaining internal edge in to_comm
                external_edges_gained += weight;
            }
        }
        
        // Improvement = edges gained internally - edges lost internally
        (external_edges_gained as f64) - (internal_edges_lost as f64)
    }

    /// Convert graph communities to directory partitions
    fn communities_to_partitions(
        &self,
        graph: &DependencyGraph,
        communities: Vec<Vec<NodeIndex>>,
        k: usize,
    ) -> Result<Vec<DirectoryPartition>> {
        let mut partitions = Vec::new();
        
        for (i, community) in communities.into_iter().take(k).enumerate() {
            let mut files = Vec::new();
            let mut total_loc = 0;
            
            for node_idx in community {
                if let Some(file_node) = graph.node_weight(node_idx) {
                    // Ensure we store the complete absolute path
                    let complete_path = if file_node.path.is_absolute() {
                        file_node.path.clone()
                    } else {
                        std::env::current_dir().unwrap_or_default().join(&file_node.path)
                    };
                    files.push(complete_path);
                    total_loc += file_node.loc;
                }
            }
            
            // Generate deterministic name for partition
            let name = self.generate_partition_name(&files, i);
            
            partitions.push(DirectoryPartition {
                name,
                files,
                loc: total_loc,
            });
        }
        
        Ok(partitions)
    }

    /// Generate deterministic partition name based on file paths
    fn generate_partition_name(&self, files: &[PathBuf], index: usize) -> String {
        // Extract common tokens from file paths
        let mut token_counts: HashMap<String, usize> = HashMap::new();
        
        for file_path in files {
            if let Some(stem) = file_path.file_stem().and_then(|s| s.to_str()) {
                // Split on common separators and count tokens
                for token in stem.split(['_', '-', '.']) {
                    let token = token.to_lowercase();
                    if token.len() > 2 && !token.chars().all(|c| c.is_ascii_digit()) {
                        *token_counts.entry(token).or_insert(0) += 1;
                    }
                }
            }
        }
        
        // Find most common meaningful token
        if let Some((best_token, _)) = token_counts.iter()
            .filter(|(token, &count)| count > 1 && !["file", "test", "spec"].contains(&token.as_str()))
            .max_by_key(|(_, &count)| count) {
            return best_token.clone();
        }
        
        // Fall back to predefined names
        self.config.partitioning.naming_fallbacks
            .get(index)
            .cloned()
            .unwrap_or_else(|| format!("partition_{}", index))
    }

    /// Calculate expected gains from reorganization
    fn calculate_reorganization_gain(
        &self, 
        current_metrics: &DirectoryMetrics,
        partitions: &[DirectoryPartition],
        dir_path: &Path
    ) -> Result<ReorganizationGain> {
        // Calculate imbalance for each proposed partition
        let mut partition_imbalances = Vec::new();
        
        for partition in partitions {
            // Create a temporary directory metrics for this partition
            let partition_files = partition.files.len();
            let _partition_subdirs = 0; // New partitions start with 0 subdirs
            let partition_loc = partition.loc;
            
            // Simulate LOC distribution within partition (simplified)
            let avg_loc_per_file = if partition_files > 0 {
                partition_loc / partition_files
            } else {
                0
            };
            let loc_distribution: Vec<usize> = (0..partition_files)
                .map(|_| avg_loc_per_file)
                .collect();
            
            // Calculate metrics for this partition
            let gini = self.calculate_gini_coefficient(&loc_distribution);
            let entropy = self.calculate_entropy(&loc_distribution);
            
            // Calculate pressure metrics
            let file_pressure = (partition_files as f64 / self.config.fsdir.max_files_per_dir as f64).min(1.0);
            let branch_pressure = 0.0; // No subdirs in new partition
            let size_pressure = (partition_loc as f64 / self.config.fsdir.max_dir_loc as f64).min(1.0);
            
            // Calculate dispersion
            let max_entropy = if partition_files > 0 { (partition_files as f64).log2() } else { 1.0 };
            let normalized_entropy = if max_entropy > 0.0 { entropy / max_entropy } else { 0.0 };
            let dispersion = gini.max(1.0 - normalized_entropy);
            
            // Apply size normalization
            let size_normalization_factor = self.calculate_size_normalization_factor(partition_files, partition_loc);
            
            // Calculate imbalance for this partition
            let raw_imbalance = 0.35 * file_pressure + 
                               0.25 * branch_pressure + 
                               0.25 * size_pressure + 
                               0.15 * dispersion;
            
            let partition_imbalance = raw_imbalance * size_normalization_factor;
            partition_imbalances.push(partition_imbalance);
        }
        
        // Calculate average imbalance of new partitions
        let avg_new_imbalance = if !partition_imbalances.is_empty() {
            partition_imbalances.iter().sum::<f64>() / partition_imbalances.len() as f64
        } else {
            current_metrics.imbalance
        };
        
        // Imbalance improvement (positive means improvement)
        let imbalance_delta = (current_metrics.imbalance - avg_new_imbalance).max(0.0);
        
        // Calculate cross-edges reduced by analyzing dependency graph
        let cross_edges_reduced = self.estimate_cross_edges_reduced(partitions, dir_path)?;
        
        Ok(ReorganizationGain {
            imbalance_delta,
            cross_edges_reduced,
        })
    }
    
    /// Estimate how many cross-partition edges would be reduced
    fn estimate_cross_edges_reduced(&self, partitions: &[DirectoryPartition], dir_path: &Path) -> Result<usize> {
        // Build dependency graph to analyze edge cuts
        let dependency_graph = self.build_dependency_graph(dir_path)?;
        
        // Create partition mapping
        let mut file_to_partition: HashMap<PathBuf, usize> = HashMap::new();
        for (partition_idx, partition) in partitions.iter().enumerate() {
            for file_path in &partition.files {
                file_to_partition.insert(file_path.clone(), partition_idx);
            }
        }
        
        // Count edges that would cross partition boundaries
        let mut cross_edges = 0;
        let mut _total_internal_edges = 0;
        
        for edge_idx in dependency_graph.edge_indices() {
            if let Some((source, target)) = dependency_graph.edge_endpoints(edge_idx) {
                if let (Some(source_node), Some(target_node)) = 
                    (dependency_graph.node_weight(source), dependency_graph.node_weight(target)) {
                    
                    _total_internal_edges += 1;
                    
                    // Check if this edge would cross partition boundaries
                    if let (Some(&source_partition), Some(&target_partition)) = 
                        (file_to_partition.get(&source_node.path), file_to_partition.get(&target_node.path)) {
                        if source_partition != target_partition {
                            cross_edges += 1;
                        }
                    }
                }
            }
        }
        
        // Return the number of edges that would be cut (became cross-partition)
        Ok(cross_edges)
    }

    /// Calculate effort required for reorganization
    fn calculate_reorganization_effort(
        &self,
        partitions: &[DirectoryPartition],
        _dir_path: &Path
    ) -> Result<ReorganizationEffort> {
        let files_moved = partitions.iter()
            .map(|p| p.files.len())
            .sum();
            
        // Estimate import updates (would need real dependency analysis)
        let import_updates_est = files_moved * 2; // Placeholder estimation
        
        Ok(ReorganizationEffort {
            files_moved,
            import_updates_est,
        })
    }

    /// Generate reorganization rules based on language/framework
    fn generate_reorganization_rules(&self, _dir_path: &Path) -> Vec<String> {
        vec![
            "preserve package APIs via re-exports where public".to_string(),
            "maintain import compatibility through __init__.py updates".to_string(),
            "update module references in affected files".to_string(),
        ]
    }
    
    /// Generate file move operations for reorganization
    fn generate_file_moves(&self, partitions: &[DirectoryPartition], dir_path: &Path) -> Result<Vec<FileMove>> {
        let mut file_moves = Vec::new();
        
        for partition in partitions {
            for file_path in &partition.files {
                // Calculate destination path within new partition subdirectory
                let file_name = file_path.file_name()
                    .ok_or_else(|| ValknutError::Internal {
                        message: format!("Invalid file path: {:?}", file_path),
                        context: None,
                    })?;
                
                let destination = dir_path.join(&partition.name).join(file_name);
                
                file_moves.push(FileMove {
                    from: file_path.clone(),
                    to: destination,
                });
            }
        }
        
        Ok(file_moves)
    }
    
    /// Calculate size normalization factor to prevent bias against larger codebases
    fn calculate_size_normalization_factor(&self, files: usize, total_loc: usize) -> f64 {
        // Normalization approach: smaller projects get less strict evaluation
        // This prevents larger, well-organized projects from being unfairly penalized
        
        // Base normalization factors
        let file_norm = if files > 0 {
            // Logarithmic scaling for file count - larger projects get some relief
            1.0 - (0.1 * (files as f64 / self.config.fsdir.max_files_per_dir as f64).ln().max(0.0))
        } else {
            1.0
        };
        
        let loc_norm = if total_loc > 0 {
            // Square root scaling for LOC - diminishing penalty for size
            1.0 - (0.15 * (total_loc as f64 / self.config.fsdir.max_dir_loc as f64).sqrt().min(1.0))
        } else {
            1.0
        };
        
        // Combine normalization factors with reasonable bounds
        let combined_norm = (file_norm * loc_norm).clamp(0.7, 1.0);
        
        // For small projects (under thresholds), be more lenient
        if files <= 10 && total_loc <= 1000 {
            combined_norm * 0.8  // 20% more lenient for small projects
        } else {
            combined_norm
        }
    }
    
    /// Extract import statements from a code file
    fn extract_imports(&self, file_path: &Path) -> Result<Vec<ImportStatement>> {
        let content = std::fs::read_to_string(file_path)?;
        let mut imports = Vec::new();
        
        if let Some(ext) = file_path.extension().and_then(|e| e.to_str()) {
            match ext {
                "py" => {
                    // Python import parsing
                    for line in content.lines() {
                        let trimmed = line.trim();
                        if trimmed.starts_with("import ") {
                            if let Some(module) = trimmed.strip_prefix("import ").map(|s| s.split_whitespace().next().unwrap_or("")) {
                                imports.push(ImportStatement {
                                    module_path: module.to_string(),
                                    import_type: "direct".to_string(),
                                });
                            }
                        } else if trimmed.starts_with("from ") {
                            if let Some(rest) = trimmed.strip_prefix("from ") {
                                if let Some(module) = rest.split(" import ").next() {
                                    imports.push(ImportStatement {
                                        module_path: module.to_string(),
                                        import_type: "from".to_string(),
                                    });
                                }
                            }
                        }
                    }
                },
                "js" | "ts" | "jsx" | "tsx" => {
                    // JavaScript/TypeScript import parsing
                    for line in content.lines() {
                        let trimmed = line.trim();
                        if trimmed.starts_with("import ") {
                            // Parse various import patterns
                            if let Some(from_part) = trimmed.rfind(" from ") {
                                let module_part = &trimmed[from_part + 6..].trim_matches([' ', '"', '\'', ';']);
                                imports.push(ImportStatement {
                                    module_path: module_part.to_string(),
                                    import_type: "es6".to_string(),
                                });
                            }
                        } else if trimmed.starts_with("const ") && trimmed.contains("require(") {
                            // CommonJS require parsing
                            if let Some(start) = trimmed.find("require(") {
                                let start = start + 8;
                                if let Some(end) = trimmed[start..].find(")") {
                                    let module = &trimmed[start..start + end].trim_matches([' ', '"', '\'']);
                                    imports.push(ImportStatement {
                                        module_path: module.to_string(),
                                        import_type: "require".to_string(),
                                    });
                                }
                            }
                        }
                    }
                },
                "rs" => {
                    // Rust use statements
                    for line in content.lines() {
                        let trimmed = line.trim();
                        if trimmed.starts_with("use ") && trimmed.ends_with(";") {
                            if let Some(module) = trimmed.strip_prefix("use ").and_then(|s| s.strip_suffix(";")) {
                                // Extract the module part (before ::)
                                let module_path = module.split("::").next().unwrap_or(module);
                                if !module_path.starts_with("std::") && !module_path.starts_with("crate::") {
                                    imports.push(ImportStatement {
                                        module_path: module_path.to_string(),
                                        import_type: "use".to_string(),
                                    });
                                }
                            }
                        }
                    }
                },
                _ => {}
            }
        }
        
        Ok(imports)
    }
    
    /// Resolve an import statement to a local file path within the directory
    fn resolve_import_to_local_file(&self, import: &ImportStatement, dir_path: &Path) -> Option<PathBuf> {
        // Simple resolution logic - in production this would be more sophisticated
        let module_path = &import.module_path;
        
        // Try different file extensions
        let extensions = ["py", "js", "ts", "jsx", "tsx", "rs"];
        
        for ext in &extensions {
            // Try as direct file
            let candidate = dir_path.join(format!("{}.{}", module_path, ext));
            if candidate.exists() {
                return Some(candidate);
            }
            
            // Try with common transformations
            let snake_case = module_path.replace('-', "_");
            let candidate = dir_path.join(format!("{}.{}", snake_case, ext));
            if candidate.exists() {
                return Some(candidate);
            }
            
            // Try as relative import
            if module_path.starts_with("./") || module_path.starts_with("../") {
                let relative_path = PathBuf::from(module_path);
                let candidate = dir_path.join(relative_path).with_extension(ext);
                if candidate.exists() {
                    return Some(candidate);
                }
            }
        }
        
        None
    }
    
    /// Extract entities (functions, classes) from Python code
    fn extract_python_entities(&self, content: &str) -> Result<Vec<EntityNode>> {
        let mut entities = Vec::new();
        let mut current_entity: Option<EntityNode> = None;
        let mut current_symbols = HashSet::new();
        let mut _line_count = 0;
        let mut indent_level = 0;
        let mut in_entity = false;
        let mut class_indent_level = 0;
        let mut in_class = false;
        
        for line in content.lines() {
            _line_count += 1;
            let trimmed = line.trim();
            let current_indent = line.len() - line.trim_start().len();
            
            // Detect function or class definitions
            if (trimmed.starts_with("def ") || trimmed.starts_with("class ")) && !trimmed.is_empty() {
                // Check if this is a method inside a class
                if in_class && trimmed.starts_with("def ") && current_indent > class_indent_level {
                    // This is a method inside the current class - don't create separate entity
                    // Just extract symbols and add to current class
                    if let Some(ref mut entity) = current_entity {
                        entity.loc += 1;
                        self.extract_symbols_from_line(trimmed, &mut current_symbols);
                    }
                } else {
                    // Save previous entity if exists
                    if let Some(mut entity) = current_entity.take() {
                        entity.symbols = current_symbols.clone();
                        entities.push(entity);
                    }
                    
                    // Start new entity
                    let (entity_type, name) = if trimmed.starts_with("def ") {
                        let name = trimmed[4..].split('(').next().unwrap_or("").trim();
                        ("function", name)
                    } else {
                        let name = trimmed[6..].split(':').next().unwrap_or("").split('(').next().unwrap_or("").trim();
                        ("class", name)
                    };
                    
                    current_entity = Some(EntityNode {
                        name: name.to_string(),
                        entity_type: entity_type.to_string(),
                        loc: 1, // Count the definition line itself
                        symbols: HashSet::new(),
                    });
                    current_symbols.clear();
                    indent_level = current_indent;
                    in_entity = true;
                    
                    // Track if this is a class
                    if trimmed.starts_with("class ") {
                        in_class = true;
                        class_indent_level = current_indent;
                    } else {
                        in_class = false;
                    }
                    
                    // Extract symbols from the definition line
                    self.extract_symbols_from_line(trimmed, &mut current_symbols);
                }
            } else if in_entity {
                if let Some(ref mut entity) = current_entity {
                // Check if we're still inside the current entity
                if trimmed.is_empty() || trimmed.starts_with("#") {
                    // Empty lines and comments - still in entity
                    if !trimmed.is_empty() {
                        entity.loc += 1;
                    }
                } else if current_indent > indent_level {
                    // Indented line - still in entity
                    entity.loc += 1;
                    self.extract_symbols_from_line(trimmed, &mut current_symbols);
                } else if current_indent == indent_level && (trimmed.starts_with("def ") || trimmed.starts_with("class ")) {
                    // New entity at same level - end current entity, but don't process this line yet
                    entity.symbols = current_symbols.clone();
                    entities.push(current_entity.take().unwrap());
                    in_entity = false;
                    in_class = false;
                    current_symbols.clear();
                    // Re-process this line (it will be caught by the first condition)
                    continue;
                } else if current_indent <= indent_level {
                    // Dedented line - end of current entity
                    entity.symbols = current_symbols.clone();
                    entities.push(current_entity.take().unwrap());
                    in_entity = false;
                    in_class = false;
                    current_symbols.clear();
                }
                }
            }
        }
        
        // Don't forget the last entity
        if let Some(mut entity) = current_entity {
            entity.symbols = current_symbols;
            entities.push(entity);
        }
        
        Ok(entities)
    }
    
    /// Extract entities from JavaScript/TypeScript code
    fn extract_javascript_entities(&self, content: &str) -> Result<Vec<EntityNode>> {
        let mut entities = Vec::new();
        let mut current_entity: Option<EntityNode> = None;
        let mut current_symbols = HashSet::new();
        let mut brace_level = 0;
        let mut in_entity = false;
        
        for line in content.lines() {
            let trimmed = line.trim();
            
            // Detect function or class definitions
            if trimmed.starts_with("function ") || trimmed.starts_with("class ") || 
               trimmed.contains("= function") || trimmed.contains("=> ") {
                
                // Save previous entity
                if let Some(mut entity) = current_entity.take() {
                    entity.symbols = current_symbols.clone();
                    entities.push(entity);
                }
                
                let (entity_type, name) = if trimmed.starts_with("function ") {
                    let name = trimmed[9..].split('(').next().unwrap_or("").trim();
                    ("function", name)
                } else if trimmed.starts_with("class ") {
                    let name = trimmed[6..].split(' ').next().unwrap_or("").trim();
                    ("class", name)
                } else if let Some(eq_pos) = trimmed.find('=') {
                    let name = trimmed[..eq_pos].trim();
                    ("function", name)
                } else {
                    ("function", "anonymous")
                };
                
                current_entity = Some(EntityNode {
                    name: name.to_string(),
                    entity_type: entity_type.to_string(),
                    loc: 0,
                    symbols: HashSet::new(),
                });
                current_symbols.clear();
                brace_level = 0;
                in_entity = true;
            }
            
            if in_entity {
                if let Some(ref mut entity) = current_entity {
                    entity.loc += 1;
                    self.extract_symbols_from_line(trimmed, &mut current_symbols);
                    
                    // Track brace level to detect end of entity
                    brace_level += trimmed.matches('{').count() as i32;
                    brace_level -= trimmed.matches('}').count() as i32;
                    
                    if brace_level < 0 {
                        in_entity = false;
                        entity.symbols = current_symbols.clone();
                        entities.push(current_entity.take().unwrap());
                        current_symbols.clear();
                    }
                }
            }
        }
        
        // Handle the last entity
        if let Some(mut entity) = current_entity {
            entity.symbols = current_symbols;
            entities.push(entity);
        }
        
        Ok(entities)
    }
    
    /// Extract entities from Rust code
    fn extract_rust_entities(&self, content: &str) -> Result<Vec<EntityNode>> {
        let mut entities = Vec::new();
        let mut current_entity: Option<EntityNode> = None;
        let mut current_symbols = HashSet::new();
        let mut brace_level = 0;
        let mut in_entity = false;
        
        for line in content.lines() {
            let trimmed = line.trim();
            
            // Detect function, struct, enum, or impl definitions
            if trimmed.starts_with("fn ") || trimmed.starts_with("pub fn ") ||
               trimmed.starts_with("struct ") || trimmed.starts_with("pub struct ") ||
               trimmed.starts_with("enum ") || trimmed.starts_with("pub enum ") ||
               trimmed.starts_with("impl ") {
                
                // Save previous entity
                if let Some(mut entity) = current_entity.take() {
                    entity.symbols = current_symbols.clone();
                    entities.push(entity);
                }
                
                let (entity_type, name) = if trimmed.contains("fn ") {
                    let start = if trimmed.starts_with("pub ") { 7 } else { 3 };
                    let name = trimmed[start..].split('(').next().unwrap_or("").trim();
                    ("function", name)
                } else if trimmed.contains("struct ") {
                    let start = if trimmed.starts_with("pub ") { 11 } else { 7 };
                    let name = trimmed[start..].split(' ').next().unwrap_or("").trim();
                    ("struct", name)
                } else if trimmed.contains("enum ") {
                    let start = if trimmed.starts_with("pub ") { 9 } else { 5 };
                    let name = trimmed[start..].split(' ').next().unwrap_or("").trim();
                    ("enum", name)
                } else {
                    let start = 5;
                    let name = trimmed[start..].split(' ').next().unwrap_or("").trim();
                    ("impl", name)
                };
                
                current_entity = Some(EntityNode {
                    name: name.to_string(),
                    entity_type: entity_type.to_string(),
                    loc: 0,
                    symbols: HashSet::new(),
                });
                current_symbols.clear();
                brace_level = 0;
                in_entity = true;
            }
            
            if in_entity {
                if let Some(ref mut entity) = current_entity {
                    entity.loc += 1;
                    self.extract_symbols_from_line(trimmed, &mut current_symbols);
                    
                    // Track brace level
                    brace_level += trimmed.matches('{').count() as i32;
                    brace_level -= trimmed.matches('}').count() as i32;
                    
                    if brace_level < 0 {
                        in_entity = false;
                        entity.symbols = current_symbols.clone();
                        entities.push(current_entity.take().unwrap());
                        current_symbols.clear();
                    }
                }
            }
        }
        
        // Handle the last entity
        if let Some(mut entity) = current_entity {
            entity.symbols = current_symbols;
            entities.push(entity);
        }
        
        Ok(entities)
    }
    
    /// Extract symbols (identifiers) from a line of code
    fn extract_symbols_from_line(&self, line: &str, symbols: &mut HashSet<String>) {
        // Simple regex-like approach to extract identifiers
        let words: Vec<&str> = line.split(|c: char| !c.is_alphanumeric() && c != '_')
            .filter(|word| !word.is_empty() && word.len() > 1) // Reduced from 2 to 1
            .filter(|word| !word.chars().all(|c| c.is_ascii_digit()))
            .collect();
        
        for word in words {
            // Filter out common keywords but allow certain important ones like 'self'
            if !Self::is_keyword(word) || word == "self" {
                symbols.insert(word.to_string());
            }
        }
    }
    
    /// Check if a word is a programming language keyword
    fn is_keyword(word: &str) -> bool {
        matches!(word, 
            "def" | "class" | "function" | "var" | "let" | "const" | "if" | "else" | "for" | 
            "while" | "return" | "import" | "from" | "fn" | "struct" | "enum" | "impl" | 
            "pub" | "use" | "mod" | "true" | "false" | "null" | "undefined" | "this" | "self" |
            "and" | "or" | "not" | "in" | "is" | "as" | "with" | "try" | "except" | "finally"
        )
    }
    
    /// Calculate Jaccard similarity between two sets of symbols
    fn calculate_jaccard_similarity(&self, set_a: &HashSet<String>, set_b: &HashSet<String>) -> f64 {
        if set_a.is_empty() && set_b.is_empty() {
            return 1.0;
        }
        
        let intersection_size = set_a.intersection(set_b).count();
        let union_size = set_a.union(set_b).count();
        
        if union_size == 0 {
            0.0
        } else {
            intersection_size as f64 / union_size as f64
        }
    }

    /// Analyze file for split potential
    fn analyze_file_for_split(&self, file_path: &Path) -> Result<Option<FileSplitPack>> {
        let metadata = std::fs::metadata(file_path)?;
        let size_bytes = metadata.len() as usize;
        let loc = self.count_lines_of_code(file_path)?;
        
        // Check if file meets "huge" criteria
        let is_huge = loc >= self.config.fsfile.huge_loc || size_bytes >= self.config.fsfile.huge_bytes;
        
        if !is_huge {
            return Ok(None);
        }

        let mut reasons = Vec::new();
        
        if loc >= self.config.fsfile.huge_loc {
            reasons.push(format!("loc {} > {}", loc, self.config.fsfile.huge_loc));
        }
        
        if size_bytes >= self.config.fsfile.huge_bytes {
            reasons.push(format!("size {} bytes > {} bytes", size_bytes, self.config.fsfile.huge_bytes));
        }

        // Build entity cohesion graph
        let cohesion_graph = self.build_entity_cohesion_graph(file_path)?;
        let communities = self.find_cohesion_communities(&cohesion_graph)?;
        
        if communities.len() >= self.config.partitioning.min_clusters {
            reasons.push(format!("{} cohesion communities", communities.len()));
        } else {
            return Ok(None); // Not worth splitting
        }

        // Generate split suggestions
        let suggested_splits = self.generate_split_suggestions(file_path, &communities)?;
        
        // Calculate value and effort
        let value = self.calculate_split_value(loc, file_path)?;
        let effort = self.calculate_split_effort(file_path)?;

        let pack = FileSplitPack {
            kind: "file_split".to_string(),
            file: file_path.to_path_buf(),
            reasons,
            suggested_splits,
            value,
            effort,
        };

        Ok(Some(pack))
    }

    /// Build entity cohesion graph for file
    fn build_entity_cohesion_graph(&self, file_path: &Path) -> Result<CohesionGraph> {
        let mut graph = Graph::new_undirected();
        let content = std::fs::read_to_string(file_path)?;
        
        // Extract entities based on file type
        let entities = if let Some(ext) = file_path.extension().and_then(|e| e.to_str()) {
            match ext {
                "py" => self.extract_python_entities(&content)?,
                "js" | "ts" | "jsx" | "tsx" => self.extract_javascript_entities(&content)?,
                "rs" => self.extract_rust_entities(&content)?,
                _ => Vec::new(),
            }
        } else {
            Vec::new()
        };
        
        if entities.len() < 2 {
            return Ok(graph); // Need at least 2 entities for cohesion analysis
        }
        
        // Add entity nodes to graph
        let mut entity_nodes = Vec::new();
        for entity in entities {
            let node_idx = graph.add_node(entity);
            entity_nodes.push(node_idx);
        }
        
        // Calculate cohesion between all pairs of entities
        for i in 0..entity_nodes.len() {
            for j in i + 1..entity_nodes.len() {
                let entity_a = &graph[entity_nodes[i]];
                let entity_b = &graph[entity_nodes[j]];
                
                let jaccard_similarity = self.calculate_jaccard_similarity(&entity_a.symbols, &entity_b.symbols);
                
                // Only add edges for significant cohesion
                if jaccard_similarity > 0.1 {
                    let shared_symbols = entity_a.symbols.intersection(&entity_b.symbols).count();
                    let edge = CohesionEdge {
                        jaccard_similarity,
                        shared_symbols,
                    };
                    
                    graph.add_edge(entity_nodes[i], entity_nodes[j], edge);
                }
            }
        }
        
        Ok(graph)
    }

    /// Find cohesion communities in entity graph
    fn find_cohesion_communities(&self, graph: &CohesionGraph) -> Result<Vec<Vec<NodeIndex>>> {
        let node_indices: Vec<_> = graph.node_indices().collect();
        
        if node_indices.len() < 2 {
            return Ok(vec![node_indices]);
        }
        
        // Use a simple but effective community detection based on edge weights
        let mut communities: Vec<Vec<NodeIndex>> = Vec::new();
        let mut assigned_nodes = HashSet::new();
        
        // Start with the highest cohesion edges and build communities
        let mut edges: Vec<_> = graph.edge_indices()
            .map(|edge_idx| {
                let (source, target) = graph.edge_endpoints(edge_idx).unwrap();
                let weight = graph.edge_weight(edge_idx).unwrap();
                (edge_idx, source, target, weight.jaccard_similarity)
            })
            .collect();
        
        // Sort by cohesion strength (descending)
        edges.sort_by(|a, b| b.3.partial_cmp(&a.3).unwrap_or(std::cmp::Ordering::Equal));
        
        // Build communities greedily
        for (_, source, target, similarity) in edges {
            if similarity < 0.2 {
                break; // Stop at low similarity threshold
            }
            
            // Find existing communities for these nodes
            // Find existing communities for these nodes (separate borrows)
            let mut source_comm_idx = None;
            let mut target_comm_idx = None;
            
            for (idx, comm) in communities.iter().enumerate() {
                if comm.contains(&source) {
                    source_comm_idx = Some(idx);
                }
                if comm.contains(&target) {
                    target_comm_idx = Some(idx);
                }
            }
            
            match (source_comm_idx, target_comm_idx) {
                (Some(comm_idx), None) => {
                    if !assigned_nodes.contains(&target) {
                        communities[comm_idx].push(target);
                        assigned_nodes.insert(target);
                    }
                },
                (None, Some(comm_idx)) => {
                    if !assigned_nodes.contains(&source) {
                        communities[comm_idx].push(source);
                        assigned_nodes.insert(source);
                    }
                },
                (None, None) => {
                    // Create new community
                    let mut new_community = Vec::new();
                    if !assigned_nodes.contains(&source) {
                        new_community.push(source);
                        assigned_nodes.insert(source);
                    }
                    if !assigned_nodes.contains(&target) {
                        new_community.push(target);
                        assigned_nodes.insert(target);
                    }
                    if !new_community.is_empty() {
                        communities.push(new_community);
                    }
                },
                (Some(_), Some(_)) => {
                    // Both nodes already in communities - could merge but skip for simplicity
                }
            }
        }
        
        // Add any remaining nodes as singleton communities
        for node in node_indices {
            if !assigned_nodes.contains(&node) {
                communities.push(vec![node]);
            }
        }
        
        // Filter out communities that are too small to be meaningful
        communities.retain(|comm| comm.len() >= self.config.fsfile.min_entities_per_split);
        
        // Limit to reasonable number of communities (2-3 for splitting)
        communities.truncate(3);
        
        Ok(communities)
    }

    /// Generate split file suggestions
    fn generate_split_suggestions(
        &self,
        file_path: &Path,
        communities: &[Vec<NodeIndex>]
    ) -> Result<Vec<SuggestedSplit>> {
        let cohesion_graph = self.build_entity_cohesion_graph(file_path)?;
        
        let base_name = file_path.file_stem()
            .and_then(|s| s.to_str())
            .unwrap_or("file");
            
        let suffixes = ["_core", "_io", "_api"];
        let mut splits = Vec::new();
        
        for (community_idx, community) in communities.iter().enumerate().take(3) {
            let suffix = suffixes.get(community_idx).unwrap_or(&"_part");
            
            let mut entities = Vec::new();
            let mut total_loc = 0;
            
            // Extract entity information from the community
            for &node_idx in community {
                if let Some(entity) = cohesion_graph.node_weight(node_idx) {
                    entities.push(entity.name.clone());
                    total_loc += entity.loc;
                }
            }
            
            // Generate meaningful name based on entity analysis
            let split_name = self.generate_split_name(base_name, suffix, &entities, file_path);
            
            splits.push(SuggestedSplit {
                name: split_name,
                entities,
                loc: total_loc,
            });
        }
        
        // If no communities found, create default splits
        if splits.is_empty() {
            for (i, suffix) in suffixes.iter().enumerate().take(2) {
                splits.push(SuggestedSplit {
                    name: format!("{}{}.{}", base_name, suffix, 
                        file_path.extension().and_then(|e| e.to_str()).unwrap_or("py")),
                    entities: vec![format!("Entity{}", i + 1)],
                    loc: 400, // Rough estimate
                });
            }
        }
        
        Ok(splits)
    }
    
    /// Generate a meaningful name for a split file based on entity analysis
    fn generate_split_name(&self, base_name: &str, suffix: &str, entities: &[String], file_path: &Path) -> String {
        let extension = file_path.extension().and_then(|e| e.to_str()).unwrap_or("py");
        
        // Analyze entity names to suggest better suffixes
        let entity_analysis = self.analyze_entity_names(entities);
        
        let final_suffix = if !entity_analysis.is_empty() {
            entity_analysis
        } else {
            suffix.to_string()
        };
        
        format!("{}{}.{}", base_name, final_suffix, extension)
    }
    
    /// Analyze entity names to suggest appropriate suffixes
    fn analyze_entity_names(&self, entities: &[String]) -> String {
        let mut io_count = 0;
        let mut api_count = 0;
        let mut core_count = 0;
        let mut util_count = 0;
        
        for entity in entities {
            let lower_entity = entity.to_lowercase();
            
            if lower_entity.contains("read") || lower_entity.contains("write") ||
               lower_entity.contains("load") || lower_entity.contains("save") ||
               lower_entity.contains("file") || lower_entity.contains("io") {
                io_count += 1;
            } else if lower_entity.contains("api") || lower_entity.contains("endpoint") ||
                     lower_entity.contains("route") || lower_entity.contains("handler") ||
                     lower_entity.contains("controller") {
                api_count += 1;
            } else if lower_entity.contains("util") || lower_entity.contains("helper") ||
                     lower_entity.contains("tool") {
                util_count += 1;
            } else {
                core_count += 1;
            }
        }
        
        // Return the most appropriate suffix based on analysis
        if io_count > api_count && io_count > core_count && io_count > util_count {
            "_io".to_string()
        } else if api_count > core_count && api_count > util_count {
            "_api".to_string()
        } else if util_count > core_count {
            "_util".to_string()
        } else {
            "_core".to_string()
        }
    }

    /// Calculate value score for file splitting
    fn calculate_split_value(&self, loc: usize, _file_path: &Path) -> Result<SplitValue> {
        let size_factor = (loc as f64 / self.config.fsfile.huge_loc as f64).min(1.0);
        let cycle_factor = 0.0; // Placeholder - would check for participation in cycles
        let clone_factor = 0.0; // Placeholder - would check for clone mass
        
        let score = 0.6 * size_factor + 0.3 * cycle_factor + 0.1 * clone_factor;
        
        Ok(SplitValue { score })
    }

    /// Calculate effort required for file splitting
    fn calculate_split_effort(&self, _file_path: &Path) -> Result<SplitEffort> {
        // Placeholder - would analyze actual exports and external references
        Ok(SplitEffort {
            exports: 5,
            external_importers: 8,
        })
    }

    /// Discover directories to analyze
    async fn discover_directories(&self, root_path: &Path) -> Result<Vec<PathBuf>> {
        let mut directories = Vec::new();
        self.collect_directories_recursive(root_path, &mut directories)?;
        Ok(directories)
    }

    /// Recursively collect directories
    fn collect_directories_recursive(&self, path: &Path, directories: &mut Vec<PathBuf>) -> Result<()> {
        if self.should_skip_directory(path) {
            return Ok(());
        }

        directories.push(path.to_path_buf());
        
        for entry in std::fs::read_dir(path)? {
            let entry = entry?;
            let child_path = entry.path();
            
            if child_path.is_dir() {
                self.collect_directories_recursive(&child_path, directories)?;
            }
        }
        
        Ok(())
    }

    /// Check if directory should be skipped
    fn should_skip_directory(&self, path: &Path) -> bool {
        let path_str = path.to_string_lossy();
        
        // Skip common generated/build/dependency directories
        path_str.contains("node_modules") ||
        path_str.contains("__pycache__") ||
        path_str.contains("target") ||
        path_str.contains(".git") ||
        path_str.contains("build") ||
        path_str.contains("dist")
    }

    /// Discover large files to analyze
    async fn discover_large_files(&self, root_path: &Path) -> Result<Vec<PathBuf>> {
        let mut files = Vec::new();
        self.collect_large_files_recursive(root_path, &mut files)?;
        Ok(files)
    }

    /// Recursively collect large files
    fn collect_large_files_recursive(&self, path: &Path, files: &mut Vec<PathBuf>) -> Result<()> {
        if self.should_skip_directory(path) {
            return Ok(());
        }

        for entry in std::fs::read_dir(path)? {
            let entry = entry?;
            let child_path = entry.path();
            
            if child_path.is_dir() {
                self.collect_large_files_recursive(&child_path, files)?;
            } else if child_path.is_file() {
                if let Some(ext) = child_path.extension().and_then(|e| e.to_str()) {
                    if self.is_code_file(ext) {
                        let metadata = std::fs::metadata(&child_path)?;
                        let size_bytes = metadata.len() as usize;
                        
                        if size_bytes >= self.config.fsfile.huge_bytes {
                            files.push(child_path);
                        } else {
                            // Also check LOC for smaller files that might still be huge by line count
                            let loc = self.count_lines_of_code(&child_path)?;
                            if loc >= self.config.fsfile.huge_loc {
                                files.push(child_path);
                            }
                        }
                    }
                }
            }
        }
        
        Ok(())
    }
}

#[async_trait]
impl FeatureExtractor for StructureExtractor {
    fn name(&self) -> &str { 
        "structure" 
    }
    
    fn features(&self) -> &[FeatureDefinition] { 
        &self.features 
    }
    
    async fn extract(&self, entity: &CodeEntity, _context: &ExtractionContext) -> Result<HashMap<String, f64>> {
        let mut features = HashMap::new();
        
        // Extract directory-level features if entity represents a directory
        if let Some(dir_path) = std::path::Path::new(&entity.file_path).parent() {
            match self.calculate_directory_metrics(dir_path) {
                Ok(metrics) => {
                    features.insert("directory_imbalance".to_string(), metrics.imbalance);
                    features.insert("file_pressure".to_string(), metrics.file_pressure);
                    features.insert("loc_dispersion".to_string(), metrics.dispersion);
                    
                    // Calculate branch reorg value
                    if let Ok(Some(_pack)) = self.analyze_directory_for_reorg(dir_path) {
                        features.insert("branch_reorg_value".to_string(), 0.8); // Would use actual value
                    } else {
                        features.insert("branch_reorg_value".to_string(), 0.0);
                    }
                }
                Err(_) => {
                    // Insert default values on error
                    features.insert("directory_imbalance".to_string(), 0.0);
                    features.insert("file_pressure".to_string(), 0.0);
                    features.insert("loc_dispersion".to_string(), 0.0);
                    features.insert("branch_reorg_value".to_string(), 0.0);
                }
            }
        }
        
        // Extract file-level features
        if let Ok(Some(_pack)) = self.analyze_file_for_split(&std::path::Path::new(&entity.file_path)) {
            features.insert("file_split_value".to_string(), 0.7); // Would use actual value
        } else {
            features.insert("file_split_value".to_string(), 0.0);
        }
        
        Ok(features)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;
    use std::fs;
    use std::collections::HashSet;

    fn create_test_directory() -> TempDir {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();
        
        // Create test files with varying LOC and realistic content
        for i in 1..=50 {
            let content = format!(
                "# Test file {}\nimport os\ndef function_{}():\n    pass\n",
                i,
                i
            ).repeat(i);
            fs::write(root.join(format!("file_{}.py", i)), content).unwrap();
        }
        
        // Create subdirectories
        for i in 1..=5 {
            fs::create_dir(root.join(format!("subdir_{}", i))).unwrap();
        }
        
        temp_dir
    }
    
    fn create_large_file_for_splitting() -> TempDir {
        let temp_dir = TempDir::new().unwrap();
        let root = temp_dir.path();
        
        // Create a large file with multiple entities for splitting
        let large_content = r#"
def load_data(filename):
    with open(filename, 'r') as f:
        data = f.read()
        processed = process_data(data)
        return processed

def save_data(data, filename):
    with open(filename, 'w') as f:
        formatted = format_data(data)
        f.write(formatted)
        return True

class DataProcessor:
    def __init__(self):
        self.cache = {}
        self.stats = {}
    
    def process(self, data):
        if data in self.cache:
            return self.cache[data]
        result = self._compute(data)
        self.cache[data] = result
        return result
        
    def _compute(self, data):
        # Complex computation
        transformed = self.transform(data)
        validated = self.validate(transformed)
        return validated

class APIHandler:
    def __init__(self):
        self.router = {}
    
    def handle_request(self, request):
        endpoint = request.get('endpoint')
        if endpoint in self.router:
            return self.router[endpoint](request)
        return {'error': 'Not found'}
        
    def register_endpoint(self, path, handler):
        self.router[path] = handler
        "#.repeat(20); // Make it large enough
        
        fs::write(root.join("large_file.py"), large_content).unwrap();
        temp_dir
    }
    
    #[tokio::test]
    async fn test_file_splitting_with_complete_paths() {
        let temp_dir = create_large_file_for_splitting();
        let extractor = StructureExtractor::new();
        let large_file = temp_dir.path().join("large_file.py");
        
        // Test file split analysis
        if let Ok(Some(split_pack)) = extractor.analyze_file_for_split(&large_file) {
            // Verify the file path is complete and absolute
            assert!(split_pack.file.is_absolute());
            assert!(split_pack.file == large_file);
            
            // Verify suggested splits have complete information
            assert!(!split_pack.suggested_splits.is_empty());
            
            for split in &split_pack.suggested_splits {
                // Names should be meaningful
                assert!(!split.name.is_empty());
                assert!(split.name.contains("large_file"));
                
                // Entities should be extracted
                assert!(!split.entities.is_empty());
                
                // LOC should be reasonable
                assert!(split.loc > 0);
            }
            
            // Check reasons include file size information
            assert!(!split_pack.reasons.is_empty());
            assert!(split_pack.reasons.iter().any(|r| r.contains("loc") || r.contains("size")));
        }
    }
    
    #[test]
    fn test_branch_reorg_pack_serialization() {
        let temp_dir = create_test_directory();
        
        let pack = BranchReorgPack {
            kind: "branch_reorg".to_string(),
            dir: temp_dir.path().to_path_buf(),
            current: DirectoryMetrics {
                files: 50,
                subdirs: 5,
                loc: 10000,
                gini: 0.6,
                entropy: 3.2,
                file_pressure: 2.0,
                branch_pressure: 0.5,
                size_pressure: 5.0,
                dispersion: 0.7,
                imbalance: 0.85,
            },
            proposal: vec![
                DirectoryPartition {
                    name: "core".to_string(),
                    files: vec![temp_dir.path().join("file_1.py"), temp_dir.path().join("file_2.py")],
                    loc: 5000,
                },
                DirectoryPartition {
                    name: "io".to_string(),
                    files: vec![temp_dir.path().join("file_3.py")],
                    loc: 3000,
                },
            ],
            file_moves: vec![
                FileMove {
                    from: temp_dir.path().join("file_1.py"),
                    to: temp_dir.path().join("core").join("file_1.py"),
                },
            ],
            gain: ReorganizationGain {
                imbalance_delta: 0.27,
                cross_edges_reduced: 19,
            },
            effort: ReorganizationEffort {
                files_moved: 31,
                import_updates_est: 24,
            },
            rules: vec!["preserve package APIs via re-exports where public".to_string()],
        };
        
        // Test serialization to JSON
        let json_value = serde_json::to_value(&pack).unwrap();
        
        // Verify required fields are present
        assert!(json_value.get("kind").is_some());
        assert!(json_value.get("dir").is_some());
        assert!(json_value.get("current").is_some());
        assert!(json_value.get("proposal").is_some());
        assert!(json_value.get("file_moves").is_some()); // Critical fix #2
        assert!(json_value.get("gain").is_some());
        assert!(json_value.get("effort").is_some());
        
        // Verify file paths in proposal are complete
        if let Some(proposal) = json_value.get("proposal").and_then(|v| v.as_array()) {
            for partition in proposal {
                if let Some(files) = partition.get("files").and_then(|v| v.as_array()) {
                    for file in files {
                        if let Some(file_path) = file.as_str() {
                            assert!(file_path.starts_with("/") || file_path.contains(temp_dir.path().to_str().unwrap()));
                        }
                    }
                }
            }
        }
        
        // Verify file_moves have complete paths
        if let Some(file_moves) = json_value.get("file_moves").and_then(|v| v.as_array()) {
            for file_move in file_moves {
                if let Some(from) = file_move.get("from").and_then(|v| v.as_str()) {
                    assert!(from.starts_with("/") || from.contains(temp_dir.path().to_str().unwrap()));
                }
                if let Some(to) = file_move.get("to").and_then(|v| v.as_str()) {
                    assert!(to.starts_with("/") || to.contains(temp_dir.path().to_str().unwrap()));
                }
            }
        }
    }

    #[test]
    fn test_directory_metrics_calculation() {
        let temp_dir = create_test_directory();
        let extractor = StructureExtractor::new();
        
        let metrics = extractor.calculate_directory_metrics(temp_dir.path()).unwrap();
        
        assert_eq!(metrics.files, 50);
        assert_eq!(metrics.subdirs, 5);
        assert!(metrics.imbalance > 0.0);
        assert!(metrics.gini >= 0.0 && metrics.gini <= 1.0);
        assert!(metrics.entropy >= 0.0);
    }

    #[test]
    fn test_gini_coefficient_calculation() {
        let extractor = StructureExtractor::new();
        
        // Perfect equality should give 0
        let equal_values = vec![10, 10, 10, 10];
        let gini = extractor.calculate_gini_coefficient(&equal_values);
        assert!((gini - 0.0).abs() < 0.01);
        
        // Perfect inequality should give high value
        let unequal_values = vec![0, 0, 0, 100];
        let gini = extractor.calculate_gini_coefficient(&unequal_values);
        assert!(gini > 0.5);
    }

    #[test]
    fn test_entropy_calculation() {
        let extractor = StructureExtractor::new();
        
        // Equal distribution should give high entropy
        let equal_values = vec![25, 25, 25, 25];
        let entropy = extractor.calculate_entropy(&equal_values);
        assert!(entropy > 1.5);
        
        // Skewed distribution should give lower entropy
        let skewed_values = vec![90, 5, 3, 2];
        let entropy = extractor.calculate_entropy(&skewed_values);
        assert!(entropy < 1.5);
    }

    #[tokio::test]
    async fn test_structure_extraction() {
        let temp_dir = create_test_directory();
        let extractor = StructureExtractor::new();
        
        let entity = CodeEntity::new(
            "test_entity",
            "file",
            "test",
            temp_dir.path().join("file_1.py").to_string_lossy(),
        )
        .with_line_range(1, 10)
        .with_source_code("# Test content");
        
        use std::sync::Arc;
        let config = Arc::new(crate::core::config::ValknutConfig::default());
        let context = ExtractionContext::new(config, "python");
        let features = extractor.extract(&entity, &context).await.unwrap();
        
        assert!(features.contains_key("directory_imbalance"));
        assert!(features.contains_key("file_pressure"));
        assert!(features.contains_key("loc_dispersion"));
        assert!(features.contains_key("branch_reorg_value"));
        assert!(features.contains_key("file_split_value"));
    }

    #[tokio::test]
    async fn test_recommendation_generation() {
        let temp_dir = create_test_directory();
        let extractor = StructureExtractor::new();
        
        let recommendations = extractor.generate_recommendations(temp_dir.path()).await.unwrap();
        
        // Should generate some recommendations for the overcrowded test directory
        assert!(!recommendations.is_empty());
    }

    #[test]
    fn test_structure_config_validation() {
        let config = StructureConfig::default();
        assert!(config.partitioning.balance_tolerance > 0.0);
        assert!(config.partitioning.max_clusters >= 2);
        assert!(!config.partitioning.naming_fallbacks.is_empty());
    }
    
    #[test]
    fn test_size_normalization_factor() {
        let extractor = StructureExtractor::new();
        
        // Small projects should get more lenient evaluation
        let small_factor = extractor.calculate_size_normalization_factor(5, 500);
        
        // Large projects should get some relief but less lenient
        let large_factor = extractor.calculate_size_normalization_factor(100, 10000);
        
        // Medium projects as baseline
        let medium_factor = extractor.calculate_size_normalization_factor(25, 2000);
        
        // Small projects should have lower factors (more lenient)
        assert!(small_factor < medium_factor);
        // All factors should be reasonable
        assert!(small_factor >= 0.5 && small_factor <= 1.0);
        assert!(large_factor >= 0.7 && large_factor <= 1.0);
        assert!(medium_factor >= 0.8 && medium_factor <= 1.0);
    }
    
    #[test]
    fn test_complete_file_paths_in_partitions() {
        let temp_dir = create_test_directory();
        let extractor = StructureExtractor::new();
        
        // Create a mock dependency graph with absolute paths
        let mut graph = Graph::new();
        let file1_path = temp_dir.path().join("file_1.py");
        let file2_path = temp_dir.path().join("file_2.py");
        
        let node1 = graph.add_node(FileNode {
            path: file1_path.clone(),
            loc: 100,
            size_bytes: 2000,
        });
        
        let node2 = graph.add_node(FileNode {
            path: file2_path.clone(),
            loc: 150,
            size_bytes: 3000,
        });
        
        let communities = vec![vec![node1], vec![node2]];
        let partitions = extractor.communities_to_partitions(&graph, communities, 2).unwrap();
        
        // Verify all file paths are absolute and complete
        for partition in &partitions {
            for file_path in &partition.files {
                assert!(file_path.is_absolute());
                assert!(file_path.to_string_lossy().contains("file_"));
                assert!(file_path.exists());
            }
        }
    }
    
    #[test] 
    fn test_file_moves_generation() {
        let temp_dir = create_test_directory();
        let extractor = StructureExtractor::new();
        
        // Create mock partitions
        let partitions = vec![
            DirectoryPartition {
                name: "core".to_string(),
                files: vec![temp_dir.path().join("file_1.py"), temp_dir.path().join("file_2.py")],
                loc: 250,
            },
            DirectoryPartition {
                name: "util".to_string(), 
                files: vec![temp_dir.path().join("file_3.py")],
                loc: 150,
            },
        ];
        
        let file_moves = extractor.generate_file_moves(&partitions, temp_dir.path()).unwrap();
        
        // Verify file moves have complete paths
        assert_eq!(file_moves.len(), 3);
        for file_move in &file_moves {
            assert!(file_move.from.is_absolute());
            assert!(file_move.to.is_absolute());
            assert!(file_move.to.to_string_lossy().contains("core") || file_move.to.to_string_lossy().contains("util"));
        }
    }
    
    #[test]
    fn test_import_extraction_python() {
        let extractor = StructureExtractor::new();
        let temp_dir = TempDir::new().unwrap();
        
        let python_content = r#"
import os
from sys import argv
import numpy as np
from .local_module import helper
        "#;
        
        let py_file = temp_dir.path().join("test.py");
        fs::write(&py_file, python_content).unwrap();
        
        let imports = extractor.extract_imports(&py_file).unwrap();
        
        assert!(!imports.is_empty());
        assert!(imports.iter().any(|imp| imp.module_path == "os"));
        assert!(imports.iter().any(|imp| imp.module_path == "sys"));
        assert!(imports.iter().any(|imp| imp.module_path == "numpy"));
    }
    
    #[test]
    fn test_import_extraction_javascript() {
        let extractor = StructureExtractor::new();
        let temp_dir = TempDir::new().unwrap();
        
        let js_content = r#"
import { Component } from 'react';
const fs = require('fs');
import utils from './utils';
        "#;
        
        let js_file = temp_dir.path().join("test.js");
        fs::write(&js_file, js_content).unwrap();
        
        let imports = extractor.extract_imports(&js_file).unwrap();
        
        assert!(!imports.is_empty());
        assert!(imports.iter().any(|imp| imp.module_path == "react"));
        assert!(imports.iter().any(|imp| imp.module_path == "fs"));
        assert!(imports.iter().any(|imp| imp.module_path == "./utils"));
    }
    
    #[test]
    fn test_entity_extraction_python() {
        let extractor = StructureExtractor::new();
        
        let python_content = r#"
def calculate_total(items):
    total = 0
    for item in items:
        total += item.price
    return total

class OrderProcessor:
    def __init__(self):
        self.orders = []
        
    def process_order(self, order):
        validated_order = self.validate(order)
        return self.save_order(validated_order)
        "#;
        
        let entities = extractor.extract_python_entities(python_content).unwrap();
        
        assert!(entities.len() >= 2);
        assert!(entities.iter().any(|e| e.name == "calculate_total" && e.entity_type == "function"));
        assert!(entities.iter().any(|e| e.name == "OrderProcessor" && e.entity_type == "class"));
        
        // Debug: Print all entities to see what was extracted
        println!("Extracted entities: {:?}", entities.iter().map(|e| (&e.name, &e.entity_type, e.loc)).collect::<Vec<_>>());
        
        // Check that entities have symbols extracted
        if let Some(class_entity) = entities.iter().find(|e| e.name == "OrderProcessor") {
            println!("OrderProcessor symbols: {:?}", class_entity.symbols);
            assert!(!class_entity.symbols.is_empty(), "Class entity should have extracted symbols");
            // Check for some expected symbols that should be in a typical class
            let has_relevant_symbols = class_entity.symbols.contains("orders") || 
                class_entity.symbols.contains("order") ||
                class_entity.symbols.contains("self") ||
                class_entity.symbols.contains("validated_order") ||
                class_entity.symbols.contains("save_order");
            assert!(has_relevant_symbols, "Class should contain relevant symbols, found: {:?}", class_entity.symbols);
        } else {
            panic!("OrderProcessor class not found in entities: {:?}", entities.iter().map(|e| &e.name).collect::<Vec<_>>());
        }
    }
    
    #[test]
    fn test_jaccard_similarity_calculation() {
        let extractor = StructureExtractor::new();
        
        let set_a: HashSet<String> = ["func1", "var1", "helper"].iter().map(|s| s.to_string()).collect();
        let set_b: HashSet<String> = ["func1", "var2", "helper"].iter().map(|s| s.to_string()).collect();
        
        let similarity = extractor.calculate_jaccard_similarity(&set_a, &set_b);
        
        // Should be 2/4 = 0.5 (intersection: func1, helper; union: func1, var1, helper, var2)
        assert!((similarity - 0.5).abs() < 0.01);
    }
    
    #[test]
    fn test_imbalance_calculation_with_normalization() {
        let temp_dir = create_test_directory();
        let extractor = StructureExtractor::new();
        
        let metrics = extractor.calculate_directory_metrics(temp_dir.path()).unwrap();
        
        // Large directory should have imbalance but with normalization applied
        assert!(metrics.imbalance > 0.0);
        
        // Test that normalization factor was applied (should be < 1.0)
        let raw_imbalance = 0.35 * metrics.file_pressure + 
                           0.25 * metrics.branch_pressure + 
                           0.25 * metrics.size_pressure + 
                           0.15 * metrics.dispersion;
        
        // With normalization, the actual imbalance should be lower than raw calculation
        assert!(metrics.imbalance <= raw_imbalance);
    }
    
    #[tokio::test]
    async fn test_complete_workflow_with_paths() {
        let temp_dir = create_test_directory();
        let extractor = StructureExtractor::new();
        
        // Generate recommendations
        let recommendations = extractor.generate_recommendations(temp_dir.path()).await.unwrap();
        
        // Verify that any branch reorg packs include complete file paths
        for recommendation in &recommendations {
            if let Some(kind) = recommendation.get("kind").and_then(|v| v.as_str()) {
                if kind == "branch_reorg" {
                    if let Some(proposal) = recommendation.get("proposal").and_then(|v| v.as_array()) {
                        for partition in proposal {
                            if let Some(files) = partition.get("files").and_then(|v| v.as_array()) {
                                for file in files {
                                    if let Some(file_path) = file.as_str() {
                                        // File paths should be complete/absolute
                                        assert!(file_path.starts_with("/") || file_path.contains(temp_dir.path().to_str().unwrap()));
                                    }
                                }
                            }
                        }
                    }
                    
                    // Check for file_moves field
                    assert!(recommendation.get("file_moves").is_some());
                }
            }
        }
    }
}