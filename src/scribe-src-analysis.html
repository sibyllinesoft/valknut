<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Repository Analysis: Scribe Analysis</title>
    <script src="https://unpkg.com/lucide@latest/dist/umd/lucide.js"></script>
    <style>
        :root {
            --bg-primary: #1a1a1a;
            --bg-secondary: #2a2a2a;
            --bg-tertiary: #3a3a3a;
            --text-primary: #e5e5e5;
            --text-secondary: #b5b5b5;
            --text-muted: #888;
            --accent-primary: #4f9cf9;
            --accent-secondary: #7c3aed;
            --border-color: #404040;
            --hover-color: #333333;
            --code-bg: #252525;
        }
        
        * {
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Inter', sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: var(--bg-primary);
            color: var(--text-primary);
            font-size: 14px;
        }
        
        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: var(--bg-secondary);
            border-radius: 12px;
            border: 1px solid var(--border-color);
            overflow: hidden;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
        }
        
        .header {
            background: rgba(255, 255, 255, 0.03);
            backdrop-filter: blur(20px);
            -webkit-backdrop-filter: blur(20px);
            border: 1px solid rgba(255, 255, 255, 0.08);
            border-bottom: 1px solid rgba(255, 255, 255, 0.02);
            color: white;
            padding: 32px;
            position: relative;
            overflow: hidden;
        }
        
        .header::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: 
                radial-gradient(circle at 20% 30%, rgba(255, 255, 255, 0.02) 0%, transparent 50%),
                radial-gradient(circle at 80% 70%, rgba(255, 255, 255, 0.01) 0%, transparent 50%);
            pointer-events: none;
        }
        
        .header::after {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: url("data:image/svg+xml,%3csvg width='40' height='40' viewBox='0 0 40 40' xmlns='http://www.w3.org/2000/svg'%3e%3cg fill='none' fill-rule='evenodd'%3e%3cg fill='%23ffffff' fill-opacity='0.02'%3e%3ccircle cx='20' cy='20' r='1'/%3e%3c/g%3e%3c/g%3e%3c/svg%3e");
            pointer-events: none;
        }
        
        .header h1 {
            margin: 0;
            font-size: 32px;
            font-weight: 700;
            display: flex;
            align-items: center;
            gap: 12px;
            position: relative;
            z-index: 1;
        }
        
        .header .meta {
            margin-top: 20px;
            opacity: 0.9;
            font-size: 13px;
            position: relative;
            z-index: 1;
            display: flex;
            justify-content: space-between;
            flex-wrap: wrap;
            gap: 16px;
        }
        
        .meta-item {
            display: flex;
            align-items: center;
            gap: 6px;
            background: rgba(255, 255, 255, 0.08);
            padding: 8px 12px;
            border-radius: 20px;
            border: 1px solid rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
            -webkit-backdrop-filter: blur(10px);
            transition: all 0.3s ease;
        }
        
        .meta-item:hover {
            background: rgba(255, 255, 255, 0.12);
            transform: translateY(-1px);
        }
        
        .stats {
            background: var(--bg-tertiary);
            padding: 24px;
            border-bottom: 1px solid var(--border-color);
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 24px;
        }
        
        .stat {
            text-align: center;
            padding: 20px;
            background: var(--bg-secondary);
            border-radius: 8px;
            border: 1px solid var(--border-color);
            transition: all 0.2s ease;
        }
        
        .stat:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
        }
        
        .stat-value {
            font-size: 28px;
            font-weight: 700;
            color: var(--accent-primary);
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 8px;
            margin-bottom: 8px;
        }
        
        .stat-label {
            font-size: 12px;
            text-transform: uppercase;
            color: var(--text-muted);
            letter-spacing: 0.5px;
            font-weight: 500;
        }
        
        .toc {
            background: var(--bg-tertiary);
            padding: 24px;
            border-bottom: 1px solid var(--border-color);
        }
        
        .toc h3 {
            margin: 0 0 20px 0;
            font-size: 18px;
            color: var(--text-primary);
            display: flex;
            align-items: center;
            gap: 8px;
            font-weight: 600;
        }
        
        .toc ul {
            margin: 0;
            padding: 0;
            list-style: none;
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 8px;
        }
        
        .toc li {
            margin: 0;
        }
        
        .toc a {
            color: var(--text-secondary);
            text-decoration: none;
            font-family: 'SF Mono', 'Monaco', 'Consolas', monospace;
            font-size: 13px;
            display: flex;
            align-items: center;
            gap: 8px;
            padding: 8px 12px;
            border-radius: 6px;
            transition: all 0.2s ease;
        }
        
        .toc a:hover {
            color: var(--accent-primary);
            background: var(--hover-color);
        }
        
        .file-list {
            max-height: 400px;
            overflow-y: auto;
            border-bottom: 1px solid var(--border-color);
            background: var(--bg-secondary);
        }
        
        .file-item {
            padding: 16px 24px;
            border-bottom: 1px solid var(--border-color);
            display: flex;
            justify-content: space-between;
            align-items: center;
            transition: all 0.2s ease;
        }
        
        .file-item:hover {
            background-color: var(--hover-color);
        }
        
        .file-item:last-child {
            border-bottom: none;
        }
        
        .file-name {
            font-family: 'SF Mono', 'Monaco', 'Consolas', monospace;
            font-size: 13px;
            color: var(--text-primary);
            display: flex;
            align-items: center;
            gap: 8px;
        }
        
        .file-meta {
            font-size: 12px;
            color: var(--text-muted);
        }
        
        .content {
            padding: 24px;
            background: var(--bg-secondary);
        }
        
        .file-section {
            margin-bottom: 32px;
            border: 1px solid var(--border-color);
            border-radius: 8px;
            overflow: hidden;
            background: var(--bg-primary);
        }
        
        .file-header {
            background: var(--bg-tertiary);
            padding: 16px 20px;
            border-bottom: 1px solid var(--border-color);
            font-family: 'SF Mono', 'Monaco', 'Consolas', monospace;
            font-weight: 600;
            font-size: 14px;
            color: var(--text-primary);
            display: flex;
            align-items: center;
            gap: 8px;
        }
        
        .file-content {
            max-height: 600px;
            overflow-y: auto;
            position: relative;
        }
        
        .file-content::-webkit-scrollbar {
            width: 8px;
        }
        
        .file-content::-webkit-scrollbar-track {
            background: var(--bg-secondary);
        }
        
        .file-content::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }
        
        .file-content::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
        
        pre {
            margin: 0;
            padding: 24px;
            background: var(--code-bg);
            font-family: 'SF Mono', 'Monaco', 'Consolas', monospace;
            font-size: 13px;
            line-height: 1.5;
            white-space: pre-wrap;
            word-wrap: break-word;
            color: var(--text-primary);
        }
        
        .icon {
            width: 16px;
            height: 16px;
        }
        
        .icon-lg {
            width: 20px;
            height: 20px;
        }

        /* Vanilla JavaScript Tree Styles */
        .tree-container {
            height: 400px;
            background: var(--bg-secondary);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            overflow-y: auto;
            padding: 8px;
        }

        .tree-node {
            display: flex;
            align-items: center;
            padding: 6px 8px;
            cursor: pointer;
            font-family: 'SF Mono', 'Monaco', 'Consolas', monospace;
            font-size: 13px;
            color: var(--text-secondary);
            transition: all 0.2s ease;
            user-select: none;
            border-radius: 4px;
            margin: 1px 0;
        }

        .tree-node:hover {
            background: var(--hover-color);
            color: var(--accent-primary);
        }

        .tree-node.selected {
            background: var(--accent-primary);
            color: white;
        }

        .tree-node-content {
            display: flex;
            align-items: center;
            gap: 6px;
            flex: 1;
            width: 100%;
        }

        .tree-arrow {
            width: 16px;
            height: 16px;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-right: 4px;
            transition: transform 0.2s ease;
            flex-shrink: 0;
            opacity: 0.6;
        }

        .tree-arrow.expanded {
            transform: rotate(90deg);
        }

        .tree-arrow.hidden {
            opacity: 0;
        }

        .tree-icon {
            width: 16px;
            height: 16px;
            flex-shrink: 0;
        }

        .tree-label {
            flex: 1;
            overflow: hidden;
            text-overflow: ellipsis;
            white-space: nowrap;
            min-width: 0;
        }

        .folder-icon {
            color: var(--accent-secondary);
        }

        .file-icon {
            color: var(--text-secondary);
        }

        .tree-children {
            margin-left: 20px;
            display: none;
        }

        .tree-children.expanded {
            display: block;
        }

        .tree-folder.expanded > .tree-children {
            display: block;
        }

        /* Scrollbar styling for tree */
        .tree-container::-webkit-scrollbar {
            width: 8px;
        }

        .tree-container::-webkit-scrollbar-track {
            background: var(--bg-tertiary);
        }

        .tree-container::-webkit-scrollbar-thumb {
            background: var(--border-color);
            border-radius: 4px;
        }

        .tree-container::-webkit-scrollbar-thumb:hover {
            background: var(--text-muted);
        }
        
        @media (max-width: 768px) {
            body {
                padding: 12px;
            }
            
            .header {
                padding: 20px;
            }
            
            .header h1 {
                font-size: 24px;
            }
            
            .header .meta {
                flex-direction: column;
                align-items: stretch;
                gap: 8px;
            }
            
            .meta-item {
                justify-content: center;
            }
            
            .stats {
                grid-template-columns: 1fr;
                gap: 16px;
                padding: 16px;
            }
            
            .toc ul {
                grid-template-columns: 1fr;
            }
            
            .content {
                padding: 16px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>
                🔍 Repository Analysis
            </h1>
            <div class="meta">
                <div class="meta-item">
                    <i data-lucide="cpu" class="icon"></i>
                    <span><strong>Algorithm:</strong> V5Integrated (Enhanced)</span>
                </div>
                <div class="meta-item">
                    <i data-lucide="clock" class="icon"></i>
                    <span><strong>Generated:</strong> 2025-09-15 04:22:57 UTC</span>
                </div>
                <div class="meta-item">
                    <i data-lucide="zap" class="icon"></i>
                    <span><strong>Selection Time:</strong> 93ms</span>
                </div>
            </div>
        </div>
        
        <div class="stats">
            <div class="stat">
                <div class="stat-value">
                    <i data-lucide="files" class="icon-lg"></i>
                    10
                </div>
                <div class="stat-label">Files Selected</div>
            </div>
            <div class="stat">
                <div class="stat-value">
                    <i data-lucide="hash" class="icon-lg"></i>
                    19,471
                </div>
                <div class="stat-label">Estimated Tokens</div>
            </div>
            <div class="stat">
                <div class="stat-value">
                    <i data-lucide="hard-drive" class="icon-lg"></i>
                    163.4 KB
                </div>
                <div class="stat-label">Total Size</div>
            </div>
            <div class="stat">
                <div class="stat-value">
                    <i data-lucide="target" class="icon-lg"></i>
                    15.6%
                </div>
                <div class="stat-label">Coverage</div>
            </div>
        </div>
        
        <div class="toc">
            <h3>
                <i data-lucide="folder-tree" class="icon"></i>
                File Explorer
            </h3>
            <div id="file-tree-container" class="tree-container"></div>
        </div>
        
        <div class="file-list">
            <div class="file-item">
                <span class="file-name"><i data-lucide="file-code" class="icon"></i>core/config.rs</span>
                <span class="file-meta">44.9 KB • ~5,469 tokens • Score: 1.61</span>
            </div>
            <div class="file-item">
                <span class="file-name"><i data-lucide="file-code" class="icon"></i>core/pipeline/mod.rs</span>
                <span class="file-meta">6.1 KB • ~501 tokens • Score: 1.56</span>
            </div>
            <div class="file-item">
                <span class="file-name"><i data-lucide="file-code" class="icon"></i>core/errors.rs</span>
                <span class="file-meta">20.7 KB • ~2,682 tokens • Score: 1.37</span>
            </div>
            <div class="file-item">
                <span class="file-name"><i data-lucide="file-code" class="icon"></i>detectors/structure/config.rs</span>
                <span class="file-meta">8.6 KB • ~1,361 tokens • Score: 1.21</span>
            </div>
            <div class="file-item">
                <span class="file-name"><i data-lucide="file-code" class="icon"></i>lang/python.rs</span>
                <span class="file-meta">25.4 KB • ~2,846 tokens • Score: 1.21</span>
            </div>
            <div class="file-item">
                <span class="file-name"><i data-lucide="file-code" class="icon"></i>core/pipeline/pipeline_config.rs</span>
                <span class="file-meta">6.7 KB • ~741 tokens • Score: 1.20</span>
            </div>
            <div class="file-item">
                <span class="file-name"><i data-lucide="file-code" class="icon"></i>core/scoring.rs</span>
                <span class="file-meta">32.4 KB • ~3,589 tokens • Score: 1.19</span>
            </div>
            <div class="file-item">
                <span class="file-name"><i data-lucide="file-code" class="icon"></i>core/file_utils.rs</span>
                <span class="file-meta">18.1 KB • ~2,176 tokens • Score: 1.12</span>
            </div>
            <div class="file-item">
                <span class="file-name"><i data-lucide="file-code" class="icon"></i>io/persistence.rs</span>
                <span class="file-meta">232 B • ~37 tokens • Score: 0.66</span>
            </div>
            <div class="file-item">
                <span class="file-name"><i data-lucide="file-code" class="icon"></i>bin/mcp/mod.rs</span>
                <span class="file-meta">352 B • ~69 tokens • Score: 0.64</span>
            </div>
        </div>
        
        <div class="content">
            <div class="file-section" id="file-1">
                <div class="file-header"><i data-lucide="file-code" class="icon"></i>core/config.rs</div>
                <div class="file-content">
                    <pre>//! Configuration types and management for valknut-rs.
//!
//! This module provides comprehensive configuration structures that mirror
//! the Python implementation while adding Rust-specific optimizations and
//! type safety guarantees.

use std::collections::HashMap;
use std::path::PathBuf;

use serde::{Deserialize, Serialize};
// Removed unused regex import

use crate::core::errors::{Result, ValknutError};
use crate::detectors::structure::StructureConfig;
// use crate::detectors::names::NamesConfig;

/// Main configuration for valknut analysis engine
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ValknutConfig {
    /// Analysis pipeline configuration
    pub analysis: AnalysisConfig,
    
    /// Scoring and normalization settings
    pub scoring: ScoringConfig,
    
    /// Graph analysis configuration
    pub graph: GraphConfig,
    
    /// LSH and similarity detection settings
    pub lsh: LshConfig,
    
    /// Enhanced duplicate detection configuration
    #[serde(default)]
    pub dedupe: DedupeConfig,
    
    /// Clone denoising configuration
    #[serde(default)]
    pub denoise: DenoiseConfig,
    
    /// Language-specific settings
    pub languages: HashMap&amp;lt;String, LanguageConfig&amp;gt;,
    
    /// I/O and persistence settings
    pub io: IoConfig,
    
    /// Performance and resource limits
    pub performance: PerformanceConfig,
    
    /// Structure analysis configuration
    pub structure: StructureConfig,
    
    /// Coverage analysis and file discovery configuration
    #[serde(default)]
    pub coverage: CoverageConfig,
    
    /// Live reachability analysis configuration
    #[serde(skip_serializing_if &#x3D; &amp;quot;Option::is_none&amp;quot;)]
    pub live_reach: Option&amp;lt;LiveReachConfig&amp;gt;,
    
    /// Code quality analysis configuration (simple pattern-based analysis)
    // pub names: NamesConfig,
    /// Placeholder to maintain serialization compatibility
    #[serde(skip)]
    pub _names_placeholder: Option&amp;lt;()&amp;gt;,
}

impl Default for ValknutConfig {
    fn default() -&amp;gt; Self {
        Self {
            analysis: AnalysisConfig::default(),
            scoring: ScoringConfig::default(),
            graph: GraphConfig::default(),
            lsh: LshConfig::default(),
            dedupe: DedupeConfig::default(),
            denoise: DenoiseConfig::default(),
            languages: Self::default_languages(),
            io: IoConfig::default(),
            performance: PerformanceConfig::default(),
            structure: StructureConfig::default(),
            coverage: CoverageConfig::default(),
            live_reach: None,
            // names: NamesConfig::default(),
            _names_placeholder: None,
        }
    }
}

impl ValknutConfig {
    /// Load configuration from a YAML file
    pub fn from_yaml_file(path: impl Into&amp;lt;PathBuf&amp;gt;) -&amp;gt; Result&amp;lt;Self&amp;gt; {
        let path &#x3D; path.into();
        let content &#x3D; std::fs::read_to_string(&amp;amp;path)
            .map_err(|e| ValknutError::io(format!(&amp;quot;Failed to read config file: {}&amp;quot;, path.display()), e))?;
        
        serde_yaml::from_str(&amp;amp;content).map_err(Into::into)
    }
    
    /// Save configuration to a YAML file
    pub fn to_yaml_file(&amp;amp;self, path: impl Into&amp;lt;PathBuf&amp;gt;) -&amp;gt; Result&amp;lt;()&amp;gt; {
        let path &#x3D; path.into();
        let content &#x3D; serde_yaml::to_string(self)?;
        std::fs::write(&amp;amp;path, content)
            .map_err(|e| ValknutError::io(format!(&amp;quot;Failed to write config file: {}&amp;quot;, path.display()), e))
    }
    
    /// Get default language configurations
    fn default_languages() -&amp;gt; HashMap&amp;lt;String, LanguageConfig&amp;gt; {
        let mut languages &#x3D; HashMap::new();
        
        languages.insert(&amp;quot;python&amp;quot;.to_string(), LanguageConfig {
            enabled: true,
            file_extensions: vec![&amp;quot;.py&amp;quot;.to_string(), &amp;quot;.pyi&amp;quot;.to_string()],
            tree_sitter_language: &amp;quot;python&amp;quot;.to_string(),
            max_file_size_mb: 10.0,
            complexity_threshold: 10.0,
            additional_settings: HashMap::new(),
        });
        
        languages.insert(&amp;quot;javascript&amp;quot;.to_string(), LanguageConfig {
            enabled: true,
            file_extensions: vec![&amp;quot;.js&amp;quot;.to_string(), &amp;quot;.mjs&amp;quot;.to_string(), &amp;quot;.jsx&amp;quot;.to_string()],
            tree_sitter_language: &amp;quot;javascript&amp;quot;.to_string(),
            max_file_size_mb: 5.0,
            complexity_threshold: 10.0,
            additional_settings: HashMap::new(),
        });
        
        languages.insert(&amp;quot;typescript&amp;quot;.to_string(), LanguageConfig {
            enabled: true,
            file_extensions: vec![&amp;quot;.ts&amp;quot;.to_string(), &amp;quot;.tsx&amp;quot;.to_string(), &amp;quot;.d.ts&amp;quot;.to_string()],
            tree_sitter_language: &amp;quot;typescript&amp;quot;.to_string(),
            max_file_size_mb: 5.0,
            complexity_threshold: 10.0,
            additional_settings: HashMap::new(),
        });
        
        languages.insert(&amp;quot;rust&amp;quot;.to_string(), LanguageConfig {
            enabled: true,
            file_extensions: vec![&amp;quot;.rs&amp;quot;.to_string()],
            tree_sitter_language: &amp;quot;rust&amp;quot;.to_string(),
            max_file_size_mb: 10.0,
            complexity_threshold: 15.0,
            additional_settings: HashMap::new(),
        });
        
        languages.insert(&amp;quot;go&amp;quot;.to_string(), LanguageConfig {
            enabled: true,
            file_extensions: vec![&amp;quot;.go&amp;quot;.to_string()],
            tree_sitter_language: &amp;quot;go&amp;quot;.to_string(),
            max_file_size_mb: 8.0,
            complexity_threshold: 12.0,
            additional_settings: HashMap::new(),
        });
        
        languages
    }
    
    /// Validate configuration settings
    pub fn validate(&amp;amp;self) -&amp;gt; Result&amp;lt;()&amp;gt; {
        self.analysis.validate()?;
        self.scoring.validate()?;
        self.graph.validate()?;
        self.lsh.validate()?;
        self.performance.validate()?;
        // Structure config has built-in validation through Default implementation
        
        // Validate language configurations
        for (lang, config) in &amp;amp;self.languages {
            config.validate().map_err(|e| ValknutError::config_field(
                format!(&amp;quot;Invalid language configuration: {e}&amp;quot;), 
                format!(&amp;quot;languages.{lang}&amp;quot;)
            ))?;
        }
        
        // Validate dedupe configuration
        self.dedupe.validate()?;
        
        // Validate denoise configuration
        self.denoise.validate()?;
        
        // Validate coverage configuration
        self.coverage.validate()?;
        
        Ok(())
    }
}

/// Analysis pipeline configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalysisConfig {
    /// Enable scoring analysis
    pub enable_scoring: bool,
    
    /// Enable graph analysis
    pub enable_graph_analysis: bool,
    
    /// Enable LSH-based similarity detection
    pub enable_lsh_analysis: bool,
    
    /// Enable refactoring analysis
    pub enable_refactoring_analysis: bool,
    
    /// Enable coverage analysis
    pub enable_coverage_analysis: bool,
    
    /// Enable structure analysis
    pub enable_structure_analysis: bool,
    
    /// Enable code quality analysis
    pub enable_names_analysis: bool,
    
    /// Minimum confidence threshold for results
    pub confidence_threshold: f64,
    
    /// Maximum number of files to process (0 &#x3D; unlimited)
    pub max_files: usize,
    
    /// File patterns to exclude from analysis
    pub exclude_patterns: Vec&amp;lt;String&amp;gt;,
    
    /// File patterns to include in analysis
    pub include_patterns: Vec&amp;lt;String&amp;gt;,
}

impl Default for AnalysisConfig {
    fn default() -&amp;gt; Self {
        Self {
            enable_scoring: true,
            enable_graph_analysis: true,
            enable_lsh_analysis: true,
            enable_refactoring_analysis: true,
            enable_coverage_analysis: true,  // Now enabled by default
            enable_structure_analysis: true,
            enable_names_analysis: true,
            confidence_threshold: 0.7,
            max_files: 0,
            exclude_patterns: vec![
                &amp;quot;*/node_modules/*&amp;quot;.to_string(),
                &amp;quot;*/venv/*&amp;quot;.to_string(),
                &amp;quot;*/target/*&amp;quot;.to_string(),
                &amp;quot;*/__pycache__/*&amp;quot;.to_string(),
                &amp;quot;*.min.js&amp;quot;.to_string(),
            ],
            include_patterns: vec![&amp;quot;**/*&amp;quot;.to_string()],
        }
    }
}

impl AnalysisConfig {
    /// Validate analysis configuration
    pub fn validate(&amp;amp;self) -&amp;gt; Result&amp;lt;()&amp;gt; {
        if !(0.0..&#x3D;1.0).contains(&amp;amp;self.confidence_threshold) {
            return Err(ValknutError::validation(
                format!(&amp;quot;confidence_threshold must be between 0.0 and 1.0, got {}&amp;quot;, self.confidence_threshold)
            ));
        }
        Ok(())
    }
}

/// Scoring and normalization configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ScoringConfig {
    /// Normalization scheme to use
    pub normalization_scheme: NormalizationScheme,
    
    /// Enable Bayesian normalization fallbacks
    pub use_bayesian_fallbacks: bool,
    
    /// Enable confidence reporting
    pub confidence_reporting: bool,
    
    /// Feature weights configuration
    pub weights: WeightsConfig,
    
    /// Statistical parameters
    pub statistical_params: StatisticalParams,
}

impl Default for ScoringConfig {
    fn default() -&amp;gt; Self {
        Self {
            normalization_scheme: NormalizationScheme::ZScore,
            use_bayesian_fallbacks: true,
            confidence_reporting: false,
            weights: WeightsConfig::default(),
            statistical_params: StatisticalParams::default(),
        }
    }
}

impl ScoringConfig {
    /// Validate scoring configuration
    pub fn validate(&amp;amp;self) -&amp;gt; Result&amp;lt;()&amp;gt; {
        self.weights.validate()?;
        self.statistical_params.validate()?;
        Ok(())
    }
}

/// Available normalization schemes
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all &#x3D; &amp;quot;snake_case&amp;quot;)]
pub enum NormalizationScheme {
    /// Z-score normalization (standardization)
    ZScore,
    /// Min-max normalization to [0, 1] range
    MinMax,
    /// Robust normalization using median and IQR
    Robust,
    /// Z-score with Bayesian priors
    ZScoreBayesian,
    /// Min-max with Bayesian estimation
    MinMaxBayesian,
    /// Robust with Bayesian estimation
    RobustBayesian,
}

/// Feature weights configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WeightsConfig {
    /// Complexity feature weights
    pub complexity: f64,
    
    /// Graph-based feature weights
    pub graph: f64,
    
    /// Structure-based feature weights
    pub structure: f64,
    
    /// Style-based feature weights
    pub style: f64,
    
    /// Coverage-based feature weights
    pub coverage: f64,
}

impl Default for WeightsConfig {
    fn default() -&amp;gt; Self {
        Self {
            complexity: 1.0,
            graph: 0.8,
            structure: 0.9,
            style: 0.5,
            coverage: 0.7,
        }
    }
}

impl WeightsConfig {
    /// Validate weights configuration
    pub fn validate(&amp;amp;self) -&amp;gt; Result&amp;lt;()&amp;gt; {
        let weights &#x3D; [self.complexity, self.graph, self.structure, self.style, self.coverage];
        
        for (name, &amp;amp;weight) in [&amp;quot;complexity&amp;quot;, &amp;quot;graph&amp;quot;, &amp;quot;structure&amp;quot;, &amp;quot;style&amp;quot;, &amp;quot;coverage&amp;quot;].iter().zip(&amp;amp;weights) {
            if weight &amp;lt; 0.0 || weight &amp;gt; 10.0 {
                return Err(ValknutError::validation(
                    format!(&amp;quot;Weight for &amp;#x27;{}&amp;#x27; must be between 0.0 and 10.0, got {}&amp;quot;, name, weight)
                ));
            }
        }
        
        Ok(())
    }
}

/// Statistical parameters for analysis
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StatisticalParams {
    /// Confidence interval level (0.95 &#x3D; 95%)
    pub confidence_level: f64,
    
    /// Minimum sample size for statistical analysis
    pub min_sample_size: usize,
    
    /// Outlier detection threshold (in standard deviations)
    pub outlier_threshold: f64,
}

impl Default for StatisticalParams {
    fn default() -&amp;gt; Self {
        Self {
            confidence_level: 0.95,
            min_sample_size: 10,
            outlier_threshold: 3.0,
        }
    }
}

impl StatisticalParams {
    /// Validate statistical parameters
    pub fn validate(&amp;amp;self) -&amp;gt; Result&amp;lt;()&amp;gt; {
        if !(0.0..1.0).contains(&amp;amp;self.confidence_level) {
            return Err(ValknutError::validation(
                format!(&amp;quot;confidence_level must be between 0.0 and 1.0, got {}&amp;quot;, self.confidence_level)
            ));
        }
        
        if self.min_sample_size &#x3D;&#x3D; 0 {
            return Err(ValknutError::validation(&amp;quot;min_sample_size must be greater than 0&amp;quot;));
        }
        
        if self.outlier_threshold &amp;lt;&#x3D; 0.0 {
            return Err(ValknutError::validation(&amp;quot;outlier_threshold must be positive&amp;quot;));
        }
        
        Ok(())
    }
}

/// Graph analysis configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GraphConfig {
    /// Enable betweenness centrality calculation
    pub enable_betweenness: bool,
    
    /// Enable closeness centrality calculation
    pub enable_closeness: bool,
    
    /// Enable cycle detection
    pub enable_cycle_detection: bool,
    
    /// Maximum graph size for exact algorithms
    pub max_exact_size: usize,
    
    /// Use approximation algorithms for large graphs
    pub use_approximation: bool,
    
    /// Sampling rate for approximation algorithms
    pub approximation_sample_rate: f64,
}

impl Default for GraphConfig {
    fn default() -&amp;gt; Self {
        Self {
            enable_betweenness: true,
            enable_closeness: false,
            enable_cycle_detection: true,
            max_exact_size: 10000,
            use_approximation: true,
            approximation_sample_rate: 0.1,
        }
    }
}

impl GraphConfig {
    /// Validate graph configuration
    pub fn validate(&amp;amp;self) -&amp;gt; Result&amp;lt;()&amp;gt; {
        if !(0.0..&#x3D;1.0).contains(&amp;amp;self.approximation_sample_rate) {
            return Err(ValknutError::validation(
                format!(&amp;quot;approximation_sample_rate must be between 0.0 and 1.0, got {}&amp;quot;, self.approximation_sample_rate)
            ));
        }
        Ok(())
    }
}

/// LSH and similarity detection configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LshConfig {
    /// Number of hash functions per band
    pub num_hashes: usize,
    
    /// Number of LSH bands
    pub num_bands: usize,
    
    /// Shingle size for text similarity
    pub shingle_size: usize,
    
    /// Minimum Jaccard similarity threshold
    pub similarity_threshold: f64,
    
    /// Maximum candidates to consider per query
    pub max_candidates: usize,
    
    /// Use advanced similarity algorithms
    pub use_semantic_similarity: bool,
}

impl Default for LshConfig {
    fn default() -&amp;gt; Self {
        Self {
            num_hashes: 128,
            num_bands: 16,
            shingle_size: 3,
            similarity_threshold: 0.7,
            max_candidates: 100,
            use_semantic_similarity: false,  // Keep name for backward compatibility
        }
    }
}

impl LshConfig {
    /// Validate LSH configuration
    pub fn validate(&amp;amp;self) -&amp;gt; Result&amp;lt;()&amp;gt; {
        if self.num_hashes &#x3D;&#x3D; 0 {
            return Err(ValknutError::validation(&amp;quot;num_hashes must be greater than 0&amp;quot;));
        }
        
        if self.num_bands &#x3D;&#x3D; 0 {
            return Err(ValknutError::validation(&amp;quot;num_bands must be greater than 0&amp;quot;));
        }
        
        if self.num_hashes % self.num_bands !&#x3D; 0 {
            return Err(ValknutError::validation(&amp;quot;num_hashes must be divisible by num_bands&amp;quot;));
        }
        
        if !(0.0..&#x3D;1.0).contains(&amp;amp;self.similarity_threshold) {
            return Err(ValknutError::validation(
                format!(&amp;quot;similarity_threshold must be between 0.0 and 1.0, got {}&amp;quot;, self.similarity_threshold)
            ));
        }
        
        Ok(())
    }
    
    /// Get the number of hashes per band
    pub fn hashes_per_band(&amp;amp;self) -&amp;gt; usize {
        self.num_hashes / self.num_bands
    }
}

/// Language-specific configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LanguageConfig {
    /// Enable analysis for this language
    pub enabled: bool,
    
    /// File extensions to process
    pub file_extensions: Vec&amp;lt;String&amp;gt;,
    
    /// Tree-sitter language identifier
    pub tree_sitter_language: String,
    
    /// Maximum file size to process (in MB)
    pub max_file_size_mb: f64,
    
    /// Complexity threshold for this language
    pub complexity_threshold: f64,
    
    /// Additional language-specific settings
    pub additional_settings: HashMap&amp;lt;String, serde_json::Value&amp;gt;,
}

impl LanguageConfig {
    /// Validate language configuration
    pub fn validate(&amp;amp;self) -&amp;gt; Result&amp;lt;()&amp;gt; {
        if self.file_extensions.is_empty() {
            return Err(ValknutError::validation(&amp;quot;file_extensions cannot be empty&amp;quot;));
        }
        
        if self.max_file_size_mb &amp;lt;&#x3D; 0.0 {
            return Err(ValknutError::validation(&amp;quot;max_file_size_mb must be positive&amp;quot;));
        }
        
        if self.complexity_threshold &amp;lt;&#x3D; 0.0 {
            return Err(ValknutError::validation(&amp;quot;complexity_threshold must be positive&amp;quot;));
        }
        
        Ok(())
    }
}

/// I/O and persistence configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IoConfig {
    /// Cache directory path
    pub cache_dir: Option&amp;lt;PathBuf&amp;gt;,
    
    /// Enable result caching
    pub enable_caching: bool,
    
    /// Cache TTL in seconds
    pub cache_ttl_seconds: u64,
    
    /// Report output directory
    pub report_dir: Option&amp;lt;PathBuf&amp;gt;,
    
    /// Report format
    pub report_format: ReportFormat,
    
    /// Enable database persistence
    #[cfg(feature &#x3D; &amp;quot;database&amp;quot;)]
    pub enable_database: bool,
    
    /// Database connection string
    #[cfg(feature &#x3D; &amp;quot;database&amp;quot;)]
    pub database_url: Option&amp;lt;String&amp;gt;,
}

impl Default for IoConfig {
    fn default() -&amp;gt; Self {
        Self {
            cache_dir: None,
            enable_caching: true,
            cache_ttl_seconds: 3600, // 1 hour
            report_dir: None,
            report_format: ReportFormat::Json,
            #[cfg(feature &#x3D; &amp;quot;database&amp;quot;)]
            enable_database: false,
            #[cfg(feature &#x3D; &amp;quot;database&amp;quot;)]
            database_url: None,
        }
    }
}

/// Available report formats
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all &#x3D; &amp;quot;snake_case&amp;quot;)]
pub enum ReportFormat {
    /// JSON format
    Json,
    /// YAML format
    Yaml,
    /// HTML format
    Html,
    /// CSV format (for tabular data)
    Csv,
}

/// Performance and resource configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceConfig {
    /// Maximum number of parallel threads
    pub max_threads: Option&amp;lt;usize&amp;gt;,
    
    /// Memory limit in MB
    pub memory_limit_mb: Option&amp;lt;usize&amp;gt;,
    
    /// Timeout for individual file analysis (seconds)
    pub file_timeout_seconds: u64,
    
    /// Timeout for entire analysis (seconds)
    pub total_timeout_seconds: Option&amp;lt;u64&amp;gt;,
    
    /// Enable SIMD optimizations
    pub enable_simd: bool,
    
    /// Batch size for parallel processing
    pub batch_size: usize,
}

impl Default for PerformanceConfig {
    fn default() -&amp;gt; Self {
        Self {
            max_threads: None, // Use system default
            memory_limit_mb: None, // No limit
            file_timeout_seconds: 30,
            total_timeout_seconds: None, // No limit
            enable_simd: cfg!(feature &#x3D; &amp;quot;simd&amp;quot;),
            batch_size: 100,
        }
    }
}

impl PerformanceConfig {
    /// Validate performance configuration
    pub fn validate(&amp;amp;self) -&amp;gt; Result&amp;lt;()&amp;gt; {
        if let Some(threads) &#x3D; self.max_threads {
            if threads &#x3D;&#x3D; 0 {
                return Err(ValknutError::validation(&amp;quot;max_threads must be greater than 0&amp;quot;));
            }
        }
        
        if let Some(memory) &#x3D; self.memory_limit_mb {
            if memory &#x3D;&#x3D; 0 {
                return Err(ValknutError::validation(&amp;quot;memory_limit_mb must be greater than 0&amp;quot;));
            }
        }
        
        if self.batch_size &#x3D;&#x3D; 0 {
            return Err(ValknutError::validation(&amp;quot;batch_size must be greater than 0&amp;quot;));
        }
        
        Ok(())
    }
}

/// Configuration for coverage analysis and automatic file discovery
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CoverageConfig {
    /// Enable automatic coverage file discovery
    pub auto_discover: bool,
    
    /// Search paths for coverage files (relative to analysis root)
    pub search_paths: Vec&amp;lt;String&amp;gt;,
    
    /// File patterns to search for
    pub file_patterns: Vec&amp;lt;String&amp;gt;,
    
    /// Maximum age of coverage files in days (0 &#x3D; no age limit)
    pub max_age_days: u32,
    
    /// Specific coverage file path (overrides auto discovery)
    pub coverage_file: Option&amp;lt;PathBuf&amp;gt;,
}

impl Default for CoverageConfig {
    fn default() -&amp;gt; Self {
        Self {
            auto_discover: true,
            search_paths: vec![
                &amp;quot;./coverage/&amp;quot;.to_string(),
                &amp;quot;./target/coverage/&amp;quot;.to_string(),
                &amp;quot;./target/tarpaulin/&amp;quot;.to_string(),
                &amp;quot;./target/&amp;quot;.to_string(),
                &amp;quot;./.coverage/&amp;quot;.to_string(),
                &amp;quot;./htmlcov/&amp;quot;.to_string(),
                &amp;quot;./coverage-reports/&amp;quot;.to_string(),
                &amp;quot;./reports/&amp;quot;.to_string(),
                &amp;quot;./test-results/&amp;quot;.to_string(),
                &amp;quot;./build/coverage/&amp;quot;.to_string(),
                &amp;quot;./build/test-results/&amp;quot;.to_string(),
                &amp;quot;./&amp;quot;.to_string(),  // Root directory last
            ],
            file_patterns: vec![
                // Primary coverage file patterns
                &amp;quot;coverage.xml&amp;quot;.to_string(),
                &amp;quot;lcov.info&amp;quot;.to_string(),
                &amp;quot;coverage.json&amp;quot;.to_string(),
                &amp;quot;coverage.lcov&amp;quot;.to_string(),
                &amp;quot;cobertura.xml&amp;quot;.to_string(),
                
                // Coverage.py variations
                &amp;quot;coverage-final.json&amp;quot;.to_string(),
                &amp;quot;coverage-summary.json&amp;quot;.to_string(),
                &amp;quot;.coverage&amp;quot;.to_string(),
                
                // Common framework patterns  
                &amp;quot;junit.xml&amp;quot;.to_string(),
                &amp;quot;jacoco.xml&amp;quot;.to_string(),
                &amp;quot;clover.xml&amp;quot;.to_string(),
                
                // Recursive patterns
                &amp;quot;**/coverage.xml&amp;quot;.to_string(),
                &amp;quot;**/lcov.info&amp;quot;.to_string(),
                &amp;quot;**/coverage.json&amp;quot;.to_string(),
                &amp;quot;**/cobertura.xml&amp;quot;.to_string(),
                &amp;quot;**/jacoco.xml&amp;quot;.to_string(),
                &amp;quot;**/clover.xml&amp;quot;.to_string(),
                
                // Language-specific patterns
                &amp;quot;target/coverage/*.xml&amp;quot;.to_string(),
                &amp;quot;target/tarpaulin/coverage.xml&amp;quot;.to_string(),
                &amp;quot;target/llvm-cov/coverage.lcov&amp;quot;.to_string(),
                &amp;quot;build/coverage/*.xml&amp;quot;.to_string(),
                &amp;quot;coverage/coverage-final.json&amp;quot;.to_string(),
                &amp;quot;htmlcov/coverage.json&amp;quot;.to_string(),
                
                // Build system patterns
                &amp;quot;**/build/jacoco/*.xml&amp;quot;.to_string(),
                &amp;quot;**/build/reports/jacoco/test/*.xml&amp;quot;.to_string(),
                &amp;quot;**/build/test-results/test/*.xml&amp;quot;.to_string(),
            ],
            max_age_days: 7, // Only use coverage files newer than 7 days
            coverage_file: None,
        }
    }
}

impl CoverageConfig {
    /// Validate coverage configuration
    pub fn validate(&amp;amp;self) -&amp;gt; Result&amp;lt;()&amp;gt; {
        if self.file_patterns.is_empty() &amp;amp;&amp;amp; self.auto_discover {
            return Err(ValknutError::validation(&amp;quot;file_patterns cannot be empty when auto_discover is enabled&amp;quot;));
        }
        
        if self.search_paths.is_empty() &amp;amp;&amp;amp; self.auto_discover {
            return Err(ValknutError::validation(&amp;quot;search_paths cannot be empty when auto_discover is enabled&amp;quot;));
        }
        
        Ok(())
    }
}

/// Configuration for live reachability analysis  
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LiveReachConfig {
    /// Ingestion configuration
    pub ingest: IngestConfig,
    
    /// Build/analysis configuration
    pub build: BuildConfig,
}

/// Configuration for stack ingestion
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IngestConfig {
    /// Namespace allow-list (prefixes to include)
    #[serde(default)]
    pub ns_allow: Vec&amp;lt;String&amp;gt;,
    
    /// Language for symbol normalization (auto|jvm|py|go|node|native)
    #[serde(default &#x3D; &amp;quot;default_language&amp;quot;)]
    pub lang: String,
    
    /// Input file glob pattern
    #[serde(default &#x3D; &amp;quot;default_input_glob&amp;quot;)]
    pub input_glob: String,
    
    /// Output directory for processed data
    #[serde(default &#x3D; &amp;quot;default_out_dir&amp;quot;)]
    pub out_dir: String,
    
    /// Upload URI for cloud storage (S3/GCS/Azure)
    #[serde(skip_serializing_if &#x3D; &amp;quot;Option::is_none&amp;quot;)]
    pub upload_uri: Option&amp;lt;String&amp;gt;,
}

/// Configuration for build/analysis phase
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BuildConfig {
    /// Analysis window in days
    #[serde(default &#x3D; &amp;quot;default_since_days&amp;quot;)]
    pub since_days: u32,
    
    /// Services to include in analysis
    #[serde(default &#x3D; &amp;quot;default_services&amp;quot;)]
    pub services: Vec&amp;lt;String&amp;gt;,
    
    /// Weight for static edges relative to runtime edges
    #[serde(default &#x3D; &amp;quot;default_weight_static&amp;quot;)]
    pub weight_static: f64,
    
    /// Island detection configuration
    pub island: IslandConfig,
}

/// Configuration for shadow island detection
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IslandConfig {
    /// Minimum community size to consider
    #[serde(default &#x3D; &amp;quot;default_min_size&amp;quot;)]
    pub min_size: usize,
    
    /// Minimum score threshold for shadow islands
    #[serde(default &#x3D; &amp;quot;default_min_score&amp;quot;)]
    pub min_score: f64,
    
    /// Louvain resolution parameter for community detection
    #[serde(default &#x3D; &amp;quot;default_resolution&amp;quot;)]
    pub resolution: f64,
}

// Default value functions
fn default_language() -&amp;gt; String { &amp;quot;auto&amp;quot;.to_string() }
fn default_input_glob() -&amp;gt; String { &amp;quot;stacks/*.txt&amp;quot;.to_string() }
fn default_out_dir() -&amp;gt; String { &amp;quot;.valknut/live/out&amp;quot;.to_string() }
fn default_since_days() -&amp;gt; u32 { 30 }
fn default_services() -&amp;gt; Vec&amp;lt;String&amp;gt; { vec![&amp;quot;api&amp;quot;.to_string()] }
fn default_weight_static() -&amp;gt; f64 { 0.1 }
fn default_min_size() -&amp;gt; usize { 5 }
fn default_min_score() -&amp;gt; f64 { 0.6 }
fn default_resolution() -&amp;gt; f64 { 0.8 }

impl Default for LiveReachConfig {
    fn default() -&amp;gt; Self {
        Self {
            ingest: IngestConfig::default(),
            build: BuildConfig::default(),
        }
    }
}

impl Default for IngestConfig {
    fn default() -&amp;gt; Self {
        Self {
            ns_allow: vec![&amp;quot;myco.&amp;quot;.to_string(), &amp;quot;github.com/myco/&amp;quot;.to_string()],
            lang: default_language(),
            input_glob: default_input_glob(),
            out_dir: default_out_dir(),
            upload_uri: Some(&amp;quot;s3://company-valknut/live&amp;quot;.to_string()),
        }
    }
}

impl Default for BuildConfig {
    fn default() -&amp;gt; Self {
        Self {
            since_days: default_since_days(),
            services: default_services(),
            weight_static: default_weight_static(),
            island: IslandConfig::default(),
        }
    }
}

impl Default for IslandConfig {
    fn default() -&amp;gt; Self {
        Self {
            min_size: default_min_size(),
            min_score: default_min_score(),
            resolution: default_resolution(),
        }
    }
}

impl LiveReachConfig {
    /// Validate the live reachability configuration
    pub fn validate(&amp;amp;self) -&amp;gt; Result&amp;lt;()&amp;gt; {
        // Validate language
        if ![&amp;quot;auto&amp;quot;, &amp;quot;jvm&amp;quot;, &amp;quot;py&amp;quot;, &amp;quot;go&amp;quot;, &amp;quot;node&amp;quot;, &amp;quot;native&amp;quot;].contains(&amp;amp;self.ingest.lang.as_str()) {
            return Err(ValknutError::validation(
                format!(&amp;quot;Invalid language: {}&amp;quot;, self.ingest.lang)
            ));
        }
        
        // Validate build config
        if self.build.since_days &#x3D;&#x3D; 0 {
            return Err(ValknutError::validation(&amp;quot;since_days must be greater than 0&amp;quot;));
        }
        
        if self.build.weight_static &amp;lt; 0.0 {
            return Err(ValknutError::validation(&amp;quot;weight_static must be non-negative&amp;quot;));
        }
        
        if self.build.island.min_size &#x3D;&#x3D; 0 {
            return Err(ValknutError::validation(&amp;quot;min_size must be greater than 0&amp;quot;));
        }
        
        if self.build.island.min_score &amp;lt; 0.0 || self.build.island.min_score &amp;gt; 1.0 {
            return Err(ValknutError::validation(&amp;quot;min_score must be between 0.0 and 1.0&amp;quot;));
        }
        
        if self.build.island.resolution &amp;lt;&#x3D; 0.0 {
            return Err(ValknutError::validation(&amp;quot;resolution must be positive&amp;quot;));
        }
        
        Ok(())
    }
}

/// Enhanced duplicate detection configuration with adaptive features
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DedupeConfig {
    /// File patterns to include in dedupe analysis
    pub include: Vec&amp;lt;String&amp;gt;,
    
    /// File patterns to exclude from dedupe analysis
    pub exclude: Vec&amp;lt;String&amp;gt;,
    
    /// Minimum number of function tokens to consider
    pub min_function_tokens: usize,
    
    /// Minimum number of AST nodes to consider
    pub min_ast_nodes: usize,
    
    /// Minimum number of matching tokens for a duplicate
    pub min_match_tokens: usize,
    
    /// Minimum coverage ratio for matches
    pub min_match_coverage: f64,
    
    /// Shingle size for k-shingles (8-10 for TF-IDF analysis)
    pub shingle_k: usize,
    
    /// Require distinct blocks for meaningful matches (≥2 basic blocks)
    pub require_distinct_blocks: usize,
    
    /// Feature weights for multi-dimensional similarity
    pub weights: DedupeWeights,
    
    /// I/O signature mismatch penalty
    pub io_mismatch_penalty: f64,
    
    /// Final similarity threshold
    pub threshold_s: f64,
    
    /// String patterns for boilerplate detection (used with tree-sitter AST analysis)
    pub stop_phrases: Vec&amp;lt;String&amp;gt;,
    
    /// Ranking criteria for duplicates
    pub rank_by: RankingCriteria,
    
    /// Minimum saved tokens to report
    pub min_saved_tokens: usize,
    
    /// Keep top N duplicates per file
    pub keep_top_per_file: usize,
    
    /// Adaptive denoising configuration
    #[serde(default)]
    pub adaptive: AdaptiveDenoiseConfig,
}

/// Clone denoising configuration for reducing noise in clone detection
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DenoiseConfig {
    /// Enable clone denoising system (default: true)
    pub enabled: bool,
    
    /// Enable automatic threshold calibration and denoising (default: true)
    pub auto: bool,
    
    /// Core thresholds (user-configurable)
    /// Minimum number of function tokens to consider (40+ recommended)
    pub min_function_tokens: usize,
    
    /// Minimum number of matching tokens for a duplicate (24+ recommended) 
    pub min_match_tokens: usize,
    
    /// Require minimum distinct blocks for meaningful matches (≥2 basic blocks)
    pub require_blocks: usize,
    
    /// Final similarity threshold for clone detection (0.0-1.0)
    pub similarity: f64,
    
    /// Advanced settings
    /// Feature weights for multi-dimensional similarity
    pub weights: DenoiseWeights,
    
    /// I/O signature mismatch penalty
    pub io_mismatch_penalty: f64,
    
    /// Final similarity threshold (alias for similarity)
    pub threshold_s: f64,
    
    /// Stop motifs configuration (AST-based boilerplate filtering)
    pub stop_motifs: StopMotifsConfig,
    
    /// Auto-calibration configuration
    pub auto_calibration: AutoCalibrationConfig,
    
    /// Payoff ranking configuration
    pub ranking: RankingConfig,
    
    /// Enable dry-run mode (analyze but don&amp;#x27;t change behavior)
    pub dry_run: bool,
}

/// Feature weights for denoising multi-dimensional similarity
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DenoiseWeights {
    /// AST similarity weight
    pub ast: f64,
    
    /// Program dependence graph weight  
    pub pdg: f64,
    
    /// Embedding similarity weight
    pub emb: f64,
}

impl Default for DenoiseWeights {
    fn default() -&amp;gt; Self {
        Self {
            ast: 0.35,
            pdg: 0.45,
            emb: 0.20,
        }
    }
}

/// Stop motifs configuration for AST-based boilerplate filtering
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StopMotifsConfig {
    /// Enable stop motifs filtering
    pub enabled: bool,
    
    /// Top percentile of patterns marked as boilerplate (0.0-1.0)
    pub percentile: f64,
    
    /// Cache refresh interval in days
    pub refresh_days: i64,
}

impl Default for StopMotifsConfig {
    fn default() -&amp;gt; Self {
        Self {
            enabled: true,
            percentile: 0.5, // Top 0.5% patterns marked as boilerplate
            refresh_days: 7,
        }
    }
}

/// Auto-calibration configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AutoCalibrationConfig {
    /// Enable auto-calibration
    pub enabled: bool,
    
    /// Quality target (percentage of candidates that must meet quality)
    pub quality_target: f64,
    
    /// Sample size for calibration (top N candidates)
    pub sample_size: usize,
    
    /// Maximum binary search iterations
    pub max_iterations: usize,
}

impl Default for AutoCalibrationConfig {
    fn default() -&amp;gt; Self {
        Self {
            enabled: true,
            quality_target: 0.8, // 80% of candidates must meet quality
            sample_size: 200,    // Top 200 candidates for calibration
            max_iterations: 50,  // Binary search limit
        }
    }
}

/// Payoff ranking configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RankingConfig {
    /// Ranking criteria
    pub by: RankingBy,
    
    /// Minimum saved tokens to report
    pub min_saved_tokens: usize,
    
    /// Minimum rarity gain threshold
    pub min_rarity_gain: f64,
    
    /// Use live reachability data if available
    pub live_reach_boost: bool,
}

/// Ranking criteria options
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all &#x3D; &amp;quot;snake_case&amp;quot;)]
pub enum RankingBy {
    /// Rank by potential token savings
    SavedTokens,
    
    /// Rank by frequency/occurrence count
    Frequency,
}

impl Default for RankingConfig {
    fn default() -&amp;gt; Self {
        Self {
            by: RankingBy::SavedTokens,
            min_saved_tokens: 100,
            min_rarity_gain: 1.2,
            live_reach_boost: true,
        }
    }
}

impl Default for DenoiseConfig {
    fn default() -&amp;gt; Self {
        Self {
            enabled: true,           // Default enabled
            auto: true,              // Default auto-calibration enabled
            min_function_tokens: 40,
            min_match_tokens: 24,
            require_blocks: 2,
            similarity: 0.82,
            weights: DenoiseWeights::default(),
            io_mismatch_penalty: 0.25,
            threshold_s: 0.82,       // Alias for similarity
            stop_motifs: StopMotifsConfig::default(),
            auto_calibration: AutoCalibrationConfig::default(),
            ranking: RankingConfig::default(),
            dry_run: false,
        }
    }
}

impl DenoiseConfig {
    /// Validate denoise configuration
    pub fn validate(&amp;amp;self) -&amp;gt; Result&amp;lt;()&amp;gt; {
        if self.min_function_tokens &#x3D;&#x3D; 0 {
            return Err(ValknutError::validation(&amp;quot;min_function_tokens must be greater than 0&amp;quot;));
        }
        
        if self.min_match_tokens &#x3D;&#x3D; 0 {
            return Err(ValknutError::validation(&amp;quot;min_match_tokens must be greater than 0&amp;quot;));
        }
        
        if self.require_blocks &#x3D;&#x3D; 0 {
            return Err(ValknutError::validation(&amp;quot;require_blocks must be greater than 0&amp;quot;));
        }
        
        if !(0.0..&#x3D;1.0).contains(&amp;amp;self.similarity) {
            return Err(ValknutError::validation(&amp;quot;similarity must be between 0.0 and 1.0&amp;quot;));
        }
        
        if !(0.0..&#x3D;1.0).contains(&amp;amp;self.threshold_s) {
            return Err(ValknutError::validation(&amp;quot;threshold_s must be between 0.0 and 1.0&amp;quot;));
        }
        
        if !(0.0..&#x3D;1.0).contains(&amp;amp;self.io_mismatch_penalty) {
            return Err(ValknutError::validation(&amp;quot;io_mismatch_penalty must be between 0.0 and 1.0&amp;quot;));
        }
        
        // Validate weights sum to approximately 1.0
        let weight_sum &#x3D; self.weights.ast + self.weights.pdg + self.weights.emb;
        if (weight_sum - 1.0).abs() &amp;gt; 0.1 {
            return Err(ValknutError::validation(&amp;quot;denoise weights should sum to approximately 1.0&amp;quot;));
        }
        
        // Validate individual weights are non-negative
        if self.weights.ast &amp;lt; 0.0 || self.weights.pdg &amp;lt; 0.0 || self.weights.emb &amp;lt; 0.0 {
            return Err(ValknutError::validation(&amp;quot;denoise weights must be non-negative&amp;quot;));
        }
        
        // Validate stop motifs config
        if !(0.0..&#x3D;1.0).contains(&amp;amp;self.stop_motifs.percentile) {
            return Err(ValknutError::validation(&amp;quot;stop_motifs.percentile must be between 0.0 and 1.0&amp;quot;));
        }
        
        if self.stop_motifs.refresh_days &amp;lt;&#x3D; 0 {
            return Err(ValknutError::validation(&amp;quot;stop_motifs.refresh_days must be greater than 0&amp;quot;));
        }
        
        // Validate auto-calibration config
        if !(0.0..&#x3D;1.0).contains(&amp;amp;self.auto_calibration.quality_target) {
            return Err(ValknutError::validation(&amp;quot;auto_calibration.quality_target must be between 0.0 and 1.0&amp;quot;));
        }
        
        if self.auto_calibration.sample_size &#x3D;&#x3D; 0 {
            return Err(ValknutError::validation(&amp;quot;auto_calibration.sample_size must be greater than 0&amp;quot;));
        }
        
        if self.auto_calibration.max_iterations &#x3D;&#x3D; 0 {
            return Err(ValknutError::validation(&amp;quot;auto_calibration.max_iterations must be greater than 0&amp;quot;));
        }
        
        // Validate ranking config
        if self.ranking.min_saved_tokens &#x3D;&#x3D; 0 {
            return Err(ValknutError::validation(&amp;quot;ranking.min_saved_tokens must be greater than 0&amp;quot;));
        }
        
        if self.ranking.min_rarity_gain &amp;lt;&#x3D; 0.0 {
            return Err(ValknutError::validation(&amp;quot;ranking.min_rarity_gain must be greater than 0.0&amp;quot;));
        }
        
        Ok(())
    }
}

/// Feature weights for multi-dimensional duplicate detection
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DedupeWeights {
    /// AST similarity weight
    pub ast: f64,
    
    /// Program dependence graph weight  
    pub pdg: f64,
    
    /// Embedding similarity weight
    pub emb: f64,
}

/// Ranking criteria for duplicates
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all &#x3D; &amp;quot;snake_case&amp;quot;)]
pub enum RankingCriteria {
    /// Rank by potential token savings
    SavedTokens,
    
    /// Rank by similarity score
    Similarity,
    
    /// Rank by both similarity and savings
    Combined,
}

/// Adaptive denoising configuration for intelligent clone detection
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AdaptiveDenoiseConfig {
    /// Enable automatic denoising with threshold tuning
    pub auto_denoise: bool,
    
    /// Enable adaptive learning of boilerplate patterns
    pub adaptive_learning: bool,
    
    /// Enable TF-IDF rarity weighting for structural analysis
    pub rarity_weighting: bool,
    
    /// Enable structural validation (PDG motifs, basic blocks)
    pub structural_validation: bool,
    
    /// Enable live reachability boost integration
    pub live_reach_integration: bool,
    
    /// Stop motif percentile threshold (0.0-1.0, e.g., 0.75 &#x3D; top 0.75%)
    pub stop_motif_percentile: f64,
    
    /// Hub suppression threshold (0.0-1.0, patterns in &amp;gt;60% of files)
    pub hub_suppression_threshold: f64,
    
    /// Quality gate percentage (0.0-1.0, 80% of candidates must meet quality)
    pub quality_gate_percentage: f64,
    
    /// TF-IDF k-gram size for structural analysis
    pub tfidf_kgram_size: usize,
    
    /// Weisfeiler-Lehman hash iterations for PDG motifs
    pub wl_iterations: usize,
    
    /// Minimum rarity gain threshold
    pub min_rarity_gain: f64,
    
    /// External call Jaccard similarity penalty threshold
    pub external_call_jaccard_threshold: f64,
    
    /// Cache refresh interval in days
    pub cache_refresh_days: i64,
    
    /// Enable automatic cache refresh
    pub auto_refresh_cache: bool,
}

impl Default for AdaptiveDenoiseConfig {
    fn default() -&amp;gt; Self {
        Self {
            auto_denoise: true,
            adaptive_learning: true,
            rarity_weighting: true,
            structural_validation: true,
            live_reach_integration: true,
            stop_motif_percentile: 0.75,
            hub_suppression_threshold: 0.6,
            quality_gate_percentage: 0.8,
            tfidf_kgram_size: 8,
            wl_iterations: 3,
            min_rarity_gain: 1.2,
            external_call_jaccard_threshold: 0.2,
            cache_refresh_days: 7,
            auto_refresh_cache: true,
        }
    }
}

impl Default for DedupeConfig {
    fn default() -&amp;gt; Self {
        Self {
            include: vec![&amp;quot;src/**&amp;quot;.to_string()],
            exclude: vec![
                &amp;quot;benches/**&amp;quot;.to_string(),
                &amp;quot;examples/**&amp;quot;.to_string(),
                &amp;quot;datasets/**&amp;quot;.to_string(),
                &amp;quot;**/generated/**&amp;quot;.to_string(),
                &amp;quot;**/*.pb.rs&amp;quot;.to_string(),
            ],
            min_function_tokens: 40,
            min_ast_nodes: 35,
            min_match_tokens: 24,
            min_match_coverage: 0.40,
            shingle_k: 9,
            require_distinct_blocks: 2,
            weights: DedupeWeights::default(),
            io_mismatch_penalty: 0.25,
            threshold_s: 0.82,
            stop_phrases: vec![
                r&amp;quot;^\s*@staticmethod\b&amp;quot;.to_string(),
                r&amp;quot;group\.bench_with_input\s*\(&amp;quot;.to_string(),
                r&amp;quot;\bb\.iter\s*\(\|\|&amp;quot;.to_string(),
                r&amp;quot;\bgroup\.finish\s*\(\)\s*;?&amp;quot;.to_string(),
                r&amp;quot;\blet\s+config\s*&#x3D;\s*AnalysisConfig::(new|default)\s*\(\)\s*;?&amp;quot;.to_string(),
                r&amp;quot;\bchecks\.push\s*\(\s*HealthCheck\s*\{&amp;quot;.to_string(),
            ],
            rank_by: RankingCriteria::SavedTokens,
            min_saved_tokens: 100,
            keep_top_per_file: 3,
            adaptive: AdaptiveDenoiseConfig::default(),
        }
    }
}

impl Default for DedupeWeights {
    fn default() -&amp;gt; Self {
        Self {
            ast: 0.35,
            pdg: 0.45,
            emb: 0.20,
        }
    }
}

impl DedupeConfig {
    /// Validate dedupe configuration
    pub fn validate(&amp;amp;self) -&amp;gt; Result&amp;lt;()&amp;gt; {
        if self.min_function_tokens &#x3D;&#x3D; 0 {
            return Err(ValknutError::validation(&amp;quot;min_function_tokens must be greater than 0&amp;quot;));
        }
        
        if self.min_ast_nodes &#x3D;&#x3D; 0 {
            return Err(ValknutError::validation(&amp;quot;min_ast_nodes must be greater than 0&amp;quot;));
        }
        
        if self.min_match_tokens &#x3D;&#x3D; 0 {
            return Err(ValknutError::validation(&amp;quot;min_match_tokens must be greater than 0&amp;quot;));
        }
        
        if !(0.0..&#x3D;1.0).contains(&amp;amp;self.min_match_coverage) {
            return Err(ValknutError::validation(&amp;quot;min_match_coverage must be between 0.0 and 1.0&amp;quot;));
        }
        
        if self.shingle_k &#x3D;&#x3D; 0 {
            return Err(ValknutError::validation(&amp;quot;shingle_k must be greater than 0&amp;quot;));
        }
        
        if !(0.0..&#x3D;1.0).contains(&amp;amp;self.io_mismatch_penalty) {
            return Err(ValknutError::validation(&amp;quot;io_mismatch_penalty must be between 0.0 and 1.0&amp;quot;));
        }
        
        if !(0.0..&#x3D;1.0).contains(&amp;amp;self.threshold_s) {
            return Err(ValknutError::validation(&amp;quot;threshold_s must be between 0.0 and 1.0&amp;quot;));
        }
        
        // Validate weights sum to reasonable values
        let weight_sum &#x3D; self.weights.ast + self.weights.pdg + self.weights.emb;
        if (weight_sum - 1.0).abs() &amp;gt; 0.1 {
            return Err(ValknutError::validation(&amp;quot;weights should sum to approximately 1.0&amp;quot;));
        }
        
        // Validate patterns (simplified - no regex validation)
        for pattern in &amp;amp;self.stop_phrases {
            if pattern.is_empty() {
                return Err(ValknutError::validation(&amp;quot;Empty pattern in stop_phrases&amp;quot;.to_string()));
            }
        }
        
        // Validate adaptive denoising configuration
        if !(0.0..&#x3D;1.0).contains(&amp;amp;self.adaptive.stop_motif_percentile) {
            return Err(ValknutError::validation(&amp;quot;adaptive.stop_motif_percentile must be between 0.0 and 1.0&amp;quot;));
        }
        
        if !(0.0..&#x3D;1.0).contains(&amp;amp;self.adaptive.hub_suppression_threshold) {
            return Err(ValknutError::validation(&amp;quot;adaptive.hub_suppression_threshold must be between 0.0 and 1.0&amp;quot;));
        }
        
        if !(0.0..&#x3D;1.0).contains(&amp;amp;self.adaptive.quality_gate_percentage) {
            return Err(ValknutError::validation(&amp;quot;adaptive.quality_gate_percentage must be between 0.0 and 1.0&amp;quot;));
        }
        
        if self.adaptive.tfidf_kgram_size &#x3D;&#x3D; 0 || self.adaptive.tfidf_kgram_size &amp;gt; 20 {
            return Err(ValknutError::validation(&amp;quot;adaptive.tfidf_kgram_size must be between 1 and 20&amp;quot;));
        }
        
        if self.adaptive.wl_iterations &#x3D;&#x3D; 0 || self.adaptive.wl_iterations &amp;gt; 10 {
            return Err(ValknutError::validation(&amp;quot;adaptive.wl_iterations must be between 1 and 10&amp;quot;));
        }
        
        if self.adaptive.min_rarity_gain &amp;lt;&#x3D; 0.0 {
            return Err(ValknutError::validation(&amp;quot;adaptive.min_rarity_gain must be greater than 0.0&amp;quot;));
        }
        
        if !(0.0..&#x3D;1.0).contains(&amp;amp;self.adaptive.external_call_jaccard_threshold) {
            return Err(ValknutError::validation(&amp;quot;adaptive.external_call_jaccard_threshold must be between 0.0 and 1.0&amp;quot;));
        }
        
        if self.adaptive.cache_refresh_days &amp;lt;&#x3D; 0 {
            return Err(ValknutError::validation(&amp;quot;adaptive.cache_refresh_days must be greater than 0&amp;quot;));
        }
        
        Ok(())
    }
}</pre>
                </div>
            </div>
            <div class="file-section" id="file-2">
                <div class="file-header"><i data-lucide="file-code" class="icon"></i>core/pipeline/mod.rs</div>
                <div class="file-content">
                    <pre>pub use pipeline_config::{AnalysisConfig, QualityGateConfig, QualityGateResult, QualityGateViolation};
pub use pipeline_results::{
    ComprehensiveAnalysisResult, AnalysisSummary, StructureAnalysisResults,
    ComplexityAnalysisResults, RefactoringAnalysisResults, ImpactAnalysisResults,
    CoverageAnalysisResults, HealthMetrics, PipelineResults, PipelineStatistics, 
    MemoryStats, ResultSummary, ScoringResults, FileScore, PipelineStatus
};
pub use pipeline_executor::{AnalysisPipeline, ProgressCallback, ExtractorRegistry};
pub use pipeline_stages::AnalysisStages;

mod pipeline_config;
mod pipeline_results; 
mod pipeline_executor;
mod pipeline_stages;

/// Additional tests for pipeline modules to improve coverage

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::PathBuf;
    use tempfile::TempDir;
    use std::fs;
    
    #[tokio::test]
    async fn test_pipeline_fit_legacy_api() {
        let pipeline &#x3D; AnalysisPipeline::default();
        let mut pipeline &#x3D; pipeline;
        let result &#x3D; pipeline.fit(&amp;amp;[]).await;
        assert!(result.is_ok());
    }

    #[tokio::test]
    async fn test_pipeline_extractor_registry() {
        let pipeline &#x3D; AnalysisPipeline::default();
        let registry &#x3D; pipeline.extractor_registry();
        let extractors: Vec&amp;lt;_&amp;gt; &#x3D; registry.get_all_extractors().collect();
        assert_eq!(extractors.len(), 0);
    }

    #[tokio::test]
    async fn test_pipeline_analyze_vectors_legacy() {
        let pipeline &#x3D; AnalysisPipeline::default();
        let result &#x3D; pipeline.analyze_vectors(vec![]).await;
        assert!(result.is_ok());
    }

    #[tokio::test]
    async fn test_pipeline_status() {
        let pipeline &#x3D; AnalysisPipeline::default();
        let status &#x3D; pipeline.get_status();
        assert!(status.ready);
        assert!(status.is_ready);
        assert!(status.config_valid);
    }

    #[tokio::test]
    async fn test_quality_gates_evaluation() {
        let pipeline &#x3D; AnalysisPipeline::default();
        let config &#x3D; QualityGateConfig::default();
        let results &#x3D; pipeline_results::ComprehensiveAnalysisResult {
            analysis_id: &amp;quot;test&amp;quot;.to_string(),
            timestamp: chrono::Utc::now(),
            processing_time: 1.0,
            config: pipeline_config::AnalysisConfig::default(),
            summary: pipeline_results::AnalysisSummary {
                total_files: 1,
                total_entities: 1,
                total_lines_of_code: 100,
                languages: vec![&amp;quot;Rust&amp;quot;.to_string()],
                total_issues: 0,
                high_priority_issues: 0,
                critical_issues: 0,
            },
            structure: pipeline_results::StructureAnalysisResults {
                enabled: true,
                directory_recommendations: vec![],
                file_splitting_recommendations: vec![],
                issues_count: 0,
            },
            complexity: pipeline_results::ComplexityAnalysisResults {
                enabled: true,
                detailed_results: vec![],
                average_cyclomatic_complexity: 2.0,
                average_cognitive_complexity: 1.5,
                average_technical_debt_score: 10.0,
                average_maintainability_index: 85.0,
                issues_count: 0,
            },
            refactoring: pipeline_results::RefactoringAnalysisResults {
                enabled: true,
                detailed_results: vec![],
                opportunities_count: 0,
            },
            impact: pipeline_results::ImpactAnalysisResults {
                enabled: true,
                dependency_cycles: vec![],
                chokepoints: vec![],
                clone_groups: vec![],
                issues_count: 0,
            },
            lsh: pipeline_results::LshAnalysisResults {
                enabled: false,
                clone_pairs: vec![],
                max_similarity: 0.0,
                avg_similarity: 0.0,
                duplicate_count: 0,
                denoising_enabled: false,
                tfidf_stats: None,
            },
            coverage: pipeline_results::CoverageAnalysisResults {
                enabled: false,
                coverage_files_used: vec![],
                coverage_gaps: vec![],
                gaps_count: 0,
                overall_coverage_percentage: None,
                analysis_method: &amp;quot;none&amp;quot;.to_string(),
            },
            health_metrics: pipeline_results::HealthMetrics {
                overall_health_score: 88.0,
                maintainability_score: 85.0,
                technical_debt_ratio: 10.0,
                complexity_score: 15.0,
                structure_quality_score: 90.0,
            },
        };

        let gate_result &#x3D; pipeline.evaluate_quality_gates(&amp;amp;config, &amp;amp;results);
        assert!(gate_result.passed);
    }

    #[tokio::test]
    async fn test_analyze_directory_integration() {
        let temp_dir &#x3D; TempDir::new().unwrap();
        let file_path &#x3D; temp_dir.path().join(&amp;quot;test.rs&amp;quot;);
        fs::write(&amp;amp;file_path, &amp;quot;fn main() { println!(\&amp;quot;Hello\&amp;quot;); }&amp;quot;).unwrap();
        
        let pipeline &#x3D; AnalysisPipeline::default();
        let result &#x3D; pipeline.analyze_directory(temp_dir.path()).await;
        assert!(result.is_ok());
    }

    #[tokio::test]
    async fn test_analyze_paths_with_progress() {
        let temp_dir &#x3D; TempDir::new().unwrap();
        let file_path &#x3D; temp_dir.path().join(&amp;quot;test.rs&amp;quot;);
        fs::write(&amp;amp;file_path, &amp;quot;fn main() { println!(\&amp;quot;Hello\&amp;quot;); }&amp;quot;).unwrap();
        
        let pipeline &#x3D; AnalysisPipeline::default();
        let paths &#x3D; vec![temp_dir.path().to_path_buf()];
        
        let progress_called &#x3D; std::sync::Arc::new(std::sync::atomic::AtomicBool::new(false));
        let progress_called_clone &#x3D; progress_called.clone();
        let progress_callback &#x3D; Some(Box::new(move |_msg: &amp;amp;str, _progress: f64| {
            progress_called_clone.store(true, std::sync::atomic::Ordering::SeqCst);
        }) as ProgressCallback);
        
        let result &#x3D; pipeline.analyze_paths(&amp;amp;paths, progress_callback).await;
        assert!(result.is_ok());
        assert!(progress_called.load(std::sync::atomic::Ordering::SeqCst));
    }
}</pre>
                </div>
            </div>
            <div class="file-section" id="file-3">
                <div class="file-header"><i data-lucide="file-code" class="icon"></i>core/errors.rs</div>
                <div class="file-content">
                    <pre>//! Error types for the valknut-rs library.
//!
//! This module provides comprehensive error handling for all valknut operations,
//! with structured error types that preserve context and enable proper error
//! propagation throughout the analysis pipeline.

use std::io;
use std::num::{ParseFloatError, ParseIntError};
use std::str::Utf8Error;

use thiserror::Error;

/// Main result type for valknut operations.
pub type Result&amp;lt;T&amp;gt; &#x3D; std::result::Result&amp;lt;T, ValknutError&amp;gt;;

/// Comprehensive error type for all valknut operations.
#[derive(Error, Debug)]
pub enum ValknutError {
    /// I/O related errors (file operations, network, etc.)
    #[error(&amp;quot;I/O error: {message}&amp;quot;)]
    Io {
        /// Human-readable error message
        message: String,
        /// Underlying I/O error
        #[source]
        source: io::Error,
    },

    /// Configuration errors
    #[error(&amp;quot;Configuration error: {message}&amp;quot;)]
    Config {
        /// Error description
        message: String,
        /// Configuration field that caused the error
        field: Option&amp;lt;String&amp;gt;,
    },

    /// Parsing and language processing errors
    #[error(&amp;quot;Parse error in {language}: {message}&amp;quot;)]
    Parse {
        /// Programming language being parsed
        language: String,
        /// Error description
        message: String,
        /// File path where error occurred
        file_path: Option&amp;lt;String&amp;gt;,
        /// Line number (if available)
        line: Option&amp;lt;usize&amp;gt;,
        /// Column number (if available)
        column: Option&amp;lt;usize&amp;gt;,
    },

    /// Mathematical computation errors
    #[error(&amp;quot;Mathematical error: {message}&amp;quot;)]
    Math {
        /// Error description
        message: String,
        /// Context of the mathematical operation
        context: Option&amp;lt;String&amp;gt;,
    },

    /// Graph algorithm errors
    #[error(&amp;quot;Graph analysis error: {message}&amp;quot;)]
    Graph {
        /// Error description
        message: String,
        /// Graph node or edge that caused the error
        element: Option&amp;lt;String&amp;gt;,
    },

    /// LSH and similarity detection errors
    #[error(&amp;quot;LSH error: {message}&amp;quot;)]
    Lsh {
        /// Error description
        message: String,
        /// LSH parameters that may have caused the issue
        parameters: Option&amp;lt;String&amp;gt;,
    },

    /// Database and persistence errors
    #[cfg(feature &#x3D; &amp;quot;database&amp;quot;)]
    #[error(&amp;quot;Database error: {message}&amp;quot;)]
    Database {
        /// Error description
        message: String,
        /// Database operation that failed
        operation: Option&amp;lt;String&amp;gt;,
        /// Underlying database error
        #[source]
        source: Option&amp;lt;Box&amp;lt;dyn std::error::Error + Send + Sync&amp;gt;&amp;gt;,
    },

    /// Analysis pipeline errors
    #[error(&amp;quot;Pipeline error at stage &amp;#x27;{stage}&amp;#x27;: {message}&amp;quot;)]
    Pipeline {
        /// Pipeline stage where error occurred
        stage: String,
        /// Error description
        message: String,
        /// Number of files processed before error
        processed_count: Option&amp;lt;usize&amp;gt;,
    },

    /// Cache and storage errors
    #[error(&amp;quot;Cache error: {message}&amp;quot;)]
    Cache {
        /// Error description
        message: String,
        /// Cache key that caused the issue
        key: Option&amp;lt;String&amp;gt;,
    },

    /// Serialization/deserialization errors
    #[error(&amp;quot;Serialization error: {message}&amp;quot;)]
    Serialization {
        /// Error description
        message: String,
        /// Data type being serialized
        data_type: Option&amp;lt;String&amp;gt;,
        /// Underlying serialization error
        #[source]
        source: Option&amp;lt;Box&amp;lt;dyn std::error::Error + Send + Sync&amp;gt;&amp;gt;,
    },

    /// Validation errors for input data
    #[error(&amp;quot;Validation error: {message}&amp;quot;)]
    Validation {
        /// Error description
        message: String,
        /// Field or input that failed validation
        field: Option&amp;lt;String&amp;gt;,
        /// Expected value or format
        expected: Option&amp;lt;String&amp;gt;,
        /// Actual value received
        actual: Option&amp;lt;String&amp;gt;,
    },

    /// Resource exhaustion errors
    #[error(&amp;quot;Resource exhaustion: {message}&amp;quot;)]
    ResourceExhaustion {
        /// Error description
        message: String,
        /// Type of resource exhausted
        resource_type: String,
        /// Current usage level
        current_usage: Option&amp;lt;String&amp;gt;,
        /// Maximum allowed usage
        limit: Option&amp;lt;String&amp;gt;,
    },

    /// Concurrency and threading errors
    #[error(&amp;quot;Concurrency error: {message}&amp;quot;)]
    Concurrency {
        /// Error description
        message: String,
        /// Thread or task identifier
        thread_id: Option&amp;lt;String&amp;gt;,
    },

    /// Feature not implemented or not available
    #[error(&amp;quot;Feature not available: {feature}&amp;quot;)]
    FeatureUnavailable {
        /// Feature name
        feature: String,
        /// Reason why it&amp;#x27;s unavailable
        reason: Option&amp;lt;String&amp;gt;,
    },

    /// Generic internal errors
    #[error(&amp;quot;Internal error: {message}&amp;quot;)]
    Internal {
        /// Error description
        message: String,
        /// Additional context
        context: Option&amp;lt;String&amp;gt;,
    },

    /// Unsupported operation or feature
    #[error(&amp;quot;Unsupported: {message}&amp;quot;)]
    Unsupported {
        /// Error description
        message: String,
    },
}

impl ValknutError {
    /// Create a new I/O error with context
    pub fn io(message: impl Into&amp;lt;String&amp;gt;, source: io::Error) -&amp;gt; Self {
        Self::Io {
            message: message.into(),
            source,
        }
    }

    /// Create a new configuration error
    pub fn config(message: impl Into&amp;lt;String&amp;gt;) -&amp;gt; Self {
        Self::Config {
            message: message.into(),
            field: None,
        }
    }

    /// Create a new configuration error with field context
    pub fn config_field(message: impl Into&amp;lt;String&amp;gt;, field: impl Into&amp;lt;String&amp;gt;) -&amp;gt; Self {
        Self::Config {
            message: message.into(),
            field: Some(field.into()),
        }
    }

    /// Create a new parse error
    pub fn parse(language: impl Into&amp;lt;String&amp;gt;, message: impl Into&amp;lt;String&amp;gt;) -&amp;gt; Self {
        Self::Parse {
            language: language.into(),
            message: message.into(),
            file_path: None,
            line: None,
            column: None,
        }
    }

    /// Create a new parse error with file context
    pub fn parse_with_location(
        language: impl Into&amp;lt;String&amp;gt;,
        message: impl Into&amp;lt;String&amp;gt;,
        file_path: impl Into&amp;lt;String&amp;gt;,
        line: Option&amp;lt;usize&amp;gt;,
        column: Option&amp;lt;usize&amp;gt;,
    ) -&amp;gt; Self {
        Self::Parse {
            language: language.into(),
            message: message.into(),
            file_path: Some(file_path.into()),
            line,
            column,
        }
    }

    /// Create a new mathematical error
    pub fn math(message: impl Into&amp;lt;String&amp;gt;) -&amp;gt; Self {
        Self::Math {
            message: message.into(),
            context: None,
        }
    }

    /// Create a new mathematical error with context
    pub fn math_with_context(message: impl Into&amp;lt;String&amp;gt;, context: impl Into&amp;lt;String&amp;gt;) -&amp;gt; Self {
        Self::Math {
            message: message.into(),
            context: Some(context.into()),
        }
    }

    /// Create a new graph analysis error
    pub fn graph(message: impl Into&amp;lt;String&amp;gt;) -&amp;gt; Self {
        Self::Graph {
            message: message.into(),
            element: None,
        }
    }

    /// Create a new LSH error
    pub fn lsh(message: impl Into&amp;lt;String&amp;gt;) -&amp;gt; Self {
        Self::Lsh {
            message: message.into(),
            parameters: None,
        }
    }

    /// Create a new pipeline error
    pub fn pipeline(stage: impl Into&amp;lt;String&amp;gt;, message: impl Into&amp;lt;String&amp;gt;) -&amp;gt; Self {
        Self::Pipeline {
            stage: stage.into(),
            message: message.into(),
            processed_count: None,
        }
    }

    /// Create a new validation error
    pub fn validation(message: impl Into&amp;lt;String&amp;gt;) -&amp;gt; Self {
        Self::Validation {
            message: message.into(),
            field: None,
            expected: None,
            actual: None,
        }
    }

    /// Create a new feature unavailable error
    pub fn feature_unavailable(feature: impl Into&amp;lt;String&amp;gt;, reason: impl Into&amp;lt;String&amp;gt;) -&amp;gt; Self {
        Self::FeatureUnavailable {
            feature: feature.into(),
            reason: Some(reason.into()),
        }
    }

    /// Create a new internal error
    pub fn internal(message: impl Into&amp;lt;String&amp;gt;) -&amp;gt; Self {
        Self::Internal {
            message: message.into(),
            context: None,
        }
    }

    /// Create a new unsupported error
    pub fn unsupported(message: impl Into&amp;lt;String&amp;gt;) -&amp;gt; Self {
        Self::Unsupported {
            message: message.into(),
        }
    }

    /// Add context to an existing error
    pub fn with_context(mut self, context: impl Into&amp;lt;String&amp;gt;) -&amp;gt; Self {
        match &amp;amp;mut self {
            Self::Math { context: ctx, .. }
            | Self::Internal { context: ctx, .. } &#x3D;&amp;gt; {
                *ctx &#x3D; Some(context.into());
            }
            _ &#x3D;&amp;gt; {} // Other variants handle context differently
        }
        self
    }
}

// Implement From traits for common error types
impl From&amp;lt;io::Error&amp;gt; for ValknutError {
    fn from(err: io::Error) -&amp;gt; Self {
        Self::io(&amp;quot;I/O operation failed&amp;quot;, err)
    }
}

impl From&amp;lt;serde_json::Error&amp;gt; for ValknutError {
    fn from(err: serde_json::Error) -&amp;gt; Self {
        Self::Serialization {
            message: format!(&amp;quot;JSON serialization failed: {err}&amp;quot;),
            data_type: Some(&amp;quot;JSON&amp;quot;.to_string()),
            source: Some(Box::new(err)),
        }
    }
}

impl From&amp;lt;serde_yaml::Error&amp;gt; for ValknutError {
    fn from(err: serde_yaml::Error) -&amp;gt; Self {
        Self::Serialization {
            message: format!(&amp;quot;YAML serialization failed: {err}&amp;quot;),
            data_type: Some(&amp;quot;YAML&amp;quot;.to_string()),
            source: Some(Box::new(err)),
        }
    }
}

impl From&amp;lt;ParseIntError&amp;gt; for ValknutError {
    fn from(err: ParseIntError) -&amp;gt; Self {
        Self::validation(format!(&amp;quot;Invalid integer: {err}&amp;quot;))
    }
}

impl From&amp;lt;ParseFloatError&amp;gt; for ValknutError {
    fn from(err: ParseFloatError) -&amp;gt; Self {
        Self::validation(format!(&amp;quot;Invalid float: {err}&amp;quot;))
    }
}

impl From&amp;lt;Utf8Error&amp;gt; for ValknutError {
    fn from(err: Utf8Error) -&amp;gt; Self {
        Self::parse(&amp;quot;unknown&amp;quot;, format!(&amp;quot;UTF-8 encoding error: {err}&amp;quot;))
    }
}

#[cfg(feature &#x3D; &amp;quot;database&amp;quot;)]
impl From&amp;lt;sqlx::Error&amp;gt; for ValknutError {
    fn from(err: sqlx::Error) -&amp;gt; Self {
        Self::Database {
            message: format!(&amp;quot;Database operation failed: {err}&amp;quot;),
            operation: None,
            source: Some(Box::new(err)),
        }
    }
}

/// Helper macro for creating context-aware errors
#[macro_export]
macro_rules! valknut_error {
    ($kind:ident, $msg:expr) &#x3D;&amp;gt; {
        $crate::core::errors::ValknutError::$kind($msg.to_string())
    };
    ($kind:ident, $msg:expr, $($arg:tt)*) &#x3D;&amp;gt; {
        $crate::core::errors::ValknutError::$kind(format!($msg, $($arg)*))
    };
}

/// Result extension trait for adding context to errors
pub trait ResultExt&amp;lt;T&amp;gt; {
    /// Add context to an error result
    fn with_context&amp;lt;F&amp;gt;(self, f: F) -&amp;gt; Result&amp;lt;T&amp;gt;
    where
        F: FnOnce() -&amp;gt; String;

    /// Add static context to an error result
    fn context(self, msg: &amp;amp;&amp;#x27;static str) -&amp;gt; Result&amp;lt;T&amp;gt;;
}

impl&amp;lt;T, E&amp;gt; ResultExt&amp;lt;T&amp;gt; for std::result::Result&amp;lt;T, E&amp;gt;
where
    E: Into&amp;lt;ValknutError&amp;gt;,
{
    fn with_context&amp;lt;F&amp;gt;(self, f: F) -&amp;gt; Result&amp;lt;T&amp;gt;
    where
        F: FnOnce() -&amp;gt; String,
    {
        self.map_err(|e| e.into().with_context(f()))
    }

    fn context(self, msg: &amp;amp;&amp;#x27;static str) -&amp;gt; Result&amp;lt;T&amp;gt; {
        self.map_err(|e| e.into().with_context(msg))
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::num::{ParseIntError, ParseFloatError};

    #[test]
    fn test_error_creation() {
        let err &#x3D; ValknutError::config(&amp;quot;Invalid configuration&amp;quot;);
        assert!(matches!(err, ValknutError::Config { .. }));
        
        let err &#x3D; ValknutError::parse(&amp;quot;python&amp;quot;, &amp;quot;Syntax error&amp;quot;);
        assert!(matches!(err, ValknutError::Parse { .. }));
    }

    #[test]
    fn test_error_with_context() {
        let err &#x3D; ValknutError::internal(&amp;quot;Something went wrong&amp;quot;)
            .with_context(&amp;quot;During file processing&amp;quot;);
        
        if let ValknutError::Internal { context, .. } &#x3D; err {
            assert_eq!(context, Some(&amp;quot;During file processing&amp;quot;.to_string()));
        } else {
            panic!(&amp;quot;Expected Internal error&amp;quot;);
        }
    }

    #[test]
    fn test_result_extension() {
        let result: std::result::Result&amp;lt;i32, std::io::Error&amp;gt; &#x3D; 
            Err(std::io::Error::new(std::io::ErrorKind::NotFound, &amp;quot;File not found&amp;quot;));
        
        let valknut_result &#x3D; result.context(&amp;quot;Failed to read configuration file&amp;quot;);
        assert!(valknut_result.is_err());
    }

    #[test]
    fn test_io_error_creation() {
        let io_err &#x3D; std::io::Error::new(std::io::ErrorKind::PermissionDenied, &amp;quot;Access denied&amp;quot;);
        let err &#x3D; ValknutError::io(&amp;quot;Failed to write file&amp;quot;, io_err);
        
        if let ValknutError::Io { message, source } &#x3D; &amp;amp;err {
            assert_eq!(message, &amp;quot;Failed to write file&amp;quot;);
            assert_eq!(source.kind(), std::io::ErrorKind::PermissionDenied);
        } else {
            panic!(&amp;quot;Expected Io error&amp;quot;);
        }
    }

    #[test]
    fn test_config_field_error() {
        let err &#x3D; ValknutError::config_field(&amp;quot;Invalid value&amp;quot;, &amp;quot;max_files&amp;quot;);
        
        if let ValknutError::Config { message, field } &#x3D; err {
            assert_eq!(message, &amp;quot;Invalid value&amp;quot;);
            assert_eq!(field, Some(&amp;quot;max_files&amp;quot;.to_string()));
        } else {
            panic!(&amp;quot;Expected Config error&amp;quot;);
        }
    }

    #[test]
    fn test_parse_with_location() {
        let err &#x3D; ValknutError::parse_with_location(
            &amp;quot;rust&amp;quot;, 
            &amp;quot;Missing semicolon&amp;quot;, 
            &amp;quot;main.rs&amp;quot;, 
            Some(42), 
            Some(10)
        );
        
        if let ValknutError::Parse { language, message, file_path, line, column } &#x3D; err {
            assert_eq!(language, &amp;quot;rust&amp;quot;);
            assert_eq!(message, &amp;quot;Missing semicolon&amp;quot;);
            assert_eq!(file_path, Some(&amp;quot;main.rs&amp;quot;.to_string()));
            assert_eq!(line, Some(42));
            assert_eq!(column, Some(10));
        } else {
            panic!(&amp;quot;Expected Parse error&amp;quot;);
        }
    }

    #[test]
    fn test_math_with_context() {
        let err &#x3D; ValknutError::math_with_context(&amp;quot;Division by zero&amp;quot;, &amp;quot;normalize_features&amp;quot;);
        
        if let ValknutError::Math { message, context } &#x3D; err {
            assert_eq!(message, &amp;quot;Division by zero&amp;quot;);
            assert_eq!(context, Some(&amp;quot;normalize_features&amp;quot;.to_string()));
        } else {
            panic!(&amp;quot;Expected Math error&amp;quot;);
        }
    }

    #[test]
    fn test_graph_error() {
        let err &#x3D; ValknutError::graph(&amp;quot;Cycle detected&amp;quot;);
        
        if let ValknutError::Graph { message, element } &#x3D; err {
            assert_eq!(message, &amp;quot;Cycle detected&amp;quot;);
            assert_eq!(element, None);
        } else {
            panic!(&amp;quot;Expected Graph error&amp;quot;);
        }
    }

    #[test]
    fn test_lsh_error() {
        let err &#x3D; ValknutError::lsh(&amp;quot;Invalid hash function&amp;quot;);
        
        if let ValknutError::Lsh { message, parameters } &#x3D; err {
            assert_eq!(message, &amp;quot;Invalid hash function&amp;quot;);
            assert_eq!(parameters, None);
        } else {
            panic!(&amp;quot;Expected Lsh error&amp;quot;);
        }
    }

    #[test]
    fn test_pipeline_error() {
        let err &#x3D; ValknutError::pipeline(&amp;quot;feature_extraction&amp;quot;, &amp;quot;Timeout exceeded&amp;quot;);
        
        if let ValknutError::Pipeline { stage, message, processed_count } &#x3D; err {
            assert_eq!(stage, &amp;quot;feature_extraction&amp;quot;);
            assert_eq!(message, &amp;quot;Timeout exceeded&amp;quot;);
            assert_eq!(processed_count, None);
        } else {
            panic!(&amp;quot;Expected Pipeline error&amp;quot;);
        }
    }

    #[test]
    fn test_validation_error() {
        let err &#x3D; ValknutError::validation(&amp;quot;Invalid range&amp;quot;);
        
        if let ValknutError::Validation { message, field, expected, actual } &#x3D; err {
            assert_eq!(message, &amp;quot;Invalid range&amp;quot;);
            assert_eq!(field, None);
            assert_eq!(expected, None);
            assert_eq!(actual, None);
        } else {
            panic!(&amp;quot;Expected Validation error&amp;quot;);
        }
    }

    #[test]
    fn test_feature_unavailable() {
        let err &#x3D; ValknutError::feature_unavailable(&amp;quot;SIMD operations&amp;quot;, &amp;quot;CPU does not support AVX2&amp;quot;);
        
        if let ValknutError::FeatureUnavailable { feature, reason } &#x3D; err {
            assert_eq!(feature, &amp;quot;SIMD operations&amp;quot;);
            assert_eq!(reason, Some(&amp;quot;CPU does not support AVX2&amp;quot;.to_string()));
        } else {
            panic!(&amp;quot;Expected FeatureUnavailable error&amp;quot;);
        }
    }

    #[test]
    fn test_unsupported_error() {
        let err &#x3D; ValknutError::unsupported(&amp;quot;Language not supported&amp;quot;);
        
        if let ValknutError::Unsupported { message } &#x3D; err {
            assert_eq!(message, &amp;quot;Language not supported&amp;quot;);
        } else {
            panic!(&amp;quot;Expected Unsupported error&amp;quot;);
        }
    }

    #[test]
    fn test_from_io_error() {
        let io_err &#x3D; std::io::Error::new(std::io::ErrorKind::NotFound, &amp;quot;File not found&amp;quot;);
        let valknut_err: ValknutError &#x3D; io_err.into();
        
        assert!(matches!(valknut_err, ValknutError::Io { .. }));
    }

    #[test]
    fn test_from_json_error() {
        let json_err &#x3D; serde_json::from_str::&amp;lt;i32&amp;gt;(&amp;quot;invalid json&amp;quot;).unwrap_err();
        let valknut_err: ValknutError &#x3D; json_err.into();
        
        if let ValknutError::Serialization { data_type, .. } &#x3D; valknut_err {
            assert_eq!(data_type, Some(&amp;quot;JSON&amp;quot;.to_string()));
        } else {
            panic!(&amp;quot;Expected Serialization error&amp;quot;);
        }
    }

    #[test]
    fn test_from_yaml_error() {
        let yaml_err &#x3D; serde_yaml::from_str::&amp;lt;i32&amp;gt;(&amp;quot;invalid: yaml: content&amp;quot;).unwrap_err();
        let valknut_err: ValknutError &#x3D; yaml_err.into();
        
        if let ValknutError::Serialization { data_type, .. } &#x3D; valknut_err {
            assert_eq!(data_type, Some(&amp;quot;YAML&amp;quot;.to_string()));
        } else {
            panic!(&amp;quot;Expected Serialization error&amp;quot;);
        }
    }

    #[test]
    fn test_from_parse_int_error() {
        let parse_err &#x3D; &amp;quot;not_a_number&amp;quot;.parse::&amp;lt;i32&amp;gt;().unwrap_err();
        let valknut_err: ValknutError &#x3D; parse_err.into();
        
        assert!(matches!(valknut_err, ValknutError::Validation { .. }));
    }

    #[test]
    fn test_from_parse_float_error() {
        let parse_err &#x3D; &amp;quot;not_a_float&amp;quot;.parse::&amp;lt;f64&amp;gt;().unwrap_err();
        let valknut_err: ValknutError &#x3D; parse_err.into();
        
        assert!(matches!(valknut_err, ValknutError::Validation { .. }));
    }

    #[test]
    fn test_from_utf8_error() {
        let invalid_utf8 &#x3D; vec![0, 159, 146, 150]; // Invalid UTF-8 sequence
        let utf8_err &#x3D; std::str::from_utf8(&amp;amp;invalid_utf8).unwrap_err();
        let valknut_err: ValknutError &#x3D; utf8_err.into();
        
        assert!(matches!(valknut_err, ValknutError::Parse { .. }));
    }

    #[test]
    fn test_with_context_math_error() {
        let mut err &#x3D; ValknutError::math(&amp;quot;Overflow occurred&amp;quot;);
        err &#x3D; err.with_context(&amp;quot;In statistical calculation&amp;quot;);
        
        if let ValknutError::Math { context, .. } &#x3D; err {
            assert_eq!(context, Some(&amp;quot;In statistical calculation&amp;quot;.to_string()));
        } else {
            panic!(&amp;quot;Expected Math error with context&amp;quot;);
        }
    }

    #[test]
    fn test_with_context_non_contextual_error() {
        let err &#x3D; ValknutError::config(&amp;quot;Bad config&amp;quot;);
        let err_with_context &#x3D; err.with_context(&amp;quot;Should not change&amp;quot;);
        
        // Config errors don&amp;#x27;t support context, so it should remain unchanged
        if let ValknutError::Config { message, .. } &#x3D; err_with_context {
            assert_eq!(message, &amp;quot;Bad config&amp;quot;);
        } else {
            panic!(&amp;quot;Expected Config error&amp;quot;);
        }
    }

    #[test]
    fn test_result_ext_with_context() {
        let result: std::result::Result&amp;lt;i32, std::io::Error&amp;gt; &#x3D; 
            Err(std::io::Error::new(std::io::ErrorKind::InvalidInput, &amp;quot;Bad input&amp;quot;));
        
        let valknut_result &#x3D; result.with_context(|| &amp;quot;Processing failed&amp;quot;.to_string());
        assert!(valknut_result.is_err());
        
        // Verify the error was converted and context was added
        let err &#x3D; valknut_result.unwrap_err();
        assert!(matches!(err, ValknutError::Io { .. }));
    }

    #[test]
    fn test_error_display_formatting() {
        let err &#x3D; ValknutError::parse_with_location(&amp;quot;python&amp;quot;, &amp;quot;Syntax error&amp;quot;, &amp;quot;test.py&amp;quot;, Some(10), Some(5));
        let display &#x3D; format!(&amp;quot;{}&amp;quot;, err);
        assert!(display.contains(&amp;quot;Parse error in python&amp;quot;));
        assert!(display.contains(&amp;quot;Syntax error&amp;quot;));
    }

    #[test]
    fn test_error_debug_formatting() {
        let err &#x3D; ValknutError::config_field(&amp;quot;Invalid threshold&amp;quot;, &amp;quot;complexity_max&amp;quot;);
        let debug &#x3D; format!(&amp;quot;{:?}&amp;quot;, err);
        assert!(debug.contains(&amp;quot;Config&amp;quot;));
        assert!(debug.contains(&amp;quot;Invalid threshold&amp;quot;));
        assert!(debug.contains(&amp;quot;complexity_max&amp;quot;));
    }
}</pre>
                </div>
            </div>
            <div class="file-section" id="file-4">
                <div class="file-header"><i data-lucide="file-code" class="icon"></i>detectors/structure/config.rs</div>
                <div class="file-content">
                    <pre>//! Configuration structs, data types, and core types for structure analysis

use std::collections::HashSet;
use std::path::PathBuf;
use petgraph::{Graph, Directed, Undirected};
use serde::{Deserialize, Serialize};

/// Configuration for structure analysis
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StructureConfig {
    /// Enable branch reorganization packs
    pub enable_branch_packs: bool,
    /// Enable file split packs
    pub enable_file_split_packs: bool,
    /// Maximum number of top packs to return
    pub top_packs: usize,
    /// File system directory settings
    pub fsdir: FsDirectoryConfig,
    /// File system file settings
    pub fsfile: FsFileConfig,
    /// Graph partitioning settings
    pub partitioning: PartitioningConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StructureToggles {
    /// Enable branch reorganization packs
    pub enable_branch_packs: bool,
    /// Enable file split packs
    pub enable_file_split_packs: bool,
    /// Maximum number of top packs to return
    pub top_packs: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FsDirectoryConfig {
    /// Maximum files per directory before pressure
    pub max_files_per_dir: usize,
    /// Maximum subdirectories per directory before pressure
    pub max_subdirs_per_dir: usize,
    /// Maximum lines of code per directory before pressure
    pub max_dir_loc: usize,
    /// Minimum imbalance gain required for branch recommendation
    pub min_branch_recommendation_gain: f64,
    /// Minimum files required before considering directory split
    pub min_files_for_split: usize,
    /// Target lines of code per subdirectory when partitioning
    pub target_loc_per_subdir: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FsFileConfig {
    /// Lines of code threshold for huge files
    pub huge_loc: usize,
    /// Byte size threshold for huge files
    pub huge_bytes: usize,
    /// Minimum lines of code before considering file split
    pub min_split_loc: usize,
    /// Minimum entities per file split
    pub min_entities_per_split: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PartitioningConfig {
    /// Balance tolerance for partitioning (0.25 &#x3D; ±25%)
    pub balance_tolerance: f64,
    /// Maximum number of clusters per partition
    pub max_clusters: usize,
    /// Minimum number of clusters per partition
    pub min_clusters: usize,
    /// Fallback names for clusters when automatic naming fails
    pub naming_fallbacks: Vec&amp;lt;String&amp;gt;,
}

impl Default for StructureConfig {
    fn default() -&amp;gt; Self {
        Self {
            enable_branch_packs: true,
            enable_file_split_packs: true,
            top_packs: 20,
            fsdir: FsDirectoryConfig {
                max_files_per_dir: 25,
                max_subdirs_per_dir: 10,
                max_dir_loc: 2000,
                min_branch_recommendation_gain: 0.15,
                min_files_for_split: 5,
                target_loc_per_subdir: 1000,
            },
            fsfile: FsFileConfig {
                huge_loc: 800,
                huge_bytes: 128_000,
                min_split_loc: 200,
                min_entities_per_split: 3,
            },
            partitioning: PartitioningConfig {
                balance_tolerance: 0.25,
                max_clusters: 4,
                min_clusters: 2,
                naming_fallbacks: vec![
                    &amp;quot;core&amp;quot;.to_string(),
                    &amp;quot;io&amp;quot;.to_string(), 
                    &amp;quot;api&amp;quot;.to_string(),
                    &amp;quot;util&amp;quot;.to_string(),
                ],
            },
        }
    }
}

/// Directory metrics for imbalance calculation
#[derive(Debug, Clone, Serialize)]
pub struct DirectoryMetrics {
    /// Number of files in directory
    pub files: usize,
    /// Number of subdirectories
    pub subdirs: usize,
    /// Total lines of code
    pub loc: usize,
    /// Gini coefficient of LOC distribution
    pub gini: f64,
    /// Entropy of LOC distribution
    pub entropy: f64,
    /// File pressure (files / max_files_per_dir)
    pub file_pressure: f64,
    /// Branch pressure (subdirs / max_subdirs_per_dir)
    pub branch_pressure: f64,
    /// Size pressure (loc / max_dir_loc)
    pub size_pressure: f64,
    /// Dispersion metric combining gini and entropy
    pub dispersion: f64,
    /// Overall imbalance score
    pub imbalance: f64,
}

/// Branch reorganization pack recommendation
#[derive(Debug, Clone, Serialize)]
pub struct BranchReorgPack {
    /// Type identifier
    pub kind: String,
    /// Directory path
    pub dir: PathBuf,
    /// Current directory state
    pub current: DirectoryMetrics,
    /// Proposed partitions
    pub proposal: Vec&amp;lt;DirectoryPartition&amp;gt;,
    /// File move operations
    pub file_moves: Vec&amp;lt;FileMove&amp;gt;,
    /// Expected gains from reorganization
    pub gain: ReorganizationGain,
    /// Estimated effort for reorganization
    pub effort: ReorganizationEffort,
    /// Rules and constraints
    pub rules: Vec&amp;lt;String&amp;gt;,
}

/// Proposed directory partition
#[derive(Debug, Clone, Serialize)]
pub struct DirectoryPartition {
    /// Suggested partition name
    pub name: String,
    /// Files to move to this partition
    pub files: Vec&amp;lt;PathBuf&amp;gt;,
    /// Total lines of code in partition
    pub loc: usize,
}

/// Expected gains from reorganization
#[derive(Debug, Clone, Serialize)]
pub struct ReorganizationGain {
    /// Change in imbalance score (positive &#x3D; improvement)
    pub imbalance_delta: f64,
    /// Number of cross-cluster edges reduced
    pub cross_edges_reduced: usize,
}

/// Effort estimation for reorganization
#[derive(Debug, Clone, Serialize)]
pub struct ReorganizationEffort {
    /// Number of files that need to be moved
    pub files_moved: usize,
    /// Estimated number of import statement updates
    pub import_updates_est: usize,
}

/// File move operation
#[derive(Debug, Clone, Serialize)]
pub struct FileMove {
    /// Source file path
    pub from: PathBuf,
    /// Destination file path
    pub to: PathBuf,
}

/// File split pack recommendation
#[derive(Debug, Clone, Serialize)]
pub struct FileSplitPack {
    /// Type identifier
    pub kind: String,
    /// File path to split
    pub file: PathBuf,
    /// Reasons for splitting
    pub reasons: Vec&amp;lt;String&amp;gt;,
    /// Suggested split files
    pub suggested_splits: Vec&amp;lt;SuggestedSplit&amp;gt;,
    /// Value metrics
    pub value: SplitValue,
    /// Effort estimation
    pub effort: SplitEffort,
}

/// Suggested file split
#[derive(Debug, Clone, Serialize)]
pub struct SuggestedSplit {
    /// Name of the split file
    pub name: String,
    /// Entities (functions, classes) to move
    pub entities: Vec&amp;lt;String&amp;gt;,
    /// Lines of code in split
    pub loc: usize,
}

/// Value metrics for file splitting
#[derive(Debug, Clone, Serialize)]
pub struct SplitValue {
    /// Overall value score
    pub score: f64,
}

/// Effort estimation for file splitting
#[derive(Debug, Clone, Serialize)]
pub struct SplitEffort {
    /// Number of exports that need updating
    pub exports: usize,
    /// Number of external importers affected
    pub external_importers: usize,
}

/// Internal dependency graph for partitioning
pub type DependencyGraph &#x3D; Graph&amp;lt;FileNode, DependencyEdge, Directed&amp;gt;;

/// File node in dependency graph
#[derive(Debug, Clone)]
pub struct FileNode {
    /// File path
    pub path: PathBuf,
    /// Lines of code
    pub loc: usize,
    /// File size in bytes
    pub size_bytes: usize,
}

/// Dependency edge in graph
#[derive(Debug, Clone)]
pub struct DependencyEdge {
    /// Weight (import count)
    pub weight: usize,
    /// Import type/relationship
    pub relationship_type: String,
}

/// Entity cohesion graph for file splitting
pub type CohesionGraph &#x3D; Graph&amp;lt;EntityNode, CohesionEdge, Undirected&amp;gt;;

/// Entity node in cohesion graph
#[derive(Debug, Clone)]
pub struct EntityNode {
    /// Entity name (function, class, etc.)
    pub name: String,
    /// Entity type (function, class, etc.)
    pub entity_type: String,
    /// Lines of code for entity
    pub loc: usize,
    /// Referenced symbols/identifiers
    pub symbols: HashSet&amp;lt;String&amp;gt;,
}

/// Cohesion edge between entities
#[derive(Debug, Clone)]
pub struct CohesionEdge {
    /// Similarity weight (0.0 to 1.0)
    pub similarity: f64,
    /// Number of shared symbols
    pub shared_symbols: usize,
}

/// Import statement for dependency analysis
#[derive(Debug, Clone)]
pub struct ImportStatement {
    /// Module being imported
    pub module: String,
    /// Specific imports (None for star imports)
    pub imports: Option&amp;lt;Vec&amp;lt;String&amp;gt;&amp;gt;,
    /// Import type (default, named, star, etc.)
    pub import_type: String,
    /// Line number in file
    pub line_number: usize,
}</pre>
                </div>
            </div>
            <div class="file-section" id="file-5">
                <div class="file-header"><i data-lucide="file-code" class="icon"></i>lang/python.rs</div>
                <div class="file-content">
                    <pre>//! Python language adapter with tree-sitter integration.

use std::collections::HashMap;
use std::sync::Arc;

use async_trait::async_trait;
use tree_sitter::{Language, Node, Parser, Tree, TreeCursor};

use super::common::{EntityKind, ParsedEntity, ParseIndex, SourceLocation, LanguageAdapter};
use crate::core::errors::{Result, ValknutError};
use crate::core::featureset::{CodeEntity, EntityId};

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_python_adapter_creation() {
        let adapter &#x3D; PythonAdapter::new();
        assert!(adapter.is_ok(), &amp;quot;Should create Python adapter successfully&amp;quot;);
    }
    
    #[test]
    fn test_parse_simple_function() {
        let mut adapter &#x3D; PythonAdapter::new().unwrap();
        let source &#x3D; r#&amp;quot;
def hello_world():
    return &amp;quot;Hello, World!&amp;quot;
&amp;quot;#;
        let result &#x3D; adapter.parse_source(source, &amp;quot;test.py&amp;quot;);
        assert!(result.is_ok(), &amp;quot;Should parse simple function&amp;quot;);
        
        let index &#x3D; result.unwrap();
        assert!(index.get_entities_in_file(&amp;quot;test.py&amp;quot;).len() &amp;gt;&#x3D; 1, &amp;quot;Should find at least one entity&amp;quot;);
    }
    
    #[test]
    fn test_parse_simple_class() {
        let mut adapter &#x3D; PythonAdapter::new().unwrap();
        let source &#x3D; r#&amp;quot;
class MyClass:
    def __init__(self):
        self.value &#x3D; 0
    
    def get_value(self):
        return self.value
&amp;quot;#;
        let result &#x3D; adapter.parse_source(source, &amp;quot;test.py&amp;quot;);
        assert!(result.is_ok(), &amp;quot;Should parse simple class&amp;quot;);
        
        let index &#x3D; result.unwrap();
        let entities &#x3D; index.get_entities_in_file(&amp;quot;test.py&amp;quot;);
        assert!(entities.len() &amp;gt;&#x3D; 1, &amp;quot;Should find at least one entity&amp;quot;);
        
        let has_class &#x3D; entities.iter().any(|e| matches!(e.kind, EntityKind::Class));
        assert!(has_class, &amp;quot;Should find a class entity&amp;quot;);
    }
    
    #[test]
    fn test_parse_complex_python() {
        let mut adapter &#x3D; PythonAdapter::new().unwrap();
        let source &#x3D; r#&amp;quot;
import os
import sys

class DataProcessor:
    &amp;quot;&amp;quot;&amp;quot;A sample data processor class.&amp;quot;&amp;quot;&amp;quot;
    
    def __init__(self, name: str):
        self.name &#x3D; name
        self.data &#x3D; []
    
    @property
    def size(self) -&amp;gt; int:
        return len(self.data)
    
    def add_data(self, item):
        self.data.append(item)

def process_file(filename: str) -&amp;gt; bool:
    &amp;quot;&amp;quot;&amp;quot;Process a file and return success status.&amp;quot;&amp;quot;&amp;quot;
    try:
        with open(filename, &amp;#x27;r&amp;#x27;) as f:
            content &#x3D; f.read()
        return True
    except FileNotFoundError:
        return False

if __name__ &#x3D;&#x3D; &amp;quot;__main__&amp;quot;:
    processor &#x3D; DataProcessor(&amp;quot;test&amp;quot;)
    success &#x3D; process_file(&amp;quot;data.txt&amp;quot;)
&amp;quot;#;
        let result &#x3D; adapter.parse_source(source, &amp;quot;complex.py&amp;quot;);
        assert!(result.is_ok(), &amp;quot;Should parse complex Python code&amp;quot;);
        
        let index &#x3D; result.unwrap();
        let entities &#x3D; index.get_entities_in_file(&amp;quot;complex.py&amp;quot;);
        assert!(entities.len() &amp;gt;&#x3D; 2, &amp;quot;Should find multiple entities&amp;quot;);
        
        let has_class &#x3D; entities.iter().any(|e| matches!(e.kind, EntityKind::Class));
        let has_function &#x3D; entities.iter().any(|e| matches!(e.kind, EntityKind::Function));
        assert!(has_class &amp;amp;&amp;amp; has_function, &amp;quot;Should find both class and function entities&amp;quot;);
    }
    
    #[test]
    fn test_extract_entity_name() {
        let mut adapter &#x3D; PythonAdapter::new().unwrap();
        let source &#x3D; &amp;quot;def test_function(): pass&amp;quot;;
        let tree &#x3D; adapter.parser.parse(source, None).unwrap();
        let function_node &#x3D; tree.root_node().child(0).unwrap(); // Should be function_definition
        
        let result &#x3D; adapter.extract_name(&amp;amp;function_node, source);
        assert!(result.is_ok());
        
        if let Ok(Some(name)) &#x3D; result {
            assert_eq!(name, &amp;quot;test_function&amp;quot;);
        }
    }
    
    #[test]
    fn test_convert_to_code_entity() {
        let adapter &#x3D; PythonAdapter::new().unwrap();
        let entity &#x3D; ParsedEntity {
            id: &amp;quot;test-id&amp;quot;.to_string(),
            name: &amp;quot;test_func&amp;quot;.to_string(),
            kind: EntityKind::Function,
            location: SourceLocation {
                file_path: &amp;quot;test.py&amp;quot;.to_string(),
                start_line: 1,
                end_line: 2,
                start_column: 0,
                end_column: 10,
            },
            parent: None,
            children: vec![],
            metadata: HashMap::new(),
        };
        
        let source &#x3D; &amp;quot;def test_func(): pass&amp;quot;;
        let result &#x3D; adapter.convert_to_code_entity(&amp;amp;entity, source);
        assert!(result.is_ok(), &amp;quot;Should convert to CodeEntity successfully&amp;quot;);
        
        let code_entity &#x3D; result.unwrap();
        assert_eq!(code_entity.name, &amp;quot;test_func&amp;quot;);
        assert_eq!(code_entity.file_path, &amp;quot;test.py&amp;quot;);
    }
    
    #[test]
    fn test_get_entities_empty_file() {
        let mut adapter &#x3D; PythonAdapter::new().unwrap();
        let source &#x3D; &amp;quot;# Just a comment\n&amp;quot;;
        let result &#x3D; adapter.parse_source(source, &amp;quot;empty.py&amp;quot;);
        assert!(result.is_ok(), &amp;quot;Should handle empty Python file&amp;quot;);
        
        let index &#x3D; result.unwrap();
        let entities &#x3D; index.get_entities_in_file(&amp;quot;empty.py&amp;quot;);
        assert_eq!(entities.len(), 0, &amp;quot;Should find no entities in comment-only file&amp;quot;);
    }
}


/// Python-specific parsing and analysis
pub struct PythonAdapter {
    /// Tree-sitter parser for Python
    parser: Parser,
    
    /// Language instance
    language: Language,
}

impl PythonAdapter {
    /// Create a new Python adapter
    pub fn new() -&amp;gt; Result&amp;lt;Self&amp;gt; {
        let language &#x3D; tree_sitter_python::language();
        let mut parser &#x3D; Parser::new();
        parser.set_language(language)
            .map_err(|e| ValknutError::parse(&amp;quot;python&amp;quot;, format!(&amp;quot;Failed to set Python language: {:?}&amp;quot;, e)))?;
        
        Ok(Self { parser, language })
    }
    
    /// Parse Python source code and extract entities
    pub fn parse_source(&amp;amp;mut self, source_code: &amp;amp;str, file_path: &amp;amp;str) -&amp;gt; Result&amp;lt;ParseIndex&amp;gt; {
        let tree &#x3D; self.parser.parse(source_code, None)
            .ok_or_else(|| ValknutError::parse(&amp;quot;python&amp;quot;, &amp;quot;Failed to parse Python source code&amp;quot;))?;
        
        let mut index &#x3D; ParseIndex::new();
        let mut entity_id_counter &#x3D; 0;
        
        // Walk the tree and extract entities
        self.extract_entities_recursive(
            tree.root_node(), 
            source_code, 
            file_path, 
            None, 
            &amp;amp;mut index, 
            &amp;amp;mut entity_id_counter
        )?;
        
        Ok(index)
    }
    
    /// Extract entities from Python code and convert to CodeEntity format
    pub fn extract_code_entities(&amp;amp;mut self, source_code: &amp;amp;str, file_path: &amp;amp;str) -&amp;gt; Result&amp;lt;Vec&amp;lt;CodeEntity&amp;gt;&amp;gt; {
        let parse_index &#x3D; self.parse_source(source_code, file_path)?;
        let mut code_entities &#x3D; Vec::new();
        
        for entity in parse_index.entities.values() {
            let code_entity &#x3D; self.convert_to_code_entity(entity, source_code)?;
            code_entities.push(code_entity);
        }
        
        Ok(code_entities)
    }
    
    /// Recursively extract entities from the AST
    fn extract_entities_recursive(
        &amp;amp;self,
        node: Node,
        source_code: &amp;amp;str,
        file_path: &amp;amp;str,
        parent_id: Option&amp;lt;String&amp;gt;,
        index: &amp;amp;mut ParseIndex,
        entity_id_counter: &amp;amp;mut usize,
    ) -&amp;gt; Result&amp;lt;()&amp;gt; {
        // Check if this node represents an entity we care about
        if let Some(entity) &#x3D; self.node_to_entity(node, source_code, file_path, parent_id.clone(), entity_id_counter)? {
            let entity_id &#x3D; entity.id.clone();
            index.add_entity(entity);
            
            // Process child nodes with this entity as parent
            let mut cursor &#x3D; node.walk();
            for child in node.children(&amp;amp;mut cursor) {
                self.extract_entities_recursive(
                    child, 
                    source_code, 
                    file_path, 
                    Some(entity_id.clone()), 
                    index, 
                    entity_id_counter
                )?;
            }
        } else {
            // Process child nodes with current parent
            let mut cursor &#x3D; node.walk();
            for child in node.children(&amp;amp;mut cursor) {
                self.extract_entities_recursive(
                    child, 
                    source_code, 
                    file_path, 
                    parent_id.clone(), 
                    index, 
                    entity_id_counter
                )?;
            }
        }
        
        Ok(())
    }
    
    /// Convert a tree-sitter node to a ParsedEntity if it represents an entity
    fn node_to_entity(
        &amp;amp;self,
        node: Node,
        source_code: &amp;amp;str,
        file_path: &amp;amp;str,
        parent_id: Option&amp;lt;String&amp;gt;,
        entity_id_counter: &amp;amp;mut usize,
    ) -&amp;gt; Result&amp;lt;Option&amp;lt;ParsedEntity&amp;gt;&amp;gt; {
        let entity_kind &#x3D; match node.kind() {
            &amp;quot;function_definition&amp;quot; &#x3D;&amp;gt; EntityKind::Function,
            &amp;quot;class_definition&amp;quot; &#x3D;&amp;gt; EntityKind::Class,
            &amp;quot;module&amp;quot; &#x3D;&amp;gt; EntityKind::Module,
            &amp;quot;assignment&amp;quot; &#x3D;&amp;gt; {
                // Check if it&amp;#x27;s a constant assignment (all uppercase)
                if let Some(name) &#x3D; self.extract_name(&amp;amp;node, source_code)? {
                    if name.chars().all(|c| c.is_uppercase() || c &#x3D;&#x3D; &amp;#x27;_&amp;#x27;) {
                        EntityKind::Constant
                    } else {
                        EntityKind::Variable
                    }
                } else {
                    return Ok(None);
                }
            }
            // Method definitions are handled as functions within classes
            _ &#x3D;&amp;gt; return Ok(None),
        };
        
        let name &#x3D; self.extract_name(&amp;amp;node, source_code)?
            .ok_or_else(|| ValknutError::parse(&amp;quot;python&amp;quot;, &amp;quot;Could not extract entity name&amp;quot;))?;
        
        *entity_id_counter +&#x3D; 1;
        let entity_id &#x3D; format!(&amp;quot;{}:{}:{}&amp;quot;, file_path, entity_kind as u8, *entity_id_counter);
        
        let location &#x3D; SourceLocation {
            file_path: file_path.to_string(),
            start_line: node.start_position().row + 1,
            end_line: node.end_position().row + 1,
            start_column: node.start_position().column + 1,
            end_column: node.end_position().column + 1,
        };
        
        let mut metadata &#x3D; HashMap::new();
        
        // Add Python-specific metadata
        metadata.insert(&amp;quot;node_kind&amp;quot;.to_string(), serde_json::Value::String(node.kind().to_string()));
        metadata.insert(&amp;quot;byte_range&amp;quot;.to_string(), serde_json::json!([node.start_byte(), node.end_byte()]));
        
        // Extract additional metadata based on entity type
        match entity_kind {
            EntityKind::Function &#x3D;&amp;gt; {
                self.extract_function_metadata(&amp;amp;node, source_code, &amp;amp;mut metadata)?;
            }
            EntityKind::Class &#x3D;&amp;gt; {
                self.extract_class_metadata(&amp;amp;node, source_code, &amp;amp;mut metadata)?;
            }
            _ &#x3D;&amp;gt; {}
        }
        
        let entity &#x3D; ParsedEntity {
            id: entity_id,
            kind: entity_kind,
            name,
            parent: parent_id,
            children: Vec::new(), // Will be populated later
            location,
            metadata,
        };
        
        Ok(Some(entity))
    }
    
    /// Extract the name of an entity from its AST node
    fn extract_name(&amp;amp;self, node: &amp;amp;Node, source_code: &amp;amp;str) -&amp;gt; Result&amp;lt;Option&amp;lt;String&amp;gt;&amp;gt; {
        let mut cursor &#x3D; node.walk();
        
        match node.kind() {
            &amp;quot;function_definition&amp;quot; | &amp;quot;class_definition&amp;quot; &#x3D;&amp;gt; {
                // Look for the identifier child
                for child in node.children(&amp;amp;mut cursor) {
                    if child.kind() &#x3D;&#x3D; &amp;quot;identifier&amp;quot; {
                        return Ok(Some(child.utf8_text(source_code.as_bytes())?.to_string()));
                    }
                }
            }
            &amp;quot;assignment&amp;quot; &#x3D;&amp;gt; {
                // Look for the left-hand side identifier
                for child in node.children(&amp;amp;mut cursor) {
                    if child.kind() &#x3D;&#x3D; &amp;quot;identifier&amp;quot; {
                        return Ok(Some(child.utf8_text(source_code.as_bytes())?.to_string()));
                    }
                }
            }
            _ &#x3D;&amp;gt; {}
        }
        
        Ok(None)
    }
    
    /// Extract function-specific metadata
    fn extract_function_metadata(&amp;amp;self, node: &amp;amp;Node, source_code: &amp;amp;str, metadata: &amp;amp;mut HashMap&amp;lt;String, serde_json::Value&amp;gt;) -&amp;gt; Result&amp;lt;()&amp;gt; {
        let mut cursor &#x3D; node.walk();
        let mut parameters &#x3D; Vec::new();
        let mut has_decorators &#x3D; false;
        let mut return_annotation &#x3D; None;
        
        for child in node.children(&amp;amp;mut cursor) {
            match child.kind() {
                &amp;quot;parameters&amp;quot; &#x3D;&amp;gt; {
                    // Extract parameter information
                    let mut param_cursor &#x3D; child.walk();
                    for param_child in child.children(&amp;amp;mut param_cursor) {
                        if param_child.kind() &#x3D;&#x3D; &amp;quot;identifier&amp;quot; {
                            let param_name &#x3D; param_child.utf8_text(source_code.as_bytes())?;
                            parameters.push(param_name);
                        }
                    }
                }
                &amp;quot;decorator&amp;quot; &#x3D;&amp;gt; {
                    has_decorators &#x3D; true;
                }
                &amp;quot;type&amp;quot; &#x3D;&amp;gt; {
                    // Return type annotation
                    return_annotation &#x3D; Some(child.utf8_text(source_code.as_bytes())?.to_string());
                }
                _ &#x3D;&amp;gt; {}
            }
        }
        
        metadata.insert(&amp;quot;parameters&amp;quot;.to_string(), serde_json::json!(parameters));
        metadata.insert(&amp;quot;has_decorators&amp;quot;.to_string(), serde_json::Value::Bool(has_decorators));
        if let Some(return_type) &#x3D; return_annotation {
            metadata.insert(&amp;quot;return_annotation&amp;quot;.to_string(), serde_json::Value::String(return_type));
        }
        
        Ok(())
    }
    
    /// Extract class-specific metadata
    fn extract_class_metadata(&amp;amp;self, node: &amp;amp;Node, source_code: &amp;amp;str, metadata: &amp;amp;mut HashMap&amp;lt;String, serde_json::Value&amp;gt;) -&amp;gt; Result&amp;lt;()&amp;gt; {
        let mut cursor &#x3D; node.walk();
        let mut base_classes &#x3D; Vec::new();
        let mut has_decorators &#x3D; false;
        
        for child in node.children(&amp;amp;mut cursor) {
            match child.kind() {
                &amp;quot;argument_list&amp;quot; &#x3D;&amp;gt; {
                    // Base classes
                    let mut arg_cursor &#x3D; child.walk();
                    for arg_child in child.children(&amp;amp;mut arg_cursor) {
                        if arg_child.kind() &#x3D;&#x3D; &amp;quot;identifier&amp;quot; {
                            let base_name &#x3D; arg_child.utf8_text(source_code.as_bytes())?;
                            base_classes.push(base_name);
                        }
                    }
                }
                &amp;quot;decorator&amp;quot; &#x3D;&amp;gt; {
                    has_decorators &#x3D; true;
                }
                _ &#x3D;&amp;gt; {}
            }
        }
        
        metadata.insert(&amp;quot;base_classes&amp;quot;.to_string(), serde_json::json!(base_classes));
        metadata.insert(&amp;quot;has_decorators&amp;quot;.to_string(), serde_json::Value::Bool(has_decorators));
        
        Ok(())
    }
    
    /// Convert ParsedEntity to CodeEntity format
    fn convert_to_code_entity(&amp;amp;self, entity: &amp;amp;ParsedEntity, source_code: &amp;amp;str) -&amp;gt; Result&amp;lt;CodeEntity&amp;gt; {
        let source_lines: Vec&amp;lt;&amp;amp;str&amp;gt; &#x3D; source_code.lines().collect();
        let entity_source &#x3D; if entity.location.start_line &amp;lt;&#x3D; source_lines.len() &amp;amp;&amp;amp; entity.location.end_line &amp;lt;&#x3D; source_lines.len() {
            source_lines[(entity.location.start_line - 1)..entity.location.end_line].join(&amp;quot;\n&amp;quot;)
        } else {
            String::new()
        };
        
        let mut code_entity &#x3D; CodeEntity::new(
            entity.id.clone(),
            format!(&amp;quot;{:?}&amp;quot;, entity.kind),
            entity.name.clone(),
            entity.location.file_path.clone(),
        )
        .with_line_range(entity.location.start_line, entity.location.end_line)
        .with_source_code(entity_source);
        
        // Add metadata from parsed entity
        for (key, value) in &amp;amp;entity.metadata {
            code_entity.add_property(key.clone(), value.clone());
        }
        
        Ok(code_entity)
    }
    
    // Helper methods for LanguageAdapter trait implementation
    
    /// Extract function calls recursively from AST
    fn extract_function_calls_recursive(&amp;amp;self, node: Node, source: &amp;amp;str, calls: &amp;amp;mut Vec&amp;lt;String&amp;gt;) -&amp;gt; Result&amp;lt;()&amp;gt; {
        match node.kind() {
            &amp;quot;call&amp;quot; &#x3D;&amp;gt; {
                // Extract the function name from call expression
                if let Some(func_node) &#x3D; node.child_by_field_name(&amp;quot;function&amp;quot;) {
                    if let Ok(func_name) &#x3D; func_node.utf8_text(source.as_bytes()) {
                        calls.push(func_name.to_string());
                    }
                }
            }
            &amp;quot;attribute&amp;quot; &#x3D;&amp;gt; {
                // Handle method calls like obj.method()
                if let Ok(attr_text) &#x3D; node.utf8_text(source.as_bytes()) {
                    calls.push(attr_text.to_string());
                }
            }
            _ &#x3D;&amp;gt; {}
        }
        
        // Process children
        let mut cursor &#x3D; node.walk();
        for child in node.children(&amp;amp;mut cursor) {
            self.extract_function_calls_recursive(child, source, calls)?;
        }
        
        Ok(())
    }
    
    /// Check for boilerplate patterns in AST recursively
    fn check_boilerplate_patterns_recursive(
        &amp;amp;self, 
        node: Node, 
        source: &amp;amp;str, 
        patterns: &amp;amp;[String], 
        found_patterns: &amp;amp;mut Vec&amp;lt;String&amp;gt;
    ) -&amp;gt; Result&amp;lt;()&amp;gt; {
        // Get text content of current node
        if let Ok(node_text) &#x3D; node.utf8_text(source.as_bytes()) {
            for pattern in patterns {
                if node_text.contains(pattern) {
                    found_patterns.push(pattern.clone());
                }
            }
        }
        
        // Check specific Python boilerplate patterns based on AST structure
        match node.kind() {
            &amp;quot;import_statement&amp;quot; | &amp;quot;import_from_statement&amp;quot; &#x3D;&amp;gt; {
                // Common import patterns
                if let Ok(import_text) &#x3D; node.utf8_text(source.as_bytes()) {
                    let common_imports &#x3D; [&amp;quot;import os&amp;quot;, &amp;quot;import sys&amp;quot;, &amp;quot;from typing import&amp;quot;];
                    for common in &amp;amp;common_imports {
                        if import_text.contains(common) {
                            found_patterns.push(common.to_string());
                        }
                    }
                }
            }
            &amp;quot;if_statement&amp;quot; &#x3D;&amp;gt; {
                // Check for if __name__ &#x3D;&#x3D; &amp;quot;__main__&amp;quot; pattern
                if let Ok(if_text) &#x3D; node.utf8_text(source.as_bytes()) {
                    if if_text.contains(&amp;quot;__name__&amp;quot;) &amp;amp;&amp;amp; if_text.contains(&amp;quot;__main__&amp;quot;) {
                        found_patterns.push(&amp;quot;if __name__ &#x3D;&#x3D; \&amp;quot;__main__\&amp;quot;&amp;quot;.to_string());
                    }
                }
            }
            &amp;quot;function_definition&amp;quot; &#x3D;&amp;gt; {
                // Check for common function patterns like __init__
                if let Some(name_node) &#x3D; node.child_by_field_name(&amp;quot;name&amp;quot;) {
                    if let Ok(func_name) &#x3D; name_node.utf8_text(source.as_bytes()) {
                        if func_name.starts_with(&amp;quot;__&amp;quot;) &amp;amp;&amp;amp; func_name.ends_with(&amp;quot;__&amp;quot;) {
                            found_patterns.push(func_name.to_string());
                        }
                    }
                }
            }
            _ &#x3D;&amp;gt; {}
        }
        
        // Process children
        let mut cursor &#x3D; node.walk();
        for child in node.children(&amp;amp;mut cursor) {
            self.check_boilerplate_patterns_recursive(child, source, patterns, found_patterns)?;
        }
        
        Ok(())
    }
    
    /// Extract identifiers recursively from AST
    fn extract_identifiers_recursive(&amp;amp;self, node: Node, source: &amp;amp;str, identifiers: &amp;amp;mut Vec&amp;lt;String&amp;gt;) -&amp;gt; Result&amp;lt;()&amp;gt; {
        match node.kind() {
            &amp;quot;identifier&amp;quot; &#x3D;&amp;gt; {
                if let Ok(identifier) &#x3D; node.utf8_text(source.as_bytes()) {
                    identifiers.push(identifier.to_string());
                }
            }
            _ &#x3D;&amp;gt; {}
        }
        
        // Process children
        let mut cursor &#x3D; node.walk();
        for child in node.children(&amp;amp;mut cursor) {
            self.extract_identifiers_recursive(child, source, identifiers)?;
        }
        
        Ok(())
    }
    
    /// Count AST nodes recursively
    fn count_nodes_recursive(&amp;amp;self, node: Node) -&amp;gt; usize {
        let mut count &#x3D; 1; // Count this node
        
        let mut cursor &#x3D; node.walk();
        for child in node.children(&amp;amp;mut cursor) {
            count +&#x3D; self.count_nodes_recursive(child);
        }
        
        count
    }
    
    /// Count distinct code blocks recursively
    fn count_blocks_recursive(&amp;amp;self, node: Node, block_count: &amp;amp;mut usize) {
        match node.kind() {
            &amp;quot;function_definition&amp;quot; | &amp;quot;class_definition&amp;quot; &#x3D;&amp;gt; {
                *block_count +&#x3D; 1;
            }
            &amp;quot;if_statement&amp;quot; | &amp;quot;for_statement&amp;quot; | &amp;quot;while_statement&amp;quot; | &amp;quot;try_statement&amp;quot; | &amp;quot;with_statement&amp;quot; &#x3D;&amp;gt; {
                *block_count +&#x3D; 1;
            }
            _ &#x3D;&amp;gt; {}
        }
        
        // Process children
        let mut cursor &#x3D; node.walk();
        for child in node.children(&amp;amp;mut cursor) {
            self.count_blocks_recursive(child, block_count);
        }
    }
    
    /// Normalize AST recursively for comparison
    fn normalize_ast_recursive(&amp;amp;self, node: Node, source: &amp;amp;str, normalized_parts: &amp;amp;mut Vec&amp;lt;String&amp;gt;) -&amp;gt; Result&amp;lt;()&amp;gt; {
        match node.kind() {
            // Include semantic tokens, exclude syntactic noise
            &amp;quot;function_definition&amp;quot; | &amp;quot;class_definition&amp;quot; | &amp;quot;if_statement&amp;quot; | &amp;quot;for_statement&amp;quot; | &amp;quot;while_statement&amp;quot; &#x3D;&amp;gt; {
                normalized_parts.push(node.kind().to_string());
            }
            &amp;quot;identifier&amp;quot; &#x3D;&amp;gt; {
                if let Ok(identifier) &#x3D; node.utf8_text(source.as_bytes()) {
                    // Normalize common identifier patterns
                    if identifier.len() &amp;gt; 1 &amp;amp;&amp;amp; !identifier.starts_with(&amp;quot;__&amp;quot;) {
                        normalized_parts.push(identifier.to_string());
                    }
                }
            }
            &amp;quot;string&amp;quot; | &amp;quot;integer&amp;quot; | &amp;quot;float&amp;quot; &#x3D;&amp;gt; {
                // Normalize literals to generic types
                normalized_parts.push(format!(&amp;quot;&amp;lt;{}&amp;gt;&amp;quot;, node.kind()));
            }
            _ &#x3D;&amp;gt; {}
        }
        
        // Process children
        let mut cursor &#x3D; node.walk();
        for child in node.children(&amp;amp;mut cursor) {
            self.normalize_ast_recursive(child, source, normalized_parts)?;
        }
        
        Ok(())
    }
}

impl Default for PythonAdapter {
    fn default() -&amp;gt; Self {
        Self::new().expect(&amp;quot;Failed to create Python adapter&amp;quot;)
    }
}

// Implement the LanguageAdapter trait for comprehensive AST analysis
#[async_trait]
impl LanguageAdapter for PythonAdapter {
    fn parse_source(&amp;amp;mut self, source: &amp;amp;str, file_path: &amp;amp;str) -&amp;gt; Result&amp;lt;ParseIndex&amp;gt; {
        // Use existing implementation
        PythonAdapter::parse_source(self, source, file_path)
    }
    
    fn extract_function_calls(&amp;amp;mut self, source: &amp;amp;str) -&amp;gt; Result&amp;lt;Vec&amp;lt;String&amp;gt;&amp;gt; {
        let tree &#x3D; self.parser.parse(source, None)
            .ok_or_else(|| ValknutError::parse(&amp;quot;python&amp;quot;, &amp;quot;Failed to parse Python source for function calls&amp;quot;))?;
        
        let mut calls &#x3D; Vec::new();
        let mut cursor &#x3D; tree.walk();
        
        self.extract_function_calls_recursive(tree.root_node(), source, &amp;amp;mut calls)?;
        
        calls.sort();
        calls.dedup();
        Ok(calls)
    }
    
    fn contains_boilerplate_patterns(&amp;amp;mut self, source: &amp;amp;str, patterns: &amp;amp;[String]) -&amp;gt; Result&amp;lt;Vec&amp;lt;String&amp;gt;&amp;gt; {
        let tree &#x3D; self.parser.parse(source, None)
            .ok_or_else(|| ValknutError::parse(&amp;quot;python&amp;quot;, &amp;quot;Failed to parse Python source for boilerplate analysis&amp;quot;))?;
        
        let mut found_patterns &#x3D; Vec::new();
        
        // Walk the AST looking for boilerplate patterns
        self.check_boilerplate_patterns_recursive(tree.root_node(), source, patterns, &amp;amp;mut found_patterns)?;
        
        found_patterns.sort();
        found_patterns.dedup();
        Ok(found_patterns)
    }
    
    fn extract_identifiers(&amp;amp;mut self, source: &amp;amp;str) -&amp;gt; Result&amp;lt;Vec&amp;lt;String&amp;gt;&amp;gt; {
        let tree &#x3D; self.parser.parse(source, None)
            .ok_or_else(|| ValknutError::parse(&amp;quot;python&amp;quot;, &amp;quot;Failed to parse Python source for identifiers&amp;quot;))?;
        
        let mut identifiers &#x3D; Vec::new();
        self.extract_identifiers_recursive(tree.root_node(), source, &amp;amp;mut identifiers)?;
        
        identifiers.sort();
        identifiers.dedup();
        Ok(identifiers)
    }
    
    fn count_ast_nodes(&amp;amp;mut self, source: &amp;amp;str) -&amp;gt; Result&amp;lt;usize&amp;gt; {
        let tree &#x3D; self.parser.parse(source, None)
            .ok_or_else(|| ValknutError::parse(&amp;quot;python&amp;quot;, &amp;quot;Failed to parse Python source for AST counting&amp;quot;))?;
        
        Ok(self.count_nodes_recursive(tree.root_node()))
    }
    
    fn count_distinct_blocks(&amp;amp;mut self, source: &amp;amp;str) -&amp;gt; Result&amp;lt;usize&amp;gt; {
        let tree &#x3D; self.parser.parse(source, None)
            .ok_or_else(|| ValknutError::parse(&amp;quot;python&amp;quot;, &amp;quot;Failed to parse Python source for block counting&amp;quot;))?;
        
        let mut block_count &#x3D; 0;
        self.count_blocks_recursive(tree.root_node(), &amp;amp;mut block_count);
        
        Ok(block_count.max(1))
    }
    
    fn normalize_source(&amp;amp;mut self, source: &amp;amp;str) -&amp;gt; Result&amp;lt;String&amp;gt; {
        let tree &#x3D; self.parser.parse(source, None)
            .ok_or_else(|| ValknutError::parse(&amp;quot;python&amp;quot;, &amp;quot;Failed to parse Python source for normalization&amp;quot;))?;
        
        let mut normalized_parts &#x3D; Vec::new();
        self.normalize_ast_recursive(tree.root_node(), source, &amp;amp;mut normalized_parts)?;
        
        Ok(normalized_parts.join(&amp;quot; &amp;quot;))
    }
    
    fn language_name(&amp;amp;self) -&amp;gt; &amp;amp;str {
        &amp;quot;python&amp;quot;
    }
}

</pre>
                </div>
            </div>
            <div class="file-section" id="file-6">
                <div class="file-header"><i data-lucide="file-code" class="icon"></i>core/pipeline/pipeline_config.rs</div>
                <div class="file-content">
                    <pre>//! Configuration types and defaults for the analysis pipeline.

use std::path::PathBuf;
use serde::{Deserialize, Serialize};

use crate::core::config::ValknutConfig;

/// Configuration for comprehensive analysis
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalysisConfig {
    /// Enable structure analysis
    pub enable_structure_analysis: bool,
    /// Enable complexity analysis
    pub enable_complexity_analysis: bool,
    /// Enable refactoring analysis
    pub enable_refactoring_analysis: bool,
    /// Enable impact analysis
    pub enable_impact_analysis: bool,
    /// Enable LSH-based clone detection
    pub enable_lsh_analysis: bool,
    /// Enable coverage analysis
    pub enable_coverage_analysis: bool,
    /// File extensions to include
    pub file_extensions: Vec&amp;lt;String&amp;gt;,
    /// Directories to exclude
    pub exclude_directories: Vec&amp;lt;String&amp;gt;,
    /// Maximum files to analyze (0 &#x3D; no limit)
    pub max_files: usize,
}

impl Default for AnalysisConfig {
    fn default() -&amp;gt; Self {
        Self {
            enable_structure_analysis: true,
            enable_complexity_analysis: true,
            enable_refactoring_analysis: true,
            enable_impact_analysis: true,
            enable_lsh_analysis: false, // Disabled by default
            enable_coverage_analysis: true, // Enabled by default for comprehensive analysis
            file_extensions: vec![
                &amp;quot;py&amp;quot;.to_string(),
                &amp;quot;js&amp;quot;.to_string(),
                &amp;quot;ts&amp;quot;.to_string(),
                &amp;quot;tsx&amp;quot;.to_string(),
                &amp;quot;jsx&amp;quot;.to_string(),
                &amp;quot;rs&amp;quot;.to_string(),
                &amp;quot;go&amp;quot;.to_string(),
                &amp;quot;java&amp;quot;.to_string(),
            ],
            exclude_directories: vec![
                &amp;quot;node_modules&amp;quot;.to_string(),
                &amp;quot;target&amp;quot;.to_string(),
                &amp;quot;__pycache__&amp;quot;.to_string(),
                &amp;quot;.git&amp;quot;.to_string(),
                &amp;quot;dist&amp;quot;.to_string(),
                &amp;quot;build&amp;quot;.to_string(),
            ],
            max_files: 5000,
        }
    }
}

impl From&amp;lt;ValknutConfig&amp;gt; for AnalysisConfig {
    fn from(config: ValknutConfig) -&amp;gt; Self {
        // Convert exclude patterns to directories - extract directory names from patterns
        let exclude_directories: Vec&amp;lt;String&amp;gt; &#x3D; config.analysis.exclude_patterns
            .into_iter()
            .filter_map(|pattern| {
                // Extract directory names from patterns like &amp;quot;*/node_modules/*&amp;quot; -&amp;gt; &amp;quot;node_modules&amp;quot;
                if pattern.contains(&amp;#x27;/&amp;#x27;) {
                    let trimmed &#x3D; pattern.trim_start_matches(&amp;quot;*/&amp;quot;).trim_end_matches(&amp;quot;/*&amp;quot;);
                    if !trimmed.is_empty() &amp;amp;&amp;amp; !trimmed.contains(&amp;#x27;*&amp;#x27;) {
                        Some(trimmed.to_string())
                    } else {
                        None
                    }
                } else {
                    None
                }
            })
            .collect();
        
        // Derive file extensions from language config
        let file_extensions: Vec&amp;lt;String&amp;gt; &#x3D; config.languages
            .values()
            .filter(|lang| lang.enabled)
            .flat_map(|lang| lang.file_extensions.clone())
            .map(|ext| ext.trim_start_matches(&amp;#x27;.&amp;#x27;).to_string()) // Remove leading dots
            .collect();
            
        let final_file_extensions &#x3D; if file_extensions.is_empty() {
            vec![
                &amp;quot;py&amp;quot;.to_string(),
                &amp;quot;js&amp;quot;.to_string(),
                &amp;quot;ts&amp;quot;.to_string(),
                &amp;quot;tsx&amp;quot;.to_string(),
                &amp;quot;jsx&amp;quot;.to_string(),
                &amp;quot;rs&amp;quot;.to_string(),
                &amp;quot;go&amp;quot;.to_string(),
                &amp;quot;java&amp;quot;.to_string(),
            ]
        } else {
            file_extensions
        };
        
        let final_exclude_directories &#x3D; if exclude_directories.is_empty() {
            vec![
                &amp;quot;node_modules&amp;quot;.to_string(),
                &amp;quot;target&amp;quot;.to_string(),
                &amp;quot;__pycache__&amp;quot;.to_string(),
                &amp;quot;.git&amp;quot;.to_string(),
                &amp;quot;dist&amp;quot;.to_string(),
                &amp;quot;build&amp;quot;.to_string(),
            ]
        } else {
            exclude_directories
        };
        
        Self {
            enable_structure_analysis: config.analysis.enable_structure_analysis,
            enable_complexity_analysis: true, // Default enabled, no equivalent in core config
            enable_refactoring_analysis: config.analysis.enable_refactoring_analysis,
            enable_impact_analysis: config.analysis.enable_graph_analysis, // Map graph analysis to impact analysis
            enable_lsh_analysis: config.analysis.enable_lsh_analysis,
            enable_coverage_analysis: config.analysis.enable_coverage_analysis,
            file_extensions: final_file_extensions,
            exclude_directories: final_exclude_directories,
            max_files: config.analysis.max_files,
        }
    }
}

/// Quality gate configuration for CI/CD integration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QualityGateConfig {
    /// Whether quality gates are enabled
    pub enabled: bool,
    /// Maximum allowed complexity score (0-100, lower is better)
    pub max_complexity_score: f64,
    /// Maximum allowed technical debt ratio (0-100, lower is better)
    pub max_technical_debt_ratio: f64,
    /// Minimum required maintainability score (0-100, higher is better)
    pub min_maintainability_score: f64,
    /// Maximum allowed critical issues
    pub max_critical_issues: usize,
    /// Maximum allowed high-priority issues
    pub max_high_priority_issues: usize,
}

impl Default for QualityGateConfig {
    fn default() -&amp;gt; Self {
        Self {
            enabled: false,
            max_complexity_score: 70.0,
            max_technical_debt_ratio: 50.0,
            min_maintainability_score: 60.0,
            max_critical_issues: 5,
            max_high_priority_issues: 20,
        }
    }
}

/// Quality gate violation details
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QualityGateViolation {
    /// Name of the violated rule
    pub rule_name: String,
    /// Description of the violation
    pub description: String,
    /// Current value that violated the threshold
    pub current_value: f64,
    /// The threshold that was violated
    pub threshold: f64,
    /// Severity of the violation
    pub severity: String,
    /// Files or components that contribute to this violation
    pub affected_files: Vec&amp;lt;PathBuf&amp;gt;,
    /// Recommended actions to fix this violation
    pub recommended_actions: Vec&amp;lt;String&amp;gt;,
}

/// Result of quality gate evaluation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QualityGateResult {
    /// Whether all quality gates passed
    pub passed: bool,
    /// List of violations (empty if all gates passed)
    pub violations: Vec&amp;lt;QualityGateViolation&amp;gt;,
    /// Overall quality score
    pub overall_score: f64,
}</pre>
                </div>
            </div>
            <div class="file-section" id="file-7">
                <div class="file-header"><i data-lucide="file-code" class="icon"></i>core/scoring.rs</div>
                <div class="file-content">
                    <pre>//! Feature normalization and scoring system.
//!
//! This module provides comprehensive scoring and normalization capabilities
//! for code analysis features, with support for various normalization schemes
//! including Bayesian approaches for handling challenging statistical cases.

use std::collections::HashMap;

use rayon::prelude::*;
use serde::{Deserialize, Serialize};

use crate::core::config::{ScoringConfig, NormalizationScheme, WeightsConfig};
use crate::core::featureset::FeatureVector;
use crate::core::bayesian::BayesianNormalizer;
use crate::core::errors::{Result, ValknutError};

/// Main feature normalization engine that supports multiple schemes
#[derive(Debug)]
pub struct FeatureNormalizer {
    /// Configuration for this normalizer
    config: ScoringConfig,
    
    /// Statistical measures for each feature (non-Bayesian schemes)
    statistics: HashMap&amp;lt;String, NormalizationStatistics&amp;gt;,
    
    /// Bayesian normalizer (if using Bayesian schemes)
    bayesian_normalizer: Option&amp;lt;BayesianNormalizer&amp;gt;,
}

/// Statistical measures used for normalization
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NormalizationStatistics {
    /// Sample mean
    pub mean: f64,
    /// Sample variance
    pub variance: f64,
    /// Sample standard deviation
    pub std_dev: f64,
    /// Minimum value observed
    pub min: f64,
    /// Maximum value observed
    pub max: f64,
    /// Number of samples
    pub n_samples: usize,
    /// Median (for robust normalization)
    pub median: f64,
    /// Median Absolute Deviation (for robust normalization)
    pub mad: f64,
    /// 25th percentile
    pub q1: f64,
    /// 75th percentile
    pub q3: f64,
    /// Interquartile range
    pub iqr: f64,
}

impl NormalizationStatistics {
    /// Calculate statistics from a vector of values
    pub fn from_values(mut values: Vec&amp;lt;f64&amp;gt;) -&amp;gt; Self {
        let n &#x3D; values.len();
        
        if n &#x3D;&#x3D; 0 {
            return Self::empty();
        }
        
        // Sort for percentile calculations
        values.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));
        
        // Basic statistics
        let sum: f64 &#x3D; values.iter().sum();
        let mean &#x3D; sum / n as f64;
        let variance &#x3D; if n &amp;gt; 1 {
            values.iter().map(|x| (x - mean).powi(2)).sum::&amp;lt;f64&amp;gt;() / (n - 1) as f64
        } else {
            0.0
        };
        let std_dev &#x3D; variance.sqrt();
        let min &#x3D; values[0];
        let max &#x3D; values[n - 1];
        
        // Percentiles
        let median &#x3D; Self::percentile(&amp;amp;values, 0.5);
        let q1 &#x3D; Self::percentile(&amp;amp;values, 0.25);
        let q3 &#x3D; Self::percentile(&amp;amp;values, 0.75);
        let iqr &#x3D; q3 - q1;
        
        // Median Absolute Deviation
        let deviations: Vec&amp;lt;f64&amp;gt; &#x3D; values.iter().map(|x| (x - median).abs()).collect();
        let mad &#x3D; Self::median_of_slice(&amp;amp;deviations);
        
        Self {
            mean,
            variance,
            std_dev,
            min,
            max,
            n_samples: n,
            median,
            mad,
            q1,
            q3,
            iqr,
        }
    }
    
    /// Create empty statistics
    pub fn empty() -&amp;gt; Self {
        Self {
            mean: 0.0,
            variance: 0.0,
            std_dev: 0.0,
            min: 0.0,
            max: 0.0,
            n_samples: 0,
            median: 0.0,
            mad: 0.0,
            q1: 0.0,
            q3: 0.0,
            iqr: 0.0,
        }
    }
    
    /// Calculate percentile of sorted values
    fn percentile(sorted_values: &amp;amp;[f64], p: f64) -&amp;gt; f64 {
        if sorted_values.is_empty() {
            return 0.0;
        }
        
        let n &#x3D; sorted_values.len();
        let index &#x3D; p * (n - 1) as f64;
        let lower_index &#x3D; index.floor() as usize;
        let upper_index &#x3D; index.ceil() as usize;
        
        if lower_index &#x3D;&#x3D; upper_index || upper_index &amp;gt;&#x3D; n {
            sorted_values[lower_index.min(n - 1)]
        } else {
            let weight &#x3D; index - lower_index as f64;
            sorted_values[lower_index] * (1.0 - weight) + sorted_values[upper_index] * weight
        }
    }
    
    /// Calculate median of a slice
    fn median_of_slice(values: &amp;amp;[f64]) -&amp;gt; f64 {
        let mut sorted &#x3D; values.to_vec();
        sorted.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Equal));
        Self::percentile(&amp;amp;sorted, 0.5)
    }
}

impl FeatureNormalizer {
    /// Create a new feature normalizer with the given configuration
    pub fn new(config: ScoringConfig) -&amp;gt; Self {
        let bayesian_normalizer &#x3D; if config.normalization_scheme.to_string().ends_with(&amp;quot;_bayesian&amp;quot;) 
            || config.use_bayesian_fallbacks 
        {
            Some(BayesianNormalizer::new(config.normalization_scheme.to_string()))
        } else {
            None
        };
        
        Self {
            config,
            statistics: HashMap::new(),
            bayesian_normalizer,
        }
    }
    
    /// Fit the normalizer to a collection of feature vectors
    pub fn fit(&amp;amp;mut self, feature_vectors: &amp;amp;[FeatureVector]) -&amp;gt; Result&amp;lt;()&amp;gt; {
        if feature_vectors.is_empty() {
            return Err(ValknutError::validation(&amp;quot;No feature vectors provided for normalization fitting&amp;quot;));
        }
        
        // If using Bayesian normalizer, delegate fitting
        if let Some(ref mut bayesian) &#x3D; self.bayesian_normalizer {
            bayesian.fit(feature_vectors)?;
            
            // Optionally report confidence diagnostics
            if self.config.confidence_reporting {
                self.report_bayesian_diagnostics();
            }
            return Ok(());
        }
        
        // Collect feature values for classical statistics
        let mut feature_values: HashMap&amp;lt;String, Vec&amp;lt;f64&amp;gt;&amp;gt; &#x3D; HashMap::new();
        for vector in feature_vectors {
            for (feature_name, &amp;amp;value) in &amp;amp;vector.features {
                feature_values.entry(feature_name.clone()).or_default().push(value);
            }
        }
        
        // Calculate classical statistics for each feature
        self.statistics &#x3D; feature_values
            .into_par_iter()
            .map(|(feature_name, values)| {
                let stats &#x3D; NormalizationStatistics::from_values(values);
                (feature_name, stats)
            })
            .collect();
        
        Ok(())
    }
    
    /// Normalize feature vectors using the fitted statistics
    pub fn normalize(&amp;amp;self, feature_vectors: &amp;amp;mut [FeatureVector]) -&amp;gt; Result&amp;lt;()&amp;gt; {
        // If using Bayesian normalizer, delegate normalization
        if let Some(ref bayesian) &#x3D; self.bayesian_normalizer {
            return bayesian.normalize(feature_vectors);
        }
        
        // Classical normalization
        feature_vectors.par_iter_mut().try_for_each(|vector| {
            for (feature_name, &amp;amp;value) in vector.features.clone().iter() {
                if let Some(stats) &#x3D; self.statistics.get(feature_name) {
                    let normalized_value &#x3D; self.normalize_value(value, stats)?;
                    vector.normalized_features.insert(feature_name.clone(), normalized_value);
                } else {
                    // No statistics available - use identity
                    vector.normalized_features.insert(feature_name.clone(), value);
                }
            }
            Ok::&amp;lt;(), ValknutError&amp;gt;(())
        })?;
        
        Ok(())
    }
    
    /// Normalize a single value using the specified scheme and statistics
    fn normalize_value(&amp;amp;self, value: f64, stats: &amp;amp;NormalizationStatistics) -&amp;gt; Result&amp;lt;f64&amp;gt; {
        if value.is_nan() || value.is_infinite() {
            return Ok(0.0);
        }
        
        let normalized &#x3D; match self.config.normalization_scheme {
            NormalizationScheme::ZScore &#x3D;&amp;gt; {
                if stats.variance &amp;lt; f64::EPSILON {
                    // Handle zero variance case
                    if self.config.use_bayesian_fallbacks {
                        // Use Bayesian fallback if available
                        self.bayesian_fallback_normalize(value, stats)
                    } else {
                        0.0
                    }
                } else {
                    (value - stats.mean) / stats.std_dev
                }
            }
            
            NormalizationScheme::MinMax &#x3D;&amp;gt; {
                let range &#x3D; stats.max - stats.min;
                if range &amp;lt; f64::EPSILON {
                    // Handle zero range case
                    if self.config.use_bayesian_fallbacks {
                        self.bayesian_fallback_normalize(value, stats)
                    } else {
                        0.5  // Middle of [0, 1] range
                    }
                } else {
                    (value - stats.min) / range
                }
            }
            
            NormalizationScheme::Robust &#x3D;&amp;gt; {
                if stats.mad &amp;lt; f64::EPSILON {
                    // Fallback to IQR if MAD is zero
                    if stats.iqr &amp;lt; f64::EPSILON {
                        if self.config.use_bayesian_fallbacks {
                            self.bayesian_fallback_normalize(value, stats)
                        } else {
                            0.0
                        }
                    } else {
                        (value - stats.median) / stats.iqr
                    }
                } else {
                    // Standard robust normalization using median and MAD
                    (value - stats.median) / (1.4826 * stats.mad)  // 1.4826 makes MAD consistent with std dev
                }
            }
            
            // Bayesian schemes should not reach here due to earlier delegation
            NormalizationScheme::ZScoreBayesian |
            NormalizationScheme::MinMaxBayesian |
            NormalizationScheme::RobustBayesian &#x3D;&amp;gt; {
                return Err(ValknutError::internal(&amp;quot;Bayesian normalization should be handled by BayesianNormalizer&amp;quot;));
            }
        };
        
        Ok(normalized.clamp(-10.0, 10.0))  // Prevent extreme outliers
    }
    
    /// Bayesian fallback for zero variance cases
    fn bayesian_fallback_normalize(&amp;amp;self, _value: f64, _stats: &amp;amp;NormalizationStatistics) -&amp;gt; f64 {
        // Simple fallback - can be enhanced with proper Bayesian inference
        // This would ideally use domain knowledge to generate reasonable normalized values
        0.0
    }
    
    /// Report Bayesian diagnostics if enabled
    fn report_bayesian_diagnostics(&amp;amp;self) {
        if let Some(ref bayesian) &#x3D; self.bayesian_normalizer {
            let diagnostics &#x3D; bayesian.get_diagnostics();
            tracing::info!(&amp;quot;Bayesian normalization diagnostics: {:#?}&amp;quot;, diagnostics);
        }
    }
    
    /// Get statistics for a specific feature
    pub fn get_statistics(&amp;amp;self, feature_name: &amp;amp;str) -&amp;gt; Option&amp;lt;&amp;amp;NormalizationStatistics&amp;gt; {
        self.statistics.get(feature_name)
    }
    
    /// Get all normalization statistics
    pub fn get_all_statistics(&amp;amp;self) -&amp;gt; &amp;amp;HashMap&amp;lt;String, NormalizationStatistics&amp;gt; {
        &amp;amp;self.statistics
    }
    
    /// Get the Bayesian normalizer if available
    pub fn get_bayesian_normalizer(&amp;amp;self) -&amp;gt; Option&amp;lt;&amp;amp;BayesianNormalizer&amp;gt; {
        self.bayesian_normalizer.as_ref()
    }
}

/// Feature scoring engine that combines normalization with weighted scoring
#[derive(Debug)]
pub struct FeatureScorer {
    /// Normalizer for feature preprocessing
    normalizer: FeatureNormalizer,
    
    /// Feature weights configuration
    weights: WeightsConfig,
}

impl FeatureScorer {
    /// Create a new feature scorer
    pub fn new(config: ScoringConfig) -&amp;gt; Self {
        Self {
            normalizer: FeatureNormalizer::new(config.clone()),
            weights: config.weights,
        }
    }
    
    /// Fit the scorer to training data
    pub fn fit(&amp;amp;mut self, feature_vectors: &amp;amp;[FeatureVector]) -&amp;gt; Result&amp;lt;()&amp;gt; {
        self.normalizer.fit(feature_vectors)
    }
    
    /// Score feature vectors, returning normalized and weighted scores
    pub fn score(&amp;amp;self, feature_vectors: &amp;amp;mut [FeatureVector]) -&amp;gt; Result&amp;lt;Vec&amp;lt;ScoringResult&amp;gt;&amp;gt; {
        // First normalize all features
        self.normalizer.normalize(feature_vectors)?;
        
        // Then compute weighted scores
        let results: Result&amp;lt;Vec&amp;lt;ScoringResult&amp;gt;&amp;gt; &#x3D; feature_vectors
            .par_iter()
            .map(|vector| self.compute_scores(vector))
            .collect();
        
        results
    }

    /// Score a single feature vector (optimized for parallel processing)
    pub fn score_single(&amp;amp;self, vector: &amp;amp;FeatureVector) -&amp;gt; Result&amp;lt;ScoringResult&amp;gt; {
        // Create a mutable copy for normalization
        let mut normalized_vector &#x3D; vector.clone();
        
        // Normalize this single vector
        self.normalizer.normalize(std::slice::from_mut(&amp;amp;mut normalized_vector))?;
        
        // Compute scores
        self.compute_scores(&amp;amp;normalized_vector)
    }
    
    /// Compute scoring results for a single feature vector
    fn compute_scores(&amp;amp;self, vector: &amp;amp;FeatureVector) -&amp;gt; Result&amp;lt;ScoringResult&amp;gt; {
        let mut category_scores &#x3D; HashMap::new();
        let mut feature_contributions &#x3D; HashMap::new();
        
        // Calculate category scores based on feature weights
        let mut total_weighted_score &#x3D; 0.0;
        let mut total_weight &#x3D; 0.0;
        
        for (feature_name, &amp;amp;normalized_value) in &amp;amp;vector.normalized_features {
            let (category, weight) &#x3D; self.get_feature_category_and_weight(feature_name);
            
            let contribution &#x3D; normalized_value * weight;
            feature_contributions.insert(feature_name.clone(), contribution);
            
            // Accumulate category score
            *category_scores.entry(category.clone()).or_insert(0.0) +&#x3D; contribution;
            
            // Accumulate total
            total_weighted_score +&#x3D; contribution;
            total_weight +&#x3D; weight;
        }
        
        // Normalize category scores by their total weight
        for (category, score) in &amp;amp;mut category_scores {
            let category_weight &#x3D; self.get_category_weight(category);
            if category_weight &amp;gt; 0.0 {
                *score /&#x3D; category_weight;
            }
        }
        
        // Calculate overall refactoring priority score
        let overall_score &#x3D; if total_weight &amp;gt; 0.0 {
            total_weighted_score / total_weight
        } else {
            0.0
        };
        
        // Determine priority level
        let priority &#x3D; Self::calculate_priority(overall_score);
        
        Ok(ScoringResult {
            entity_id: vector.entity_id.clone(),
            overall_score,
            priority,
            category_scores,
            feature_contributions,
            normalized_feature_count: vector.normalized_features.len(),
            confidence: self.calculate_confidence(vector),
        })
    }
    
    /// Get the category and weight for a feature
    fn get_feature_category_and_weight(&amp;amp;self, feature_name: &amp;amp;str) -&amp;gt; (String, f64) {
        // Map feature names to categories and return corresponding weights
        let category &#x3D; match feature_name {
            name if name.contains(&amp;quot;cyclomatic&amp;quot;) || name.contains(&amp;quot;cognitive&amp;quot;) || name.contains(&amp;quot;complexity&amp;quot;) &#x3D;&amp;gt; {
                (&amp;quot;complexity&amp;quot;.to_string(), self.weights.complexity)
            }
            name if name.contains(&amp;quot;betweenness&amp;quot;) || name.contains(&amp;quot;centrality&amp;quot;) || name.contains(&amp;quot;fan_&amp;quot;) &#x3D;&amp;gt; {
                (&amp;quot;graph&amp;quot;.to_string(), self.weights.graph)
            }
            name if name.contains(&amp;quot;structure&amp;quot;) || name.contains(&amp;quot;class&amp;quot;) || name.contains(&amp;quot;method&amp;quot;) &#x3D;&amp;gt; {
                (&amp;quot;structure&amp;quot;.to_string(), self.weights.structure)
            }
            name if name.contains(&amp;quot;style&amp;quot;) || name.contains(&amp;quot;naming&amp;quot;) || name.contains(&amp;quot;format&amp;quot;) &#x3D;&amp;gt; {
                (&amp;quot;style&amp;quot;.to_string(), self.weights.style)
            }
            name if name.contains(&amp;quot;coverage&amp;quot;) || name.contains(&amp;quot;test&amp;quot;) &#x3D;&amp;gt; {
                (&amp;quot;coverage&amp;quot;.to_string(), self.weights.coverage)
            }
            _ &#x3D;&amp;gt; (&amp;quot;other&amp;quot;.to_string(), 1.0)
        };
        
        category
    }
    
    /// Get the total weight for a category
    fn get_category_weight(&amp;amp;self, category: &amp;amp;str) -&amp;gt; f64 {
        match category {
            &amp;quot;complexity&amp;quot; &#x3D;&amp;gt; self.weights.complexity,
            &amp;quot;graph&amp;quot; &#x3D;&amp;gt; self.weights.graph,
            &amp;quot;structure&amp;quot; &#x3D;&amp;gt; self.weights.structure,
            &amp;quot;style&amp;quot; &#x3D;&amp;gt; self.weights.style,
            &amp;quot;coverage&amp;quot; &#x3D;&amp;gt; self.weights.coverage,
            _ &#x3D;&amp;gt; 1.0,
        }
    }
    
    /// Calculate priority level from overall score
    fn calculate_priority(score: f64) -&amp;gt; Priority {
        let abs_score &#x3D; score.abs();
        
        if abs_score &amp;gt;&#x3D; 2.0 {
            Priority::Critical
        } else if abs_score &amp;gt;&#x3D; 1.5 {
            Priority::High
        } else if abs_score &amp;gt;&#x3D; 1.0 {
            Priority::Medium
        } else if abs_score &amp;gt;&#x3D; 0.5 {
            Priority::Low
        } else {
            Priority::None
        }
    }
    
    /// Calculate confidence in the scoring result
    fn calculate_confidence(&amp;amp;self, vector: &amp;amp;FeatureVector) -&amp;gt; f64 {
        let feature_count &#x3D; vector.normalized_features.len() as f64;
        let base_confidence &#x3D; (feature_count / 10.0).min(1.0);  // More features &#x3D; higher confidence
        
        // Adjust based on Bayesian confidence if available
        if let Some(bayesian) &#x3D; self.normalizer.get_bayesian_normalizer() {
            let mut confidence_sum &#x3D; 0.0;
            let mut confidence_count &#x3D; 0;
            
            for feature_name in vector.normalized_features.keys() {
                if let Some(confidence) &#x3D; bayesian.get_confidence(feature_name) {
                    confidence_sum +&#x3D; confidence.score();
                    confidence_count +&#x3D; 1;
                }
            }
            
            if confidence_count &amp;gt; 0 {
                let avg_bayesian_confidence &#x3D; confidence_sum / confidence_count as f64;
                base_confidence * avg_bayesian_confidence
            } else {
                base_confidence
            }
        } else {
            base_confidence
        }
    }
    
    /// Get the underlying normalizer
    pub fn get_normalizer(&amp;amp;self) -&amp;gt; &amp;amp;FeatureNormalizer {
        &amp;amp;self.normalizer
    }
}

/// Priority levels for refactoring suggestions
#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq, PartialOrd, Ord)]
pub enum Priority {
    /// No refactoring needed
    None,
    /// Low priority refactoring
    Low,
    /// Medium priority refactoring
    Medium,
    /// High priority refactoring
    High,
    /// Critical refactoring required
    Critical,
}

impl Priority {
    /// Get numeric priority value
    pub fn value(self) -&amp;gt; f64 {
        match self {
            Self::None &#x3D;&amp;gt; 0.0,
            Self::Low &#x3D;&amp;gt; 0.25,
            Self::Medium &#x3D;&amp;gt; 0.5,
            Self::High &#x3D;&amp;gt; 0.75,
            Self::Critical &#x3D;&amp;gt; 1.0,
        }
    }
}

/// Result of feature scoring for an entity
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ScoringResult {
    /// Entity identifier
    pub entity_id: String,
    
    /// Overall refactoring priority score
    pub overall_score: f64,
    
    /// Priority level
    pub priority: Priority,
    
    /// Scores broken down by feature category
    pub category_scores: HashMap&amp;lt;String, f64&amp;gt;,
    
    /// Individual feature contributions to the score
    pub feature_contributions: HashMap&amp;lt;String, f64&amp;gt;,
    
    /// Number of normalized features used in scoring
    pub normalized_feature_count: usize,
    
    /// Confidence in the scoring result (0.0-1.0)
    pub confidence: f64,
}

impl ScoringResult {
    /// Check if this entity needs refactoring
    pub fn needs_refactoring(&amp;amp;self) -&amp;gt; bool {
        self.priority !&#x3D; Priority::None
    }
    
    /// Check if this is a high-priority refactoring candidate
    pub fn is_high_priority(&amp;amp;self) -&amp;gt; bool {
        matches!(self.priority, Priority::High | Priority::Critical)
    }
    
    /// Get the dominant feature category (highest scoring)
    pub fn dominant_category(&amp;amp;self) -&amp;gt; Option&amp;lt;(String, f64)&amp;gt; {
        self.category_scores
            .iter()
            .max_by(|a, b| a.1.partial_cmp(b.1).unwrap_or(std::cmp::Ordering::Equal))
            .map(|(k, v)| (k.clone(), *v))
    }
    
    /// Get the top contributing features
    pub fn top_contributing_features(&amp;amp;self, count: usize) -&amp;gt; Vec&amp;lt;(String, f64)&amp;gt; {
        let mut contributions: Vec&amp;lt;_&amp;gt; &#x3D; self.feature_contributions.iter()
            .map(|(k, v)| (k.clone(), *v))
            .collect();
        contributions.sort_by(|a, b| b.1.partial_cmp(&amp;amp;a.1).unwrap_or(std::cmp::Ordering::Equal));
        contributions.into_iter().take(count).collect()
    }
}

// Extension trait for NormalizationScheme to convert to string
trait NormalizationSchemeExt {
    fn to_string(&amp;amp;self) -&amp;gt; String;
}

impl NormalizationSchemeExt for NormalizationScheme {
    fn to_string(&amp;amp;self) -&amp;gt; String {
        match self {
            Self::ZScore &#x3D;&amp;gt; &amp;quot;z_score&amp;quot;.to_string(),
            Self::MinMax &#x3D;&amp;gt; &amp;quot;min_max&amp;quot;.to_string(),
            Self::Robust &#x3D;&amp;gt; &amp;quot;robust&amp;quot;.to_string(),
            Self::ZScoreBayesian &#x3D;&amp;gt; &amp;quot;z_score_bayesian&amp;quot;.to_string(),
            Self::MinMaxBayesian &#x3D;&amp;gt; &amp;quot;min_max_bayesian&amp;quot;.to_string(),
            Self::RobustBayesian &#x3D;&amp;gt; &amp;quot;robust_bayesian&amp;quot;.to_string(),
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::core::config::{ScoringConfig, NormalizationScheme, WeightsConfig};

    fn create_test_config() -&amp;gt; ScoringConfig {
        ScoringConfig {
            normalization_scheme: NormalizationScheme::ZScore,
            use_bayesian_fallbacks: false,
            confidence_reporting: false,
            weights: WeightsConfig::default(),
            statistical_params: crate::core::config::StatisticalParams::default(),
        }
    }

    #[test]
    fn test_normalization_statistics() {
        let values &#x3D; vec![1.0, 2.0, 3.0, 4.0, 5.0];
        let stats &#x3D; NormalizationStatistics::from_values(values);
        
        assert_eq!(stats.mean, 3.0);
        assert_eq!(stats.median, 3.0);
        assert_eq!(stats.min, 1.0);
        assert_eq!(stats.max, 5.0);
        assert!(stats.variance &amp;gt; 0.0);
    }
    
    #[test]
    fn test_feature_normalizer() {
        let config &#x3D; create_test_config();
        let mut normalizer &#x3D; FeatureNormalizer::new(config);
        
        let mut vectors &#x3D; vec![
            FeatureVector::new(&amp;quot;entity1&amp;quot;),
            FeatureVector::new(&amp;quot;entity2&amp;quot;),
            FeatureVector::new(&amp;quot;entity3&amp;quot;),
        ];
        
        vectors[0].add_feature(&amp;quot;complexity&amp;quot;, 1.0);
        vectors[1].add_feature(&amp;quot;complexity&amp;quot;, 5.0);
        vectors[2].add_feature(&amp;quot;complexity&amp;quot;, 3.0);
        
        // Fit and normalize
        normalizer.fit(&amp;amp;vectors).unwrap();
        normalizer.normalize(&amp;amp;mut vectors).unwrap();
        
        // Check that normalization was applied
        assert!(vectors[0].normalized_features.contains_key(&amp;quot;complexity&amp;quot;));
        assert!(vectors[1].normalized_features.contains_key(&amp;quot;complexity&amp;quot;));
        assert!(vectors[2].normalized_features.contains_key(&amp;quot;complexity&amp;quot;));
        
        // Mean should be approximately 0
        let normalized_values: Vec&amp;lt;f64&amp;gt; &#x3D; vectors.iter()
            .map(|v| v.normalized_features[&amp;quot;complexity&amp;quot;])
            .collect();
        let mean: f64 &#x3D; normalized_values.iter().sum::&amp;lt;f64&amp;gt;() / normalized_values.len() as f64;
        assert!((mean.abs() &amp;lt; 0.1), &amp;quot;Mean should be close to 0, got {}&amp;quot;, mean);
    }
    
    #[test]
    fn test_feature_scorer() {
        let config &#x3D; create_test_config();
        let mut scorer &#x3D; FeatureScorer::new(config);
        
        let mut vectors &#x3D; vec![
            FeatureVector::new(&amp;quot;high_complexity&amp;quot;),
            FeatureVector::new(&amp;quot;low_complexity&amp;quot;),
        ];
        
        vectors[0].add_feature(&amp;quot;cyclomatic&amp;quot;, 10.0);
        vectors[0].add_feature(&amp;quot;fan_out&amp;quot;, 15.0);
        
        vectors[1].add_feature(&amp;quot;cyclomatic&amp;quot;, 2.0);
        vectors[1].add_feature(&amp;quot;fan_out&amp;quot;, 3.0);
        
        // Fit and score
        scorer.fit(&amp;amp;vectors).unwrap();
        let results &#x3D; scorer.score(&amp;amp;mut vectors).unwrap();
        
        assert_eq!(results.len(), 2);
        
        // High complexity entity should have higher score
        let high_result &#x3D; &amp;amp;results[0];
        let low_result &#x3D; &amp;amp;results[1];
        
        assert!(high_result.overall_score &amp;gt; low_result.overall_score);
        assert!(high_result.priority !&#x3D; Priority::None);
    }
    
    #[test]
    fn test_priority_calculation() {
        assert_eq!(FeatureScorer::calculate_priority(2.5), Priority::Critical);
        assert_eq!(FeatureScorer::calculate_priority(1.7), Priority::High);
        assert_eq!(FeatureScorer::calculate_priority(1.2), Priority::Medium);
        assert_eq!(FeatureScorer::calculate_priority(0.8), Priority::Low);
        assert_eq!(FeatureScorer::calculate_priority(0.3), Priority::None);
    }
    
    #[test]
    fn test_scoring_result() {
        let mut result &#x3D; ScoringResult {
            entity_id: &amp;quot;test&amp;quot;.to_string(),
            overall_score: 1.5,
            priority: Priority::High,
            category_scores: HashMap::new(),
            feature_contributions: HashMap::new(),
            normalized_feature_count: 5,
            confidence: 0.8,
        };
        
        result.category_scores.insert(&amp;quot;complexity&amp;quot;.to_string(), 2.0);
        result.category_scores.insert(&amp;quot;structure&amp;quot;.to_string(), 1.0);
        
        result.feature_contributions.insert(&amp;quot;cyclomatic&amp;quot;.to_string(), 1.5);
        result.feature_contributions.insert(&amp;quot;fan_out&amp;quot;.to_string(), 0.8);
        
        assert!(result.needs_refactoring());
        assert!(result.is_high_priority());
        
        let dominant &#x3D; result.dominant_category().unwrap();
        assert_eq!(dominant.0, &amp;quot;complexity&amp;quot;);
        assert_eq!(dominant.1, 2.0);
        
        let top_features &#x3D; result.top_contributing_features(1);
        assert_eq!(top_features[0].0, &amp;quot;cyclomatic&amp;quot;);
    }

    #[test]
    fn test_feature_normalizer_normalize_value() {
        let config &#x3D; create_test_config();
        let mut normalizer &#x3D; FeatureNormalizer::new(config);
        
        let mut vectors &#x3D; vec![
            FeatureVector::new(&amp;quot;entity1&amp;quot;),
            FeatureVector::new(&amp;quot;entity2&amp;quot;),
        ];
        
        vectors[0].add_feature(&amp;quot;complexity&amp;quot;, 2.0);
        vectors[1].add_feature(&amp;quot;complexity&amp;quot;, 8.0);
        
        normalizer.fit(&amp;amp;vectors).unwrap();
        
        let stats &#x3D; NormalizationStatistics {
            mean: 3.0,
            variance: 1.0,
            std_dev: 1.0,
            min: 1.0,
            max: 5.0,
            n_samples: 10,
            median: 3.0,
            mad: 0.5,
            q1: 2.0,
            q3: 4.0,
            iqr: 2.0,
        };
        let normalized &#x3D; normalizer.normalize_value(5.0, &amp;amp;stats);
        assert!(normalized.is_ok());
        let value &#x3D; normalized.unwrap();
        assert!(value &amp;gt;&#x3D; -3.0 &amp;amp;&amp;amp; value &amp;lt;&#x3D; 3.0); // Should be reasonable z-score
    }

    #[test]
    fn test_feature_normalizer_get_statistics() {
        let config &#x3D; create_test_config();
        let mut normalizer &#x3D; FeatureNormalizer::new(config);
        
        let mut vectors &#x3D; vec![
            FeatureVector::new(&amp;quot;entity1&amp;quot;),
            FeatureVector::new(&amp;quot;entity2&amp;quot;),
        ];
        
        vectors[0].add_feature(&amp;quot;complexity&amp;quot;, 1.0);
        vectors[1].add_feature(&amp;quot;complexity&amp;quot;, 9.0);
        
        normalizer.fit(&amp;amp;vectors).unwrap();
        
        let stats &#x3D; normalizer.get_statistics(&amp;quot;complexity&amp;quot;);
        assert!(stats.is_some());
        let stats &#x3D; stats.unwrap();
        assert_eq!(stats.mean, 5.0);
        assert_eq!(stats.min, 1.0);
        assert_eq!(stats.max, 9.0);
    }

    #[test]
    fn test_feature_normalizer_get_all_statistics() {
        let config &#x3D; create_test_config();
        let mut normalizer &#x3D; FeatureNormalizer::new(config);
        
        let mut vectors &#x3D; vec![
            FeatureVector::new(&amp;quot;entity1&amp;quot;),
            FeatureVector::new(&amp;quot;entity2&amp;quot;),
        ];
        
        vectors[0].add_feature(&amp;quot;complexity&amp;quot;, 1.0);
        vectors[0].add_feature(&amp;quot;length&amp;quot;, 10.0);
        vectors[1].add_feature(&amp;quot;complexity&amp;quot;, 5.0);
        vectors[1].add_feature(&amp;quot;length&amp;quot;, 50.0);
        
        normalizer.fit(&amp;amp;vectors).unwrap();
        
        let all_stats &#x3D; normalizer.get_all_statistics();
        assert_eq!(all_stats.len(), 2);
        assert!(all_stats.contains_key(&amp;quot;complexity&amp;quot;));
        assert!(all_stats.contains_key(&amp;quot;length&amp;quot;));
    }

    #[test]
    fn test_normalization_statistics_empty() {
        let stats &#x3D; NormalizationStatistics::empty();
        
        assert_eq!(stats.mean, 0.0);
        assert_eq!(stats.median, 0.0);
        assert_eq!(stats.std_dev, 0.0);
        assert_eq!(stats.min, 0.0);
        assert_eq!(stats.max, 0.0);
        assert_eq!(stats.n_samples, 0);
    }

    #[test]
    fn test_normalization_statistics_percentile() {
        let values &#x3D; vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0];
        let stats &#x3D; NormalizationStatistics::from_values(values);
        
        let values &#x3D; vec![1.0, 2.0, 3.0, 4.0, 5.0];
        let p25 &#x3D; NormalizationStatistics::percentile(&amp;amp;values, 0.25);
        let p50 &#x3D; NormalizationStatistics::percentile(&amp;amp;values, 0.50);
        let p75 &#x3D; NormalizationStatistics::percentile(&amp;amp;values, 0.75);
        
        assert!(p25 &amp;lt; p50);
        assert!(p50 &amp;lt; p75);
        assert_eq!(p50, 3.0); // Median of [1,2,3,4,5]
    }

    #[test]
    fn test_feature_scorer_compute_scores() {
        let config &#x3D; create_test_config();
        let mut scorer &#x3D; FeatureScorer::new(config);
        
        let mut vectors &#x3D; vec![
            FeatureVector::new(&amp;quot;entity1&amp;quot;),
            FeatureVector::new(&amp;quot;entity2&amp;quot;),
        ];
        
        vectors[0].add_feature(&amp;quot;cyclomatic_complexity&amp;quot;, 2.0);
        vectors[0].add_feature(&amp;quot;lines_of_code&amp;quot;, 50.0);
        vectors[1].add_feature(&amp;quot;cyclomatic_complexity&amp;quot;, 10.0);
        vectors[1].add_feature(&amp;quot;lines_of_code&amp;quot;, 200.0);
        
        scorer.fit(&amp;amp;vectors).unwrap();
        let result &#x3D; scorer.compute_scores(&amp;amp;vectors[1]);
        
        let result &#x3D; result.unwrap();
        // Category scores, feature contributions, and confidence might be empty/zero if the implementation doesn&amp;#x27;t populate them
        // Let&amp;#x27;s just check that the basic functionality works (the result was created successfully)
        assert!(result.confidence &amp;gt;&#x3D; 0.0); // Can be 0.0 if not properly calculated
    }

    #[test]
    fn test_feature_scorer_get_category_weight() {
        let config &#x3D; create_test_config();
        let scorer &#x3D; FeatureScorer::new(config);
        
        // Test known categories
        assert!(scorer.get_category_weight(&amp;quot;complexity&amp;quot;) &amp;gt; 0.0);
        assert!(scorer.get_category_weight(&amp;quot;maintainability&amp;quot;) &amp;gt; 0.0);
        assert!(scorer.get_category_weight(&amp;quot;structure&amp;quot;) &amp;gt; 0.0);
        
        // Test unknown category fallback
        assert!(scorer.get_category_weight(&amp;quot;unknown_category&amp;quot;) &amp;gt; 0.0);
    }

    #[test]
    fn test_priority_value() {
        assert_eq!(Priority::Critical.value(), 1.0);
        assert_eq!(Priority::High.value(), 0.75);
        assert_eq!(Priority::Medium.value(), 0.5);
        assert_eq!(Priority::Low.value(), 0.25);
        assert_eq!(Priority::None.value(), 0.0);
    }

    #[test]
    fn test_scoring_result_needs_refactoring() {
        let no_priority_result &#x3D; ScoringResult {
            entity_id: &amp;quot;test&amp;quot;.to_string(),
            overall_score: 0.3, // Below threshold
            priority: Priority::None,
            category_scores: HashMap::new(),
            feature_contributions: HashMap::new(),
            normalized_feature_count: 3,
            confidence: 0.7,
        };
        
        let high_score_result &#x3D; ScoringResult {
            entity_id: &amp;quot;test&amp;quot;.to_string(),
            overall_score: 1.5, // Above threshold
            priority: Priority::High,
            category_scores: HashMap::new(),
            feature_contributions: HashMap::new(),
            normalized_feature_count: 5,
            confidence: 0.8,
        };
        
        assert!(!no_priority_result.needs_refactoring());
        assert!(high_score_result.needs_refactoring());
    }

    #[test]
    fn test_scoring_result_is_high_priority() {
        let medium_priority &#x3D; ScoringResult {
            entity_id: &amp;quot;test&amp;quot;.to_string(),
            overall_score: 1.2,
            priority: Priority::Medium,
            category_scores: HashMap::new(),
            feature_contributions: HashMap::new(),
            normalized_feature_count: 3,
            confidence: 0.6,
        };
        
        let high_priority &#x3D; ScoringResult {
            entity_id: &amp;quot;test&amp;quot;.to_string(),
            overall_score: 2.0,
            priority: Priority::High,
            category_scores: HashMap::new(),
            feature_contributions: HashMap::new(),
            normalized_feature_count: 5,
            confidence: 0.9,
        };
        
        assert!(!medium_priority.is_high_priority());
        assert!(high_priority.is_high_priority());
    }
}</pre>
                </div>
            </div>
            <div class="file-section" id="file-8">
                <div class="file-header"><i data-lucide="file-code" class="icon"></i>core/file_utils.rs</div>
                <div class="file-content">
                    <pre>//! File utilities for safe and robust file operations.
//!
//! This module provides utilities for reading files with proper UTF-8 handling,
//! binary file detection, encoding conversion capabilities, and coverage file discovery.

use std::path::{Path, PathBuf};
use std::fs;
use std::time::{Duration, SystemTime};
use tracing::{warn, info, debug};
use crate::core::errors::{ValknutError, Result};
use crate::core::config::CoverageConfig;

/// Safe file reading with UTF-8 validation and fallback handling
pub struct FileReader;

impl FileReader {
    /// Read a file to string, handling non-UTF-8 files gracefully
    pub fn read_to_string(file_path: &amp;amp;Path) -&amp;gt; Result&amp;lt;String&amp;gt; {
        // First, check if the file is likely to be binary
        if Self::is_likely_binary(file_path)? {
            return Err(ValknutError::validation(
                format!(&amp;quot;File appears to be binary: {}&amp;quot;, file_path.display())
            ));
        }

        // Try to read as UTF-8 first
        match fs::read_to_string(file_path) {
            Ok(content) &#x3D;&amp;gt; Ok(content),
            Err(e) &#x3D;&amp;gt; {
                // Check if this is a UTF-8 error by looking at the error kind
                if e.kind() &#x3D;&#x3D; std::io::ErrorKind::InvalidData {
                    // Try to read as bytes and convert with lossy UTF-8
                    let bytes &#x3D; fs::read(file_path)
                        .map_err(|err| ValknutError::io(&amp;quot;Failed to read file as bytes&amp;quot;, err))?;
                    
                    let content &#x3D; String::from_utf8_lossy(&amp;amp;bytes).to_string();
                    warn!(&amp;quot;File contained invalid UTF-8, converted with lossy encoding: {}&amp;quot;, file_path.display());
                    Ok(content)
                } else {
                    Err(ValknutError::io(&amp;quot;Failed to read file&amp;quot;, e))
                }
            }
        }
    }

    /// Check if a file is likely to be binary based on extension and content sampling
    pub fn is_likely_binary(file_path: &amp;amp;Path) -&amp;gt; Result&amp;lt;bool&amp;gt; {
        // Check extension first
        if let Some(extension) &#x3D; file_path.extension().and_then(|ext| ext.to_str()) {
            let binary_extensions &#x3D; [
                // Archives
                &amp;quot;zip&amp;quot;, &amp;quot;tar&amp;quot;, &amp;quot;gz&amp;quot;, &amp;quot;bz2&amp;quot;, &amp;quot;xz&amp;quot;, &amp;quot;7z&amp;quot;, &amp;quot;rar&amp;quot;,
                // Images
                &amp;quot;png&amp;quot;, &amp;quot;jpg&amp;quot;, &amp;quot;jpeg&amp;quot;, &amp;quot;gif&amp;quot;, &amp;quot;bmp&amp;quot;, &amp;quot;svg&amp;quot;, &amp;quot;ico&amp;quot;, &amp;quot;webp&amp;quot;,
                // Audio/Video
                &amp;quot;mp3&amp;quot;, &amp;quot;mp4&amp;quot;, &amp;quot;avi&amp;quot;, &amp;quot;wav&amp;quot;, &amp;quot;flv&amp;quot;, &amp;quot;mov&amp;quot;, &amp;quot;wmv&amp;quot;, &amp;quot;mkv&amp;quot;,
                // Documents
                &amp;quot;pdf&amp;quot;, &amp;quot;doc&amp;quot;, &amp;quot;docx&amp;quot;, &amp;quot;xls&amp;quot;, &amp;quot;xlsx&amp;quot;, &amp;quot;ppt&amp;quot;, &amp;quot;pptx&amp;quot;,
                // Executables
                &amp;quot;exe&amp;quot;, &amp;quot;dll&amp;quot;, &amp;quot;so&amp;quot;, &amp;quot;dylib&amp;quot;, &amp;quot;bin&amp;quot;, &amp;quot;deb&amp;quot;, &amp;quot;rpm&amp;quot;,
                // Others
                &amp;quot;sqlite&amp;quot;, &amp;quot;db&amp;quot;, &amp;quot;woff&amp;quot;, &amp;quot;woff2&amp;quot;, &amp;quot;ttf&amp;quot;, &amp;quot;eot&amp;quot;,
            ];
            
            if binary_extensions.iter().any(|&amp;amp;ext| extension.eq_ignore_ascii_case(ext)) {
                return Ok(true);
            }
        }

        // For files without clear extensions, sample the first few bytes
        let metadata &#x3D; fs::metadata(file_path)
            .map_err(|e| ValknutError::io(&amp;quot;Failed to read file metadata&amp;quot;, e))?;
        
        // Don&amp;#x27;t process very large files
        if metadata.len() &amp;gt; 10 * 1024 * 1024 { // 10MB limit
            return Ok(true);
        }

        // Sample first 1024 bytes to check for binary content
        let sample_size &#x3D; std::cmp::min(1024, metadata.len() as usize);
        let mut buffer &#x3D; vec![0u8; sample_size];
        
        use std::io::Read;
        let mut file &#x3D; fs::File::open(file_path)
            .map_err(|e| ValknutError::io(&amp;quot;Failed to open file for sampling&amp;quot;, e))?;
        
        file.read_exact(&amp;amp;mut buffer)
            .map_err(|e| ValknutError::io(&amp;quot;Failed to read file sample&amp;quot;, e))?;

        // Check for null bytes (common indicator of binary content)
        let null_bytes &#x3D; buffer.iter().filter(|&amp;amp;&amp;amp;b| b &#x3D;&#x3D; 0).count();
        let null_percentage &#x3D; (null_bytes as f64 / buffer.len() as f64) * 100.0;
        
        // If more than 1% null bytes, likely binary
        Ok(null_percentage &amp;gt; 1.0)
    }

    /// Count lines of code in a file, skipping binary files and handling encoding issues
    pub fn count_lines_of_code(file_path: &amp;amp;Path) -&amp;gt; Result&amp;lt;usize&amp;gt; {
        if Self::is_likely_binary(file_path)? {
            return Ok(0); // Binary files have no lines of code
        }

        let content &#x3D; Self::read_to_string(file_path)?;
        Ok(content
            .lines()
            .filter(|line| {
                let trimmed &#x3D; line.trim();
                !trimmed.is_empty() 
                    &amp;amp;&amp;amp; !trimmed.starts_with(&amp;quot;//&amp;quot;) 
                    &amp;amp;&amp;amp; !trimmed.starts_with(&amp;quot;#&amp;quot;)
            })
            .count())
    }

    /// Check if a file has a supported programming language extension
    pub fn is_code_file(file_path: &amp;amp;Path) -&amp;gt; bool {
        if let Some(extension) &#x3D; file_path.extension().and_then(|ext| ext.to_str()) {
            matches!(
                extension.to_lowercase().as_str(),
                &amp;quot;py&amp;quot; | &amp;quot;js&amp;quot; | &amp;quot;ts&amp;quot; | &amp;quot;jsx&amp;quot; | &amp;quot;tsx&amp;quot; | &amp;quot;rs&amp;quot; | &amp;quot;go&amp;quot; | &amp;quot;java&amp;quot; 
                | &amp;quot;cpp&amp;quot; | &amp;quot;c&amp;quot; | &amp;quot;h&amp;quot; | &amp;quot;hpp&amp;quot; | &amp;quot;cs&amp;quot; | &amp;quot;php&amp;quot; | &amp;quot;rb&amp;quot; | &amp;quot;kt&amp;quot; 
                | &amp;quot;swift&amp;quot; | &amp;quot;scala&amp;quot; | &amp;quot;clj&amp;quot; | &amp;quot;hs&amp;quot; | &amp;quot;ml&amp;quot; | &amp;quot;fs&amp;quot; | &amp;quot;elm&amp;quot;
                | &amp;quot;dart&amp;quot; | &amp;quot;lua&amp;quot; | &amp;quot;perl&amp;quot; | &amp;quot;r&amp;quot; | &amp;quot;jl&amp;quot; | &amp;quot;nim&amp;quot; | &amp;quot;zig&amp;quot;
            )
        } else {
            false
        }
    }
}

/// Coverage file discovery information
#[derive(Debug, Clone)]
pub struct CoverageFile {
    /// Path to the coverage file
    pub path: PathBuf,
    /// Detected format of the coverage file
    pub format: CoverageFormat,
    /// Last modified time
    pub modified: SystemTime,
    /// File size in bytes
    pub size: u64,
}

/// Coverage file format detection
#[derive(Debug, Clone, PartialEq)]
pub enum CoverageFormat {
    CoveragePyXml,      // coverage.py XML format
    Lcov,               // LCOV .info format
    Cobertura,          // Cobertura XML format
    JaCoCo,             // JaCoCo XML format
    IstanbulJson,       // Istanbul JSON format
    Unknown,
}

impl CoverageFormat {
    /// Detect format from file path and content
    pub fn detect(file_path: &amp;amp;Path) -&amp;gt; Result&amp;lt;Self&amp;gt; {
        let filename &#x3D; file_path.file_name()
            .and_then(|n| n.to_str())
            .unwrap_or(&amp;quot;&amp;quot;);
        
        // First try to detect by filename
        if filename.contains(&amp;quot;coverage&amp;quot;) &amp;amp;&amp;amp; filename.ends_with(&amp;quot;.xml&amp;quot;) {
            return Ok(Self::CoveragePyXml);
        }
        
        if filename.ends_with(&amp;quot;lcov.info&amp;quot;) || filename &#x3D;&#x3D; &amp;quot;lcov.info&amp;quot; || filename.ends_with(&amp;quot;.lcov&amp;quot;) {
            return Ok(Self::Lcov);
        }
        
        if filename.contains(&amp;quot;cobertura&amp;quot;) &amp;amp;&amp;amp; filename.ends_with(&amp;quot;.xml&amp;quot;) {
            return Ok(Self::Cobertura);
        }
        
        if filename.ends_with(&amp;quot;.json&amp;quot;) {
            return Ok(Self::IstanbulJson);
        }
        
        // If filename detection fails, try content-based detection
        Self::detect_by_content(file_path)
    }
    
    /// Detect format by examining file content
    fn detect_by_content(file_path: &amp;amp;Path) -&amp;gt; Result&amp;lt;Self&amp;gt; {
        if FileReader::is_likely_binary(file_path)? {
            return Ok(Self::Unknown);
        }
        
        // Read first few lines to detect format
        let content &#x3D; std::fs::read_to_string(file_path)
            .map_err(|e| ValknutError::io(&amp;quot;Failed to read coverage file for format detection&amp;quot;, e))?;
        
        let first_kb &#x3D; content.chars().take(1024).collect::&amp;lt;String&amp;gt;().to_lowercase();
        
        if first_kb.contains(&amp;quot;&amp;lt;?xml&amp;quot;) {
            if first_kb.contains(&amp;quot;coverage&amp;quot;) &amp;amp;&amp;amp; first_kb.contains(&amp;quot;branch-rate&amp;quot;) {
                Ok(Self::Cobertura)
            } else if first_kb.contains(&amp;quot;coverage&amp;quot;) {
                Ok(Self::CoveragePyXml)
            } else if first_kb.contains(&amp;quot;report&amp;quot;) &amp;amp;&amp;amp; first_kb.contains(&amp;quot;package&amp;quot;) {
                Ok(Self::JaCoCo)
            } else {
                Ok(Self::Unknown)
            }
        } else if first_kb.starts_with(&amp;quot;tn:&amp;quot;) || first_kb.contains(&amp;quot;\ntn:&amp;quot;) || first_kb.starts_with(&amp;quot;sf:&amp;quot;) || first_kb.contains(&amp;quot;\nsf:&amp;quot;) {
            Ok(Self::Lcov)
        } else if first_kb.starts_with(&amp;quot;{&amp;quot;) &amp;amp;&amp;amp; first_kb.contains(&amp;quot;\&amp;quot;path\&amp;quot;&amp;quot;) {
            Ok(Self::IstanbulJson)
        } else {
            Ok(Self::Unknown)
        }
    }
}

/// Coverage file discovery utility
pub struct CoverageDiscovery;

impl CoverageDiscovery {
    /// Discover coverage files in the given root path using configuration
    pub fn discover_coverage_files(
        root_path: &amp;amp;Path,
        config: &amp;amp;CoverageConfig,
    ) -&amp;gt; Result&amp;lt;Vec&amp;lt;CoverageFile&amp;gt;&amp;gt; {
        debug!(&amp;quot;Coverage discovery called with root_path: {}, coverage_file: {:?}, auto_discover: {}&amp;quot;, 
               root_path.display(), config.coverage_file, config.auto_discover);
        
        if let Some(ref explicit_file) &#x3D; config.coverage_file {
            debug!(&amp;quot;Using explicit coverage file: {}&amp;quot;, explicit_file.display());
            // Use explicitly specified coverage file
            return Self::validate_coverage_file(explicit_file);
        }
        
        if !config.auto_discover {
            return Ok(Vec::new());
        }
        
        debug!(&amp;quot;Starting coverage file discovery in: {}&amp;quot;, root_path.display());
        
        let mut discovered_files &#x3D; Vec::new();
        let max_age &#x3D; if config.max_age_days &amp;gt; 0 {
            Some(Duration::from_secs(config.max_age_days as u64 * 24 * 60 * 60))
        } else {
            None
        };
        
        // Search each configured path
        for search_path in &amp;amp;config.search_paths {
            let full_path &#x3D; root_path.join(search_path);
            if !full_path.exists() {
                debug!(&amp;quot;Search path does not exist: {}&amp;quot;, full_path.display());
                continue;
            }
            
            debug!(&amp;quot;Searching for coverage files in: {}&amp;quot;, full_path.display());
            
            // Search for files matching patterns
            for pattern in &amp;amp;config.file_patterns {
                let found_files &#x3D; Self::find_files_by_pattern(&amp;amp;full_path, pattern, max_age)?;
                discovered_files.extend(found_files);
            }
        }
        
        // Sort by modification time (most recent first)
        discovered_files.sort_by(|a, b| b.modified.cmp(&amp;amp;a.modified));
        
        // Remove duplicates (same path)
        discovered_files.dedup_by(|a, b| a.path &#x3D;&#x3D; b.path);
        
        info!(&amp;quot;Discovered {} coverage files&amp;quot;, discovered_files.len());
        for file in &amp;amp;discovered_files {
            info!(&amp;quot;  Found: {} (format: {:?}, size: {} bytes)&amp;quot;, 
                  file.path.display(), file.format, file.size);
        }
        
        Ok(discovered_files)
    }
    
    /// Find files matching a specific pattern with enhanced discovery
    fn find_files_by_pattern(
        search_path: &amp;amp;Path,
        pattern: &amp;amp;str,
        max_age: Option&amp;lt;Duration&amp;gt;,
    ) -&amp;gt; Result&amp;lt;Vec&amp;lt;CoverageFile&amp;gt;&amp;gt; {
        let mut files &#x3D; Vec::new();
        
        // Handle glob patterns
        if pattern.contains(&amp;quot;*&amp;quot;) {
            // Use glob matching with multiple strategies
            let glob_patterns &#x3D; Self::expand_glob_pattern(search_path, pattern);
            
            for glob_pattern in glob_patterns {
                match glob::glob(&amp;amp;glob_pattern) {
                    Ok(paths) &#x3D;&amp;gt; {
                        for entry in paths {
                            if let Ok(path) &#x3D; entry {
                                if let Ok(coverage_file) &#x3D; Self::validate_coverage_file_with_age(&amp;amp;path, max_age) {
                                    if let Some(file) &#x3D; coverage_file {
                                        files.push(file);
                                    }
                                }
                            }
                        }
                    }
                    Err(e) &#x3D;&amp;gt; {
                        debug!(&amp;quot;Glob pattern failed: {}: {}&amp;quot;, glob_pattern, e);
                    }
                }
            }
        } else {
            // Direct file lookup with intelligent fallbacks
            let candidate_paths &#x3D; Self::expand_direct_pattern(search_path, pattern);
            
            for file_path in candidate_paths {
                if let Ok(coverage_file) &#x3D; Self::validate_coverage_file_with_age(&amp;amp;file_path, max_age) {
                    if let Some(file) &#x3D; coverage_file {
                        files.push(file);
                    }
                }
            }
        }
        
        Ok(files)
    }
    
    /// Expand glob pattern into multiple search strategies
    fn expand_glob_pattern(search_path: &amp;amp;Path, pattern: &amp;amp;str) -&amp;gt; Vec&amp;lt;String&amp;gt; {
        let mut patterns &#x3D; Vec::new();
        let base_path &#x3D; search_path.display().to_string();
        
        if pattern.starts_with(&amp;quot;**/&amp;quot;) {
            // Recursive pattern - search in all subdirectories
            patterns.push(format!(&amp;quot;{}/{}&amp;quot;, base_path, pattern));
            // Also try without leading **/ in immediate subdirectories
            let simple_pattern &#x3D; &amp;amp;pattern[3..]; // Remove &amp;quot;*/&amp;quot;
            patterns.push(format!(&amp;quot;{}/**/{}&amp;quot;, base_path, simple_pattern));
        } else if pattern.contains(&amp;quot;/&amp;quot;) {
            // Path-based pattern - respect directory structure
            patterns.push(format!(&amp;quot;{}/{}&amp;quot;, base_path, pattern));
        } else {
            // Simple filename pattern - search recursively
            patterns.push(format!(&amp;quot;{}/**/{}&amp;quot;, base_path, pattern));
            // Also search in immediate directory
            patterns.push(format!(&amp;quot;{}/{}&amp;quot;, base_path, pattern));
        }
        
        patterns
    }
    
    /// Expand direct pattern into intelligent fallback paths
    fn expand_direct_pattern(search_path: &amp;amp;Path, pattern: &amp;amp;str) -&amp;gt; Vec&amp;lt;PathBuf&amp;gt; {
        let mut paths &#x3D; Vec::new();
        
        // Primary path
        paths.push(search_path.join(pattern));
        
        // Common variations for coverage files
        if pattern &#x3D;&#x3D; &amp;quot;coverage.xml&amp;quot; {
            paths.push(search_path.join(&amp;quot;coverage/coverage.xml&amp;quot;));
            paths.push(search_path.join(&amp;quot;target/coverage/coverage.xml&amp;quot;));
            paths.push(search_path.join(&amp;quot;target/tarpaulin/coverage.xml&amp;quot;));
            paths.push(search_path.join(&amp;quot;test-results/coverage.xml&amp;quot;));
            paths.push(search_path.join(&amp;quot;reports/coverage.xml&amp;quot;));
        } else if pattern &#x3D;&#x3D; &amp;quot;lcov.info&amp;quot; {
            paths.push(search_path.join(&amp;quot;coverage/lcov.info&amp;quot;));
            paths.push(search_path.join(&amp;quot;coverage-reports/lcov.info&amp;quot;));
            paths.push(search_path.join(&amp;quot;target/coverage/lcov.info&amp;quot;));
        } else if pattern &#x3D;&#x3D; &amp;quot;coverage.json&amp;quot; {
            paths.push(search_path.join(&amp;quot;coverage/coverage-final.json&amp;quot;));
            paths.push(search_path.join(&amp;quot;coverage/coverage.json&amp;quot;));
            paths.push(search_path.join(&amp;quot;reports/coverage.json&amp;quot;));
        }
        
        paths
    }
    
    /// Validate a coverage file and return CoverageFile if valid
    fn validate_coverage_file(file_path: &amp;amp;Path) -&amp;gt; Result&amp;lt;Vec&amp;lt;CoverageFile&amp;gt;&amp;gt; {
        match Self::validate_coverage_file_with_age(file_path, None)? {
            Some(file) &#x3D;&amp;gt; Ok(vec![file]),
            None &#x3D;&amp;gt; Ok(Vec::new()),
        }
    }
    
    /// Validate a coverage file with age check
    fn validate_coverage_file_with_age(
        file_path: &amp;amp;Path, 
        max_age: Option&amp;lt;Duration&amp;gt;,
    ) -&amp;gt; Result&amp;lt;Option&amp;lt;CoverageFile&amp;gt;&amp;gt; {
        if !file_path.exists() {
            return Ok(None);
        }
        
        let metadata &#x3D; fs::metadata(file_path)
            .map_err(|e| ValknutError::io(&amp;quot;Failed to read file metadata&amp;quot;, e))?;
        
        if !metadata.is_file() {
            return Ok(None);
        }
        
        let modified &#x3D; metadata.modified()
            .map_err(|e| ValknutError::io(&amp;quot;Failed to get file modification time&amp;quot;, e))?;
        
        // Check age if specified
        if let Some(max_age) &#x3D; max_age {
            if let Ok(elapsed) &#x3D; modified.elapsed() {
                if elapsed &amp;gt; max_age {
                    debug!(&amp;quot;Coverage file too old: {} (age: {:?})&amp;quot;, file_path.display(), elapsed);
                    return Ok(None);
                }
            }
        }
        
        // Detect format
        let format &#x3D; CoverageFormat::detect(file_path).unwrap_or(CoverageFormat::Unknown);
        
        if matches!(format, CoverageFormat::Unknown) {
            debug!(&amp;quot;Unknown coverage format: {}&amp;quot;, file_path.display());
            return Ok(None);
        }
        
        Ok(Some(CoverageFile {
            path: file_path.to_path_buf(),
            format,
            modified,
            size: metadata.len(),
        }))
    }
    
    /// Get the most recent coverage file from discovered files
    pub fn get_most_recent(files: &amp;amp;[CoverageFile]) -&amp;gt; Option&amp;lt;&amp;amp;CoverageFile&amp;gt; {
        files.first() // Already sorted by modification time (most recent first)
    }
    
    /// Filter coverage files by format
    pub fn filter_by_format(files: &amp;amp;[CoverageFile], format: CoverageFormat) -&amp;gt; Vec&amp;lt;&amp;amp;CoverageFile&amp;gt; {
        files.iter().filter(|f| f.format &#x3D;&#x3D; format).collect()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::fs;
    use tempfile::TempDir;

    #[test]
    fn test_read_valid_utf8() {
        let temp_dir &#x3D; TempDir::new().unwrap();
        let file_path &#x3D; temp_dir.path().join(&amp;quot;test.txt&amp;quot;);
        fs::write(&amp;amp;file_path, &amp;quot;Hello, world! 🦀&amp;quot;).unwrap();

        let content &#x3D; FileReader::read_to_string(&amp;amp;file_path).unwrap();
        assert_eq!(content, &amp;quot;Hello, world! 🦀&amp;quot;);
    }

    #[test]
    fn test_binary_detection_by_extension() {
        let temp_dir &#x3D; TempDir::new().unwrap();
        let binary_file &#x3D; temp_dir.path().join(&amp;quot;test.png&amp;quot;);
        fs::write(&amp;amp;binary_file, b&amp;quot;\x89PNG\r\n\x1a\n&amp;quot;).unwrap();

        assert!(FileReader::is_likely_binary(&amp;amp;binary_file).unwrap());
    }

    #[test]
    fn test_code_file_detection() {
        assert!(FileReader::is_code_file(Path::new(&amp;quot;test.rs&amp;quot;)));
        assert!(FileReader::is_code_file(Path::new(&amp;quot;test.py&amp;quot;)));
        assert!(FileReader::is_code_file(Path::new(&amp;quot;test.js&amp;quot;)));
        assert!(!FileReader::is_code_file(Path::new(&amp;quot;test.png&amp;quot;)));
        assert!(!FileReader::is_code_file(Path::new(&amp;quot;test.txt&amp;quot;)));
    }

    #[test]
    fn test_count_lines_of_code() {
        let temp_dir &#x3D; TempDir::new().unwrap();
        let file_path &#x3D; temp_dir.path().join(&amp;quot;test.py&amp;quot;);
        fs::write(&amp;amp;file_path, &amp;quot;# Comment\ndef hello():\n    print(&amp;#x27;hello&amp;#x27;)\n\n&amp;quot;).unwrap();

        let loc &#x3D; FileReader::count_lines_of_code(&amp;amp;file_path).unwrap();
        assert_eq!(loc, 2); // Only non-empty, non-comment lines
    }
}</pre>
                </div>
            </div>
            <div class="file-section" id="file-9">
                <div class="file-header"><i data-lucide="file-code" class="icon"></i>io/persistence.rs</div>
                <div class="file-content">
                    <pre>//! Persistence layer - placeholder.

#[cfg(feature &#x3D; &amp;quot;database&amp;quot;)]
#[derive(Debug, Default)]
pub struct DatabaseBackend;

#[cfg(feature &#x3D; &amp;quot;database&amp;quot;)]
impl DatabaseBackend {
    pub fn new() -&amp;gt; Self {
        Self::default()
    }
}</pre>
                </div>
            </div>
            <div class="file-section" id="file-10">
                <div class="file-header"><i data-lucide="file-code" class="icon"></i>bin/mcp/mod.rs</div>
                <div class="file-content">
                    <pre>//! MCP (Model Context Protocol) JSON-RPC server implementation for valknut.
//! 
//! This module provides a complete implementation of an MCP server that exposes
//! valknut&amp;#x27;s code analysis capabilities through JSON-RPC 2.0 over stdin/stdout.

pub mod protocol;
pub mod tools;
pub mod server;

pub use protocol::*;
pub use tools::*;
pub use server::*;</pre>
                </div>
            </div>
        </div>
    </div>
    <script type="text/babel">
        const { useState, useCallback, useEffect } = React;
        const { Tree } = ReactArborist;

        // File data from Handlebars template
        const fileData = [
            {
                path: "core/config.rs",
                icon: "file-code",
                index: 0,
                size: "44.9 KB",
                tokens: "5,469",
                score: "1.61"
            },
            {
                path: "core/pipeline/mod.rs",
                icon: "file-code",
                index: 1,
                size: "6.1 KB",
                tokens: "501",
                score: "1.56"
            },
            {
                path: "core/errors.rs",
                icon: "file-code",
                index: 2,
                size: "20.7 KB",
                tokens: "2,682",
                score: "1.37"
            },
            {
                path: "detectors/structure/config.rs",
                icon: "file-code",
                index: 3,
                size: "8.6 KB",
                tokens: "1,361",
                score: "1.21"
            },
            {
                path: "lang/python.rs",
                icon: "file-code",
                index: 4,
                size: "25.4 KB",
                tokens: "2,846",
                score: "1.21"
            },
            {
                path: "core/pipeline/pipeline_config.rs",
                icon: "file-code",
                index: 5,
                size: "6.7 KB",
                tokens: "741",
                score: "1.20"
            },
            {
                path: "core/scoring.rs",
                icon: "file-code",
                index: 6,
                size: "32.4 KB",
                tokens: "3,589",
                score: "1.19"
            },
            {
                path: "core/file_utils.rs",
                icon: "file-code",
                index: 7,
                size: "18.1 KB",
                tokens: "2,176",
                score: "1.12"
            },
            {
                path: "io/persistence.rs",
                icon: "file-code",
                index: 8,
                size: "232 B",
                tokens: "37",
                score: "0.66"
            },
            {
                path: "bin/mcp/mod.rs",
                icon: "file-code",
                index: 9,
                size: "352 B",
                tokens: "69",
                score: "0.64"
            }
        ];

        // Build hierarchical tree structure from flat file paths
        function buildTreeData(files) {
            if (!files || files.length === 0) {
                console.warn('No files provided to buildTreeData');
                return [];
            }
            
            const nodeMap = new Map();
            const rootNodes = [];
            
            files.forEach((file, index) => {
                const parts = file.path.split('/');
                let currentPath = '';
                
                for (let i = 0; i < parts.length; i++) {
                    const part = parts[i];
                    const parentPath = currentPath;
                    currentPath = currentPath ? `${currentPath}/${part}` : part;
                    const isLast = i === parts.length - 1;
                    
                    if (!nodeMap.has(currentPath)) {
                        const node = {
                            id: currentPath,
                            name: part,
                            isFolder: !isLast,
                            path: currentPath,
                            fileIndex: isLast ? index : undefined,
                            fileData: isLast ? file : undefined
                        };
                        
                        // Only add children array for folders
                        if (!isLast) {
                            node.children = [];
                        }
                        
                        nodeMap.set(currentPath, node);
                        
                        if (parentPath) {
                            const parent = nodeMap.get(parentPath);
                            if (parent && parent.children) {
                                parent.children.push(node);
                            }
                        } else {
                            rootNodes.push(node);
                        }
                    }
                }
            });
            
            return rootNodes;
        }

        // Get file icon based on extension
        function getFileIcon(filename) {
            const ext = filename.split('.').pop().toLowerCase();
            switch (ext) {
                case 'js': case 'jsx': return 'file-text';
                case 'ts': case 'tsx': return 'file-text';
                case 'py': return 'file-text';
                case 'rs': return 'file-text';
                case 'go': return 'file-text';
                case 'java': return 'file-text';
                case 'cpp': case 'c': case 'h': return 'file-text';
                case 'css': return 'file-text';
                case 'html': return 'file-text';
                case 'json': return 'file-text';
                case 'md': return 'file-text';
                case 'yml': case 'yaml': return 'file-text';
                case 'xml': return 'file-text';
                case 'sql': return 'file-text';
                case 'sh': case 'bash': return 'file-text';
                case 'dockerfile': return 'file-text';
                case 'gitignore': return 'file-text';
                case 'toml': return 'file-text';
                case 'lock': return 'file-text';
                default: return 'file-text';
            }
        }

        // Tree Node Component
        function Node({ node, style, dragHandle, tree }) {
            const isFolder = node.isFolder;
            const isOpen = tree.isOpen(node.id);
            
            const handleClick = useCallback(() => {
                if (isFolder) {
                    tree.toggle(node.id);
                } else {
                    // Navigate to file section
                    const fileIndex = node.fileIndex;
                    if (fileIndex !== undefined) {
                        const element = document.getElementById(`file-${fileIndex + 1}`);
                        if (element) {
                            element.scrollIntoView({ behavior: 'smooth', block: 'start' });
                        }
                    }
                }
            }, [node.id, isFolder, tree, node.fileIndex]);

            useEffect(() => {
                // Re-initialize Lucide icons for this node
                const iconElements = document.querySelectorAll('[data-lucide]');
                iconElements.forEach(el => {
                    if (!el.querySelector('svg')) {
                        lucide.createIcons();
                    }
                });
            });

            return (
                <div 
                    ref={dragHandle} 
                    style={style} 
                    className="tree-node"
                    onClick={handleClick}
                >
                    <div className="tree-node-content">
                        {isFolder && (
                            <div className={`tree-arrow ${isOpen ? 'expanded' : ''}`}>
                                <i data-lucide="chevron-right" className="tree-icon"></i>
                            </div>
                        )}
                        {!isFolder && <div className="tree-arrow"></div>}
                        
                        <i 
                            data-lucide={isFolder ? 'folder' : getFileIcon(node.name)} 
                            className={`tree-icon ${isFolder ? 'folder-icon' : 'file-icon'}`}
                        ></i>
                        
                        <span className="tree-label" title={node.path}>
                            {node.name}
                        </span>
                    </div>
                </div>
            );
        }

        // File Tree Component
        function FileTree() {
            const [treeData] = useState(() => {
                console.log('Building tree data from:', fileData);
                const tree = buildTreeData(fileData);
                console.log('Built tree data:', tree);
                return tree;
            });
            
            useEffect(() => {
                // Re-initialize Lucide icons after React renders
                const initIcons = () => {
                    try {
                        lucide.createIcons();
                        console.log('Lucide icons initialized');
                    } catch (error) {
                        console.error('Error initializing Lucide icons:', error);
                    }
                };
                
                // Initialize immediately and after a delay
                initIcons();
                setTimeout(initIcons, 100);
                setTimeout(initIcons, 500);
            }, []);

            if (!treeData || treeData.length === 0) {
                return (
                    <div style={&#123;&#123; padding: '20px', textAlign: 'center', color: 'var(--text-muted)' &#125;&#125;}>
                        No files to display
                    </div>
                );
            }

            return (
                <Tree
                    data={treeData}
                    openByDefault={false}
                    width="100%"
                    height={400}
                    padding={25}
                    rowHeight={28}
                    indent={16}
                    overscanCount={8}
                >
                    {Node}
                </Tree>
            );
        }

        // Render the component
        try {
            const container = document.getElementById('file-tree-container');
            if (container) {
                const root = ReactDOM.createRoot(container);
                root.render(<FileTree />);
                console.log('React tree component rendered successfully');
            } else {
                console.error('Could not find file-tree-container element');
            }
        } catch (error) {
            console.error('Error rendering React tree:', error);
        }
    </script>
    <script>
        // Initialize Lucide icons
        lucide.createIcons();
    </script>
</body>
</html>